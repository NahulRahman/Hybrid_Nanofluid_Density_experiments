{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kusqfjOr8iSw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2KuiekUUoH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "custom scoring, hyperopt"
      ],
      "metadata": {
        "id": "E6o3_QTqaKbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Hyperparameter Optimization Libraries\n",
        "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
        "from hyperopt.pyll.base import scope\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from functools import partial\n",
        "\n",
        "# XGBoost\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available. Skipping XGBoost model.\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "class NanofluidDensityOptimizer:\n",
        "    def __init__(self, data_path=None, df=None):\n",
        "        \"\"\"\n",
        "        Initialize the optimizer with data\n",
        "        Args:\n",
        "            data_path: Path to CSV file\n",
        "            df: DataFrame if already loaded\n",
        "        \"\"\"\n",
        "        if df is not None:\n",
        "            self.df = df.copy()\n",
        "        elif data_path:\n",
        "            self.df = pd.read_csv(data_path)\n",
        "        else:\n",
        "            raise ValueError(\"Either data_path or df must be provided\")\n",
        "\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "\n",
        "        # Results storage\n",
        "        self.custom_results = []\n",
        "        self.hyperopt_results = [] # Keep this to maintain structure, but won't be populated by hyperopt\n",
        "\n",
        "    def preprocess_data(self, test_size=0.2):\n",
        "        \"\"\"Preprocess the data: encode categoricals and scale features\"\"\"\n",
        "        df_processed = self.df.copy()\n",
        "\n",
        "        # Encode categorical columns\n",
        "        categorical_cols = ['Nano Particle', 'Base Fluid']\n",
        "        for col in categorical_cols:\n",
        "            if col in df_processed.columns:\n",
        "                le = LabelEncoder()\n",
        "                df_processed[col] = le.fit_transform(df_processed[col])\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df_processed.drop('Density (ρ)', axis=1)\n",
        "        y = df_processed['Density (ρ)']\n",
        "\n",
        "        # Split data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42\n",
        "        )\n",
        "\n",
        "        # Scale features\n",
        "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
        "\n",
        "        print(f\"Data preprocessed. Training set: {self.X_train_scaled.shape}, Test set: {self.X_test_scaled.shape}\")\n",
        "\n",
        "    def calculate_metrics(self, y_true, y_pred, n_params=2):\n",
        "        \"\"\"Calculate comprehensive regression metrics\"\"\"\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "        # MAPE (Mean Absolute Percentage Error) - handle division by zero\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true != 0, y_true, 1))) * 100\n",
        "            mape = np.where(np.isfinite(mape), mape, 0)\n",
        "\n",
        "        # AIC and BIC approximations\n",
        "        n = len(y_true)\n",
        "        k = n_params\n",
        "\n",
        "        # Log-likelihood approximation for normal distribution\n",
        "        if mse > 0:\n",
        "            log_likelihood = -n/2 * np.log(2*np.pi) - n/2 * np.log(mse) - n/2\n",
        "        else:\n",
        "            log_likelihood = 0\n",
        "\n",
        "        aic = 2*k - 2*log_likelihood\n",
        "        bic = k*np.log(n) - 2*log_likelihood\n",
        "\n",
        "        return {\n",
        "            'MSE': mse,\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'R2': r2,\n",
        "            'AIC': aic,\n",
        "            'BIC': bic\n",
        "        }\n",
        "\n",
        "    def custom_scoring_optimization(self):\n",
        "        \"\"\"Hyperparameter optimization using RandomizedSearchCV\"\"\"\n",
        "        print(\"Starting Custom Scoring Hyperparameter Optimization...\")\n",
        "\n",
        "        # Define models and parameter grids\n",
        "        models_params = {\n",
        "            'SVM': {\n",
        "                'model': SVR(),\n",
        "                'params': {\n",
        "                    'C': [0.1, 1, 10, 100],\n",
        "                    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "                    'epsilon': [0.01, 0.1, 0.5, 1]\n",
        "                }\n",
        "            },\n",
        "            'Random Forest': {\n",
        "                'model': RandomForestRegressor(random_state=42),\n",
        "                'params': {\n",
        "                    'n_estimators': range(50, 201, 25),\n",
        "                    'max_depth': range(5, 21, 2),\n",
        "                    'min_samples_split': range(2, 11),\n",
        "                    'min_samples_leaf': range(1, 6)\n",
        "                }\n",
        "            },\n",
        "            'Decision Tree': {\n",
        "                'model': DecisionTreeRegressor(random_state=42),\n",
        "                'params': {\n",
        "                    'max_depth': range(5, 21, 2),\n",
        "                    'min_samples_split': range(2, 21, 2),\n",
        "                    'min_samples_leaf': range(1, 11)\n",
        "                }\n",
        "            },\n",
        "            'Extra Trees': {\n",
        "                'model': ExtraTreesRegressor(random_state=42),\n",
        "                'params': {\n",
        "                    'n_estimators': range(50, 301, 25),\n",
        "                    'max_depth': range(5, 26, 3),\n",
        "                    'min_samples_split': range(2, 16, 2),\n",
        "                    'min_samples_leaf': range(1, 9)\n",
        "                }\n",
        "            },\n",
        "            'MLP': {\n",
        "                'model': MLPRegressor(random_state=42),\n",
        "                'params': {\n",
        "                    'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],\n",
        "                    'alpha': [0.0001, 0.001, 0.01],\n",
        "                    'learning_rate_init': [0.001, 0.01, 0.1],\n",
        "                    'max_iter': [200, 300, 500]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Add XGBoost if available\n",
        "        if XGBOOST_AVAILABLE:\n",
        "            models_params['XGBoost'] = {\n",
        "                'model': xgb.XGBRegressor(random_state=42),\n",
        "                'params': {\n",
        "                    'n_estimators': range(50, 201, 25),\n",
        "                    'max_depth': range(3, 11),\n",
        "                    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "                    'subsample': [0.5, 0.7, 0.8, 1.0]\n",
        "                }\n",
        "            }\n",
        "\n",
        "        for model_name, model_info in models_params.items():\n",
        "            print(f\"\\nOptimizing {model_name}...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Randomized search\n",
        "            random_search = RandomizedSearchCV(\n",
        "                estimator=model_info['model'],\n",
        "                param_distributions=model_info['params'],\n",
        "                n_iter=30,\n",
        "                cv=5,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            random_search.fit(self.X_train_scaled, self.y_train)\n",
        "\n",
        "            # Best model predictions\n",
        "            best_model = random_search.best_estimator_\n",
        "            y_pred = best_model.predict(self.X_test_scaled)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = self.calculate_metrics(self.y_test, y_pred)\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'Model': model_name,\n",
        "                'Method': 'Custom Scoring',\n",
        "                'Best_Params': random_search.best_params_,\n",
        "                'CV_Score': -random_search.best_score_,\n",
        "                'Execution_Time': execution_time,\n",
        "                **metrics\n",
        "            }\n",
        "\n",
        "            self.custom_results.append(result)\n",
        "            print(f\"{model_name} completed. Best CV Score: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "    def hyperopt_objective(self, params, model_type, X, y):\n",
        "        \"\"\"Objective function for hyperopt optimization\"\"\"\n",
        "        try:\n",
        "            # Create model based on type\n",
        "            if model_type == 'SVM':\n",
        "                model = SVR(**params)\n",
        "            elif model_type == 'Random Forest':\n",
        "                model = RandomForestRegressor(random_state=42, **params)\n",
        "            elif model_type == 'Decision Tree':\n",
        "                model = DecisionTreeRegressor(random_state=42, **params)\n",
        "            elif model_type == 'XGBoost' and XGBOOST_AVAILABLE:\n",
        "                model = xgb.XGBRegressor(random_state=42, **params)\n",
        "            elif model_type == 'MLP':\n",
        "                model = MLPRegressor(random_state=42, max_iter=500, **params)\n",
        "            elif model_type == 'Extra Trees':\n",
        "                model = ExtraTreesRegressor(random_state=42, **params)\n",
        "            else:\n",
        "                # This case should ideally not be reached with the defined models\n",
        "                return {'loss': float('inf'), 'status': 'fail', 'exception': 'Unknown model type'}\n",
        "\n",
        "            # Cross-validation\n",
        "            scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "            return {'loss': -np.mean(scores), 'status': 'ok'}\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch any exception during model creation or cross-validation and return a large loss\n",
        "            print(f\"Optimization trial failed for {model_type} with params {params}: {e}\")\n",
        "            return {'loss': float('inf'), 'status': 'fail', 'exception': str(e)}\n",
        "\n",
        "    def hyperopt_optimization(self):\n",
        "        \"\"\"Hyperparameter optimization using Hyperopt\"\"\"\n",
        "        print(\"\\nStarting Hyperopt Hyperparameter Optimization...\")\n",
        "\n",
        "        # Define hyperopt parameter spaces\n",
        "        param_spaces = {\n",
        "            'SVM': {\n",
        "                'C': hp.uniform('C', 0.1, 100),\n",
        "                'gamma': hp.uniform('gamma', 0.001, 1),\n",
        "                'epsilon': hp.uniform('epsilon', 0.01, 1)\n",
        "            },\n",
        "            'Random Forest': {\n",
        "                'n_estimators': scope.int(hp.quniform('n_estimators', 50, 200, 25)),\n",
        "                'max_depth': scope.int(hp.quniform('max_depth', 5, 20, 1)),\n",
        "                'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
        "                'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 5, 1))\n",
        "            },\n",
        "            'Decision Tree': {\n",
        "                'max_depth': scope.int(hp.quniform('max_depth', 5, 20, 1)),\n",
        "                'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 20, 1)),\n",
        "                'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1))\n",
        "            },\n",
        "            'Extra Trees': {\n",
        "                'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 25)),\n",
        "                'max_depth': scope.int(hp.quniform('max_depth', 5, 25, 1)),\n",
        "                'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 15, 1)),\n",
        "                'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 8, 1))\n",
        "            },\n",
        "            'MLP': {\n",
        "                'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(50,), (100,), (50, 25)]),\n",
        "                'alpha': hp.uniform('alpha', 0.0001, 0.01),\n",
        "                'learning_rate_init': hp.uniform('learning_rate_init', 0.001, 0.1)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Add XGBoost if available\n",
        "        if XGBOOST_AVAILABLE:\n",
        "            param_spaces['XGBoost'] = {\n",
        "                'n_estimators': scope.int(hp.quniform('n_estimators', 50, 200, 25)),\n",
        "                'max_depth': scope.int(hp.quniform('max_depth', 3, 10, 1)),\n",
        "                'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "                'subsample': hp.uniform('subsample', 0.5, 1.0)\n",
        "            }\n",
        "\n",
        "        for model_name, param_space in param_spaces.items():\n",
        "            print(f\"\\nOptimizing {model_name} with Hyperopt...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Create objective function\n",
        "            objective = partial(\n",
        "                self.hyperopt_objective,\n",
        "                model_type=model_name,\n",
        "                X=self.X_train_scaled,\n",
        "                y=self.y_train\n",
        "            )\n",
        "\n",
        "            # Run optimization\n",
        "            trials = Trials()\n",
        "            try:\n",
        "                best = fmin(\n",
        "                    fn=objective,\n",
        "                    space=param_space,\n",
        "                    algo=tpe.suggest,\n",
        "                    max_evals=50,\n",
        "                    trials=trials,\n",
        "                    verbose=False\n",
        "                )\n",
        "            except Exception as e:\n",
        "                 print(f\"Hyperopt optimization failed for {model_name}: {e}\")\n",
        "                 continue # Move to the next model if optimization fails\n",
        "\n",
        "            # Get best parameters from successful trials\n",
        "            successful_trials = [trial for trial in trials.trials if trial['result']['status'] == 'ok' and 'loss' in trial['result']]\n",
        "\n",
        "            if not successful_trials:\n",
        "                print(f\"No successful trials for {model_name}. Skipping evaluation.\")\n",
        "                continue # Move to the next model if no successful trials\n",
        "\n",
        "            # Find the trial with the minimum loss among successful trials\n",
        "            best_trial = min(successful_trials, key=lambda x: x['result']['loss'])\n",
        "            best_params = space_eval(param_space, best_trial['misc']['vals'])\n",
        "            best_loss = best_trial['result']['loss']\n",
        "\n",
        "\n",
        "            # Train best model and evaluate\n",
        "            if model_name == 'SVM':\n",
        "                best_model = SVR(**best_params)\n",
        "            elif model_name == 'Random Forest':\n",
        "                best_model = RandomForestRegressor(random_state=42, **best_params)\n",
        "            elif model_name == 'Decision Tree':\n",
        "                best_model = DecisionTreeRegressor(random_state=42, **best_params)\n",
        "            elif model_name == 'XGBoost' and XGBOOST_AVAILABLE:\n",
        "                best_model = xgb.XGBRegressor(random_state=42, **best_params)\n",
        "            elif model_name == 'MLP':\n",
        "                best_model = MLPRegressor(random_state=42, max_iter=500, **best_params)\n",
        "            elif model_name == 'Extra Trees':\n",
        "                best_model = ExtraTreesRegressor(random_state=42, **best_params)\n",
        "            else:\n",
        "                best_model = None # Should not happen with defined models\n",
        "\n",
        "            if best_model:\n",
        "                best_model.fit(self.X_train_scaled, self.y_train)\n",
        "                y_pred = best_model.predict(self.X_test_scaled)\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics = self.calculate_metrics(self.y_test, y_pred)\n",
        "                execution_time = time.time() - start_time\n",
        "\n",
        "                # Store results\n",
        "                result = {\n",
        "                    'Model': model_name,\n",
        "                    'Method': 'Hyperopt',\n",
        "                    'Best_Params': best_params,\n",
        "                    'CV_Score': best_loss,\n",
        "                    'Execution_Time': execution_time,\n",
        "                    **metrics\n",
        "                }\n",
        "\n",
        "                self.hyperopt_results.append(result)\n",
        "                print(f\"{model_name} completed. Best Score: {result['CV_Score']:.4f}\")\n",
        "            else:\n",
        "                print(f\"Could not train the best model for {model_name}.\")\n",
        "\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save optimization results to CSV files\"\"\"\n",
        "        # Combine all results\n",
        "        all_results = self.custom_results + self.hyperopt_results\n",
        "\n",
        "        # Create performance metrics DataFrame\n",
        "        performance_columns = ['Model', 'Method', 'MSE', 'MAE', 'RMSE', 'MAPE', 'R2', 'AIC', 'BIC', 'CV_Score', 'Execution_Time']\n",
        "        performance_data = []\n",
        "\n",
        "        hyperparams_data = []\n",
        "\n",
        "        for result in all_results:\n",
        "            # Performance metrics\n",
        "            perf_row = {col: result.get(col, 0) for col in performance_columns}\n",
        "            performance_data.append(perf_row)\n",
        "\n",
        "            # Hyperparameters\n",
        "            hyperparam_row = {\n",
        "                'Model': result['Model'],\n",
        "                'Method': result['Method'],\n",
        "                **result['Best_Params']\n",
        "            }\n",
        "            hyperparams_data.append(hyperparam_row)\n",
        "\n",
        "        # Create DataFrames\n",
        "        performance_df = pd.DataFrame(performance_data)\n",
        "        hyperparams_df = pd.DataFrame(hyperparams_data)\n",
        "\n",
        "        # Save to CSV\n",
        "        performance_df.to_csv('performance_metrics.csv', index=False)\n",
        "        hyperparams_df.to_csv('optimal_hyperparameters.csv', index=False)\n",
        "\n",
        "        print(\"\\nResults saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\")\n",
        "\n",
        "        # Display summary\n",
        "        print(\"\\nPerformance Summary (Top 5 by R2 Score):\")\n",
        "        print(performance_df.nlargest(5, 'R2')[['Model', 'Method', 'R2', 'RMSE', 'Execution_Time']].to_string(index=False))\n",
        "\n",
        "        return performance_df, hyperparams_df\n",
        "\n",
        "    def plot_results(self):\n",
        "        \"\"\"Create visualization of optimization results\"\"\"\n",
        "        all_results = self.custom_results + self.hyperopt_results\n",
        "        df_results = pd.DataFrame(all_results)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('Hyperparameter Optimization Results Comparison', fontsize=16)\n",
        "\n",
        "        # R2 Score comparison\n",
        "        sns.barplot(data=df_results, x='Model', y='R2', hue='Method', ax=axes[0,0])\n",
        "        axes[0,0].set_title('R² Score by Model and Method')\n",
        "        axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # RMSE comparison\n",
        "        sns.barplot(data=df_results, x='Model', y='RMSE', hue='Method', ax=axes[0,1])\n",
        "        axes[0,1].set_title('RMSE by Model and Method')\n",
        "        axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Execution time comparison\n",
        "        sns.barplot(data=df_results, x='Model', y='Execution_Time', hue='Method', ax=axes[1,0])\n",
        "        axes[1,0].set_title('Execution Time by Model and Method')\n",
        "        axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # MAE vs R2 scatter\n",
        "        sns.scatterplot(data=df_results, x='MAE', y='R2', hue='Method', style='Model', ax=axes[1,1])\n",
        "        axes[1,1].set_title('MAE vs R² Score')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('hyperparameter_optimization_results.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Load your data here - replace with your actual data loading\n",
        "    # For this example, I'll assume you have your DataFrame ready\n",
        "\n",
        "    # Initialize optimizer with your data\n",
        "    # optimizer = NanofluidDensityOptimizer(data_path='Density_Prediction_Dataset.csv')\n",
        "    # Or if you have df already loaded:\n",
        "    # optimizer = NanofluidDensityOptimizer(df=df)\n",
        "\n",
        "    print(\"Nanofluid Density Prediction - Hyperparameter Optimization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Assuming df is your loaded DataFrame (replace with actual loading)\n",
        "    # optimizer = NanofluidDensityOptimizer(df=df)\n",
        "\n",
        "    # For demonstration, create a sample run\n",
        "    print(\"Please load your DataFrame and run:\")\n",
        "    print(\"optimizer = NanofluidDensityOptimizer(df=your_dataframe)\")\n",
        "    print(\"optimizer.preprocess_data()\")\n",
        "    print(\"optimizer.custom_scoring_optimization()\")\n",
        "    print(\"optimizer.hyperopt_optimization()\") # Removed this line to avoid TypeError\n",
        "    print(\"performance_df, hyperparams_df = optimizer.save_results()\")\n",
        "    print(\"optimizer.plot_results()\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage (uncomment and modify for your use case):\n",
        "\n",
        "    # Load your data\n",
        "    df = pd.read_csv('Density_Prediction_Dataset.csv')\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = NanofluidDensityOptimizer(df=df)\n",
        "\n",
        "    # Preprocess data\n",
        "    optimizer.preprocess_data()\n",
        "\n",
        "    # Run custom scoring optimization (Hyperopt is skipped due to persistent error)\n",
        "    optimizer.custom_scoring_optimization()\n",
        "    # optimizer.hyperopt_optimization() # Commented out due to TypeError\n",
        "\n",
        "    # Save results and create visualizations\n",
        "    performance_df, hyperparams_df = optimizer.save_results()\n",
        "    optimizer.plot_results()\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HtT0tiLrWHHs",
        "outputId": "28dc6d8b-7c7b-4ae0-8e7a-5c741aed003a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessed. Training set: (348, 9), Test set: (88, 9)\n",
            "Starting Custom Scoring Hyperparameter Optimization...\n",
            "\n",
            "Optimizing SVM...\n",
            "SVM completed. Best CV Score: 24.5654\n",
            "\n",
            "Optimizing Random Forest...\n",
            "Random Forest completed. Best CV Score: 13.7249\n",
            "\n",
            "Optimizing Decision Tree...\n",
            "Decision Tree completed. Best CV Score: 22.2873\n",
            "\n",
            "Optimizing Extra Trees...\n",
            "Extra Trees completed. Best CV Score: 11.8708\n",
            "\n",
            "Optimizing MLP...\n",
            "MLP completed. Best CV Score: 102.9596\n",
            "\n",
            "Optimizing XGBoost...\n",
            "XGBoost completed. Best CV Score: 4.4410\n",
            "\n",
            "Results saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\n",
            "\n",
            "Performance Summary (Top 5 by R2 Score):\n",
            "        Model         Method       R2     RMSE  Execution_Time\n",
            "      XGBoost Custom Scoring 0.988073 2.759779        5.768738\n",
            "  Extra Trees Custom Scoring 0.974948 3.999746       20.975993\n",
            "Random Forest Custom Scoring 0.965259 4.710151       24.629552\n",
            "Decision Tree Custom Scoring 0.955224 5.347298        0.423585\n",
            "          SVM Custom Scoring 0.941941 6.088991        1.184253\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1200 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nanofluid Density Prediction - Hyperparameter Optimization\n",
            "============================================================\n",
            "Please load your DataFrame and run:\n",
            "optimizer = NanofluidDensityOptimizer(df=your_dataframe)\n",
            "optimizer.preprocess_data()\n",
            "optimizer.custom_scoring_optimization()\n",
            "optimizer.hyperopt_optimization()\n",
            "performance_df, hyperparams_df = optimizer.save_results()\n",
            "optimizer.plot_results()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4XM_njoWHKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSRz-BuJZRJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBbXLDhcZRPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMqNtG5bZRSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OylJ6eSgZRVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdQXIZktZRYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRU6nT2LZRbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRib-_hoZReo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMSfAfJgZRhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DWnSc-9xZRk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKKXxKC4ZRne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzG0PIVxWHMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YL4t3oXuWHPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTyCu0Z7UoN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CpMfIJHlUoRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0uz8mcxIUoTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "id": "0e9qf40s8jMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e2e278-3a4d-4cca-edac-8ff5ec8d32e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deap) (2.0.2)\n",
            "Downloading deap-1.4.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap\n",
            "Successfully installed deap-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from deap import base, creator, tools, algorithms\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Load and preprocess the dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Remove outliers using IQR method\n",
        "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numerical_columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "\n",
        "    # Label encoding for categorical variables\n",
        "    le = LabelEncoder()\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_columns:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop('Density (ρ)', axis=1)\n",
        "    y = df['Density (ρ)']\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# Genetic Algorithm for hyperparameter optimization\n",
        "def evaluate_hyperparameters(individual, model_type, X, y):\n",
        "    \"\"\"Evaluate hyperparameters using cross-validation\"\"\"\n",
        "    try:\n",
        "        # Decode individual to hyperparameters\n",
        "        params = decode_individual(individual, model_type)\n",
        "\n",
        "        # Create model with decoded parameters\n",
        "        model = create_model(model_type, params)\n",
        "\n",
        "        # 5-fold cross-validation\n",
        "        cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "        # Return negative MSE as fitness (higher is better)\n",
        "        return np.mean(cv_scores),\n",
        "    except:\n",
        "        return -np.inf,\n",
        "\n",
        "def decode_individual(individual, model_type):\n",
        "    \"\"\"Decode individual to hyperparameters\"\"\"\n",
        "    params = {}\n",
        "\n",
        "    if model_type == 'SVM':\n",
        "        params['C'] = 0.1 + individual[0] * (100 - 0.1)\n",
        "        params['gamma'] = 0.001 + individual[1] * (1 - 0.001)\n",
        "        params['epsilon'] = 0.01 + individual[2] * (1 - 0.01)\n",
        "\n",
        "    elif model_type == 'Decision Tree':\n",
        "        params['max_depth'] = int(5 + individual[0] * (20 - 5))\n",
        "        params['min_samples_split'] = int(2 + individual[1] * (20 - 2))\n",
        "        params['min_samples_leaf'] = int(1 + individual[2] * (10 - 1))\n",
        "        params['max_features'] = 0.1 + individual[3] * (1.0 - 0.1)\n",
        "\n",
        "    elif model_type == 'Extra Trees':\n",
        "        params['n_estimators'] = int(50 + individual[0] * (300 - 50))\n",
        "        params['max_depth'] = int(5 + individual[1] * (25 - 5))\n",
        "        params['min_samples_split'] = int(2 + individual[2] * (15 - 2))\n",
        "        params['min_samples_leaf'] = int(1 + individual[3] * (8 - 1))\n",
        "        params['max_features'] = 0.1 + individual[4] * (1.0 - 0.1)\n",
        "\n",
        "    elif model_type == 'Random Forest':\n",
        "        params['n_estimators'] = int(50 + individual[0] * (200 - 50))\n",
        "        params['max_depth'] = int(5 + individual[1] * (20 - 5))\n",
        "        params['min_samples_split'] = int(2 + individual[2] * (10 - 2))\n",
        "        params['min_samples_leaf'] = int(1 + individual[3] * (5 - 1))\n",
        "\n",
        "    elif model_type == 'XGBoost':\n",
        "        params['n_estimators'] = int(50 + individual[0] * (200 - 50))\n",
        "        params['max_depth'] = int(3 + individual[1] * (10 - 3))\n",
        "        params['learning_rate'] = 0.01 + individual[2] * (0.3 - 0.01)\n",
        "        params['subsample'] = 0.5 + individual[3] * (1.0 - 0.5)\n",
        "\n",
        "    elif model_type == 'MLP':\n",
        "        hidden_size = int(50 + individual[0] * (200 - 50))\n",
        "        params['hidden_layer_sizes'] = (hidden_size,)\n",
        "        params['alpha'] = 0.0001 + individual[1] * (0.01 - 0.0001)\n",
        "        params['learning_rate_init'] = 0.001 + individual[2] * (0.1 - 0.001)\n",
        "        params['max_iter'] = 500\n",
        "\n",
        "    return params\n",
        "\n",
        "def create_model(model_type, params):\n",
        "    \"\"\"Create model with given parameters\"\"\"\n",
        "    if model_type == 'SVM':\n",
        "        return SVR(**params)\n",
        "    elif model_type == 'Decision Tree':\n",
        "        return DecisionTreeRegressor(**params, random_state=42)\n",
        "    elif model_type == 'Extra Trees':\n",
        "        return ExtraTreesRegressor(**params, random_state=42)\n",
        "    elif model_type == 'Random Forest':\n",
        "        return RandomForestRegressor(**params, random_state=42)\n",
        "    elif model_type == 'XGBoost':\n",
        "        return xgb.XGBRegressor(**params, random_state=42)\n",
        "    elif model_type == 'MLP':\n",
        "        return MLPRegressor(**params, random_state=42)\n",
        "\n",
        "def genetic_algorithm_optimization(model_type, X, y, n_population=30, n_generation=20):\n",
        "    \"\"\"Optimize hyperparameters using genetic algorithm\"\"\"\n",
        "\n",
        "    # Determine number of parameters\n",
        "    param_counts = {\n",
        "        'SVM': 3, 'Decision Tree': 4, 'Extra Trees': 5,\n",
        "        'Random Forest': 4, 'XGBoost': 4, 'MLP': 3\n",
        "    }\n",
        "    n_params = param_counts[model_type]\n",
        "\n",
        "    # Reset creator\n",
        "    for name in ['FitnessMax', 'Individual']:\n",
        "        if hasattr(creator, name):\n",
        "            delattr(creator, name)\n",
        "\n",
        "    # Create fitness function and individual\n",
        "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "    # Create toolbox\n",
        "    toolbox = base.Toolbox()\n",
        "    toolbox.register(\"attr_float\", random.random)\n",
        "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n_params)\n",
        "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "    toolbox.register(\"evaluate\", evaluate_hyperparameters, model_type=model_type, X=X, y=y)\n",
        "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)\n",
        "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "    # Initialize population\n",
        "    pop = toolbox.population(n=n_population)\n",
        "    hof = tools.HallOfFame(1)\n",
        "\n",
        "    # Run genetic algorithm\n",
        "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2,\n",
        "                                   ngen=n_generation, stats=None, halloffame=hof,\n",
        "                                   verbose=False)\n",
        "\n",
        "    # Get best individual\n",
        "    best_individual = hof[0]\n",
        "    best_params = decode_individual(best_individual, model_type)\n",
        "\n",
        "    return best_params, best_individual.fitness.values[0]\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate performance metrics\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # MAPE\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    # AIC and BIC (simplified versions)\n",
        "    n = len(y_true)\n",
        "    aic = n * np.log(mse) + 2 * 1  # Simplified for regression\n",
        "    bic = n * np.log(mse) + np.log(n) * 1\n",
        "\n",
        "    # COD (Coefficient of Determination)\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'MAPE': mape,\n",
        "        'AIC': aic, 'BIC': bic, 'COD': cod, 'R2': r2\n",
        "    }\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    file_path = '/content/Density_Prediction_Dataset.csv'\n",
        "    X, y = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Models to optimize\n",
        "    models = ['SVM', 'Decision Tree', 'Extra Trees', 'Random Forest', 'XGBoost', 'MLP']\n",
        "\n",
        "    # Results storage\n",
        "    performance_results = []\n",
        "    hyperparameter_results = []\n",
        "\n",
        "    print(\"Starting Genetic Algorithm Hyperparameter Optimization...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Optimize each model\n",
        "    for model_name in models:\n",
        "        print(f\"\\nOptimizing {model_name}...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Genetic algorithm optimization\n",
        "        best_params, best_fitness = genetic_algorithm_optimization(\n",
        "            model_name, X_train, y_train, n_population=30, n_generation=20\n",
        "        )\n",
        "\n",
        "        # Train final model with best parameters\n",
        "        final_model = create_model(model_name, best_params)\n",
        "        final_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = final_model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "\n",
        "        # Store results\n",
        "        performance_result = {\n",
        "            'Model': model_name,\n",
        "            'MSE': metrics['MSE'],\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'AIC': metrics['AIC'],\n",
        "            'BIC': metrics['BIC'],\n",
        "            'COD': metrics['COD'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Execution_Time': execution_time\n",
        "        }\n",
        "        performance_results.append(performance_result)\n",
        "\n",
        "        # Store hyperparameters\n",
        "        hyperparameter_result = {'Model': model_name}\n",
        "        hyperparameter_result.update(best_params)\n",
        "        hyperparameter_results.append(hyperparameter_result)\n",
        "\n",
        "        print(f\"Best R2 Score: {metrics['R2']:.4f}\")\n",
        "        print(f\"Best Parameters: {best_params}\")\n",
        "        print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Create DataFrames\n",
        "    performance_df = pd.DataFrame(performance_results)\n",
        "    hyperparameter_df = pd.DataFrame(hyperparameter_results)\n",
        "\n",
        "    # Sort by R2 score (descending)\n",
        "    performance_df = performance_df.sort_values('R2', ascending=False)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nPerformance Metrics (sorted by R2 score):\")\n",
        "    print(performance_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\nOptimal Hyperparameters:\")\n",
        "    print(hyperparameter_df.to_string(index=False))\n",
        "\n",
        "    # Save results to CSV files\n",
        "    performance_df.to_csv('GA_performance_metrics.csv', index=False)\n",
        "    hyperparameter_df.to_csv('GA_optimal_hyperparameters.csv', index=False)\n",
        "\n",
        "    print(\"\\nResults saved to:\")\n",
        "    print(\"- GA_performance_metrics.csv\")\n",
        "    print(\"- GA_optimal_hyperparameters.csv\")"
      ],
      "metadata": {
        "id": "dmhDTaEA8pi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693444ec-8086-4ae7-e57a-129fb26be41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Genetic Algorithm Hyperparameter Optimization...\n",
            "============================================================\n",
            "\n",
            "Optimizing SVM...\n",
            "Best R2 Score: 0.9939\n",
            "Best Parameters: {'C': 135.7234428629581, 'gamma': 0.362627437577259, 'epsilon': 0.010093951333055077}\n",
            "Execution Time: 62.20 seconds\n",
            "----------------------------------------\n",
            "\n",
            "Optimizing Decision Tree...\n",
            "Best R2 Score: 0.8578\n",
            "Best Parameters: {'max_depth': 22, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.7252980685899402}\n",
            "Execution Time: 7.60 seconds\n",
            "----------------------------------------\n",
            "\n",
            "Optimizing Extra Trees...\n",
            "Best R2 Score: 0.9709\n",
            "Best Parameters: {'n_estimators': 102, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.551989153573097}\n",
            "Execution Time: 231.27 seconds\n",
            "----------------------------------------\n",
            "\n",
            "Optimizing Random Forest...\n",
            "Best R2 Score: 0.9624\n",
            "Best Parameters: {'n_estimators': 66, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
            "Execution Time: 196.52 seconds\n",
            "----------------------------------------\n",
            "\n",
            "Optimizing XGBoost...\n",
            "Best R2 Score: 0.9924\n",
            "Best Parameters: {'n_estimators': 123, 'max_depth': 3, 'learning_rate': 0.22388230267188, 'subsample': 0.7155775824474986}\n",
            "Execution Time: 93.21 seconds\n",
            "----------------------------------------\n",
            "\n",
            "Optimizing MLP...\n",
            "Best R2 Score: -2.6383\n",
            "Best Parameters: {'hidden_layer_sizes': (72,), 'alpha': 0.007126359575945757, 'learning_rate_init': 0.07963240470044945, 'max_iter': 500}\n",
            "Execution Time: 473.29 seconds\n",
            "----------------------------------------\n",
            "\n",
            "============================================================\n",
            "FINAL RESULTS\n",
            "============================================================\n",
            "\n",
            "Performance Metrics (sorted by R2 score):\n",
            "        Model        MSE       MAE      RMSE     MAPE        AIC        BIC       COD        R2  Execution_Time\n",
            "          SVM   0.933902  0.709842  0.966386 0.069810  -0.872114   0.865555  0.993929  0.993929       62.202472\n",
            "      XGBoost   1.165646  0.904120  1.079651 0.088749   8.437557  10.175227  0.992422  0.992422       93.205695\n",
            "  Extra Trees   4.482171  1.624938  2.117114 0.160430  65.004516  66.742186  0.970861  0.970861      231.269672\n",
            "Random Forest   5.785667  1.846091  2.405341 0.182047  75.726116  77.463786  0.962387  0.962387      196.518855\n",
            "Decision Tree  21.868143  3.655813  4.676339 0.360751 131.571299 133.308968  0.857835  0.857835        7.602673\n",
            "          MLP 559.650727 12.759478 23.656938 1.262194 267.747141 269.484811 -2.638284 -2.638284      473.293537\n",
            "\n",
            "Optimal Hyperparameters:\n",
            "        Model          C    gamma  epsilon  max_depth  min_samples_split  min_samples_leaf  max_features  n_estimators  learning_rate  subsample hidden_layer_sizes    alpha  learning_rate_init  max_iter\n",
            "          SVM 135.723443 0.362627 0.010094        NaN                NaN               NaN           NaN           NaN            NaN        NaN                NaN      NaN                 NaN       NaN\n",
            "Decision Tree        NaN      NaN      NaN       22.0                5.0               2.0      0.725298           NaN            NaN        NaN                NaN      NaN                 NaN       NaN\n",
            "  Extra Trees        NaN      NaN      NaN       15.0                2.0               1.0      0.551989         102.0            NaN        NaN                NaN      NaN                 NaN       NaN\n",
            "Random Forest        NaN      NaN      NaN       17.0                2.0               1.0           NaN          66.0            NaN        NaN                NaN      NaN                 NaN       NaN\n",
            "      XGBoost        NaN      NaN      NaN        3.0                NaN               NaN           NaN         123.0       0.223882   0.715578                NaN      NaN                 NaN       NaN\n",
            "          MLP        NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN            NaN        NaN              (72,) 0.007126            0.079632     500.0\n",
            "\n",
            "Results saved to:\n",
            "- GA_performance_metrics.csv\n",
            "- GA_optimal_hyperparameters.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38Bl0V-y8pl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simulated Annealing**"
      ],
      "metadata": {
        "id": "-aY6hwbf7Xkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Load and preprocess the dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Remove outliers using IQR method\n",
        "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numerical_columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "\n",
        "    # Label encoding for categorical variables\n",
        "    le = LabelEncoder()\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_columns:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop('Density (ρ)', axis=1)\n",
        "    y = df['Density (ρ)']\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "# Define hyperparameter search spaces\n",
        "def get_param_space(model_type):\n",
        "    \"\"\"Get hyperparameter search space for each model\"\"\"\n",
        "    param_spaces = {\n",
        "        'SVM': OrderedDict([\n",
        "            ('C', np.logspace(-1, 2, 10).tolist()),\n",
        "            ('gamma', np.logspace(-3, 0, 10).tolist()),\n",
        "            ('epsilon', np.logspace(-2, 0, 10).tolist())\n",
        "        ]),\n",
        "        'Decision Tree': OrderedDict([\n",
        "            ('max_depth', list(range(5, 21))),\n",
        "            ('min_samples_split', list(range(2, 21))),\n",
        "            ('min_samples_leaf', list(range(1, 11))),\n",
        "            ('max_features', np.linspace(0.1, 1.0, 10).tolist())\n",
        "        ]),\n",
        "        'Extra Trees': OrderedDict([\n",
        "            ('n_estimators', list(range(50, 301, 25))),\n",
        "            ('max_depth', list(range(5, 26))),\n",
        "            ('min_samples_split', list(range(2, 16))),\n",
        "            ('min_samples_leaf', list(range(1, 9))),\n",
        "            ('max_features', np.linspace(0.1, 1.0, 10).tolist())\n",
        "        ]),\n",
        "        'Random Forest': OrderedDict([\n",
        "            ('n_estimators', list(range(50, 201, 25))),\n",
        "            ('max_depth', list(range(5, 21))),\n",
        "            ('min_samples_split', list(range(2, 11))),\n",
        "            ('min_samples_leaf', list(range(1, 6)))\n",
        "        ]),\n",
        "        'XGBoost': OrderedDict([\n",
        "            ('n_estimators', list(range(50, 201, 25))),\n",
        "            ('max_depth', list(range(3, 11))),\n",
        "            ('learning_rate', np.linspace(0.01, 0.3, 10).tolist()),\n",
        "            ('subsample', np.linspace(0.5, 1.0, 6).tolist())\n",
        "        ]),\n",
        "        'MLP': OrderedDict([\n",
        "            ('hidden_layer_sizes', list(range(50, 201, 25))),\n",
        "            ('alpha', np.logspace(-4, -2, 10).tolist()),\n",
        "            ('learning_rate_init', np.logspace(-3, -1, 10).tolist())\n",
        "        ])\n",
        "    }\n",
        "    return param_spaces[model_type]\n",
        "\n",
        "# Model training function\n",
        "def train_model(curr_params, const_param, X_train, X_valid, y_train, y_valid, model_type):\n",
        "    \"\"\"Train model with given hyperparameters\"\"\"\n",
        "    try:\n",
        "        params_copy = const_param.copy()\n",
        "        params_copy.update(curr_params)\n",
        "\n",
        "        if model_type == 'SVM':\n",
        "            model = SVR(**params_copy)\n",
        "        elif model_type == 'Decision Tree':\n",
        "            model = DecisionTreeRegressor(**params_copy, random_state=42)\n",
        "        elif model_type == 'Extra Trees':\n",
        "            model = ExtraTreesRegressor(**params_copy, random_state=42)\n",
        "        elif model_type == 'Random Forest':\n",
        "            model = RandomForestRegressor(**params_copy, random_state=42)\n",
        "        elif model_type == 'XGBoost':\n",
        "            model = xgb.XGBRegressor(**params_copy, random_state=42)\n",
        "        elif model_type == 'MLP':\n",
        "            if 'hidden_layer_sizes' in params_copy:\n",
        "                params_copy['hidden_layer_sizes'] = (params_copy['hidden_layer_sizes'],)\n",
        "            params_copy['max_iter'] = 500\n",
        "            model = MLPRegressor(**params_copy, random_state=42)\n",
        "\n",
        "        # Use cross-validation for evaluation\n",
        "        combined_X = pd.concat([X_train, X_valid])\n",
        "        combined_y = pd.concat([y_train, y_valid])\n",
        "\n",
        "        cv_scores = cross_val_score(model, combined_X, combined_y, cv=5, scoring='r2')\n",
        "        metric_val = np.mean(cv_scores)\n",
        "\n",
        "        # Train on full training data\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        return model, metric_val\n",
        "    except:\n",
        "        return None, -np.inf\n",
        "\n",
        "# Parameter selection function\n",
        "def choose_params(param_dict, curr_params=None):\n",
        "    \"\"\"Function to choose parameters for next iteration\"\"\"\n",
        "    if curr_params:\n",
        "        next_params = curr_params.copy()\n",
        "        param_to_update = np.random.choice(list(param_dict.keys()))\n",
        "        param_vals = param_dict[param_to_update]\n",
        "        curr_index = param_vals.index(curr_params[param_to_update])\n",
        "\n",
        "        if curr_index == 0:\n",
        "            next_params[param_to_update] = param_vals[1]\n",
        "        elif curr_index == len(param_vals) - 1:\n",
        "            next_params[param_to_update] = param_vals[curr_index - 1]\n",
        "        else:\n",
        "            next_params[param_to_update] = param_vals[curr_index + np.random.choice([-1, 1])]\n",
        "    else:\n",
        "        next_params = dict()\n",
        "        for k, v in param_dict.items():\n",
        "            next_params[k] = np.random.choice(v)\n",
        "\n",
        "    return next_params\n",
        "\n",
        "# Simulated Annealing implementation\n",
        "def simulate_annealing(param_dict, const_param, X_train, X_valid, y_train, y_valid,\n",
        "                      model_type, maxiters=50, alpha=0.85, beta=1.3, T_0=0.40, update_iters=5):\n",
        "    \"\"\"Simulated Annealing optimization\"\"\"\n",
        "    best_metric = -np.inf\n",
        "    prev_metric = -np.inf\n",
        "    prev_params = None\n",
        "    best_params = dict()\n",
        "    best_model = None\n",
        "    weights = list(map(lambda x: 10**x, list(range(len(param_dict)))))\n",
        "    hash_values = set()\n",
        "    T = T_0\n",
        "\n",
        "    for i in range(maxiters):\n",
        "        print(f'Starting Iteration {i}')\n",
        "\n",
        "        while True:\n",
        "            curr_params = choose_params(param_dict, prev_params)\n",
        "            indices = [param_dict[k].index(v) for k, v in curr_params.items()]\n",
        "            hash_val = sum([i * j for (i, j) in zip(weights, indices)])\n",
        "            if hash_val in hash_values:\n",
        "                print('Combination revisited')\n",
        "            else:\n",
        "                hash_values.add(hash_val)\n",
        "                break\n",
        "\n",
        "        model, metric = train_model(curr_params, const_param, X_train, X_valid,\n",
        "                                   y_train, y_valid, model_type)\n",
        "\n",
        "        if metric > prev_metric:\n",
        "            print(f'Local Improvement in metric from {prev_metric:.4f} to {metric:.4f} - parameters accepted')\n",
        "            prev_params = curr_params.copy()\n",
        "            prev_metric = metric\n",
        "\n",
        "            if metric > best_metric:\n",
        "                print(f'Global improvement in metric from {best_metric:.4f} to {metric:.4f} - best parameters updated')\n",
        "                best_metric = metric\n",
        "                best_params = curr_params.copy()\n",
        "                best_model = model\n",
        "        else:\n",
        "            rnd = np.random.uniform()\n",
        "            diff = metric - prev_metric\n",
        "            threshold = np.exp(beta * diff / T)\n",
        "            if rnd < threshold:\n",
        "                print(f'No Improvement but parameters accepted. Metric change: {diff:.4f} threshold: {threshold:.4f} random number: {rnd:.4f}')\n",
        "                prev_metric = metric\n",
        "                prev_params = curr_params\n",
        "            else:\n",
        "                print(f'No Improvement and parameters rejected. Metric change: {diff:.4f} threshold: {threshold:.4f} random number: {rnd:.4f}')\n",
        "\n",
        "        if i % update_iters == 0:\n",
        "            T = alpha * T\n",
        "\n",
        "    return best_params, best_model\n",
        "\n",
        "# Performance metrics calculation\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate performance metrics\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # MAPE\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    # AIC and BIC (simplified versions)\n",
        "    n = len(y_true)\n",
        "    aic = n * np.log(mse) + 2 * 1\n",
        "    bic = n * np.log(mse) + np.log(n) * 1\n",
        "\n",
        "    # COD (Coefficient of Determination)\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'MAPE': mape,\n",
        "        'AIC': aic, 'BIC': bic, 'COD': cod, 'R2': r2\n",
        "    }\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    file_path = '/content/Density_Prediction_Dataset.csv'\n",
        "    X, y = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Models to optimize\n",
        "    models = ['SVM', 'Decision Tree', 'Extra Trees', 'Random Forest', 'XGBoost', 'MLP']\n",
        "\n",
        "    # Results storage\n",
        "    performance_results = []\n",
        "    hyperparameter_results = []\n",
        "\n",
        "    print(\"Starting Simulated Annealing Hyperparameter Optimization...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Optimize each model\n",
        "    for model_name in models:\n",
        "        print(f\"\\nOptimizing {model_name}...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Get parameter space\n",
        "        param_space = get_param_space(model_name)\n",
        "        const_param = {}\n",
        "\n",
        "        # Simulated Annealing optimization\n",
        "        best_params, best_model = simulate_annealing(\n",
        "            param_space, const_param, X_train, X_valid, y_train, y_valid,\n",
        "            model_name, maxiters=50, alpha=0.85, beta=1.3, T_0=0.40, update_iters=5\n",
        "        )\n",
        "\n",
        "        # Train final model on full training data\n",
        "        X_train_full = pd.concat([X_train, X_valid])\n",
        "        y_train_full = pd.concat([y_train, y_valid])\n",
        "\n",
        "        if best_model:\n",
        "            best_model.fit(X_train_full, y_train_full)\n",
        "            y_pred = best_model.predict(X_test)\n",
        "        else:\n",
        "            y_pred = np.full(len(y_test), np.mean(y_test))\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "\n",
        "        # Store results\n",
        "        performance_result = {\n",
        "            'Model': model_name,\n",
        "            'MSE': metrics['MSE'],\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'AIC': metrics['AIC'],\n",
        "            'BIC': metrics['BIC'],\n",
        "            'COD': metrics['COD'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Execution_Time': execution_time\n",
        "        }\n",
        "        performance_results.append(performance_result)\n",
        "\n",
        "        # Store hyperparameters\n",
        "        hyperparameter_result = {'Model': model_name}\n",
        "        hyperparameter_result.update(best_params)\n",
        "        hyperparameter_results.append(hyperparameter_result)\n",
        "\n",
        "        print(f\"Best R2 Score: {metrics['R2']:.4f}\")\n",
        "        print(f\"Best Parameters: {best_params}\")\n",
        "        print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Create DataFrames\n",
        "    performance_df = pd.DataFrame(performance_results)\n",
        "    hyperparameter_df = pd.DataFrame(hyperparameter_results)\n",
        "\n",
        "    # Sort by R2 score (descending)\n",
        "    performance_df = performance_df.sort_values('R2', ascending=False)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nPerformance Metrics (sorted by R2 score):\")\n",
        "    print(performance_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\nOptimal Hyperparameters:\")\n",
        "    print(hyperparameter_df.to_string(index=False))\n",
        "\n",
        "    # Save results to CSV files\n",
        "    performance_df.to_csv('SA_performance_metrics.csv', index=False)\n",
        "    hyperparameter_df.to_csv('SA_optimal_hyperparameters.csv', index=False)\n",
        "\n",
        "    print(\"\\nResults saved to:\")\n",
        "    print(\"- SA_performance_metrics.csv\")\n",
        "    print(\"- SA_optimal_hyperparameters.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ks1k-b37aPI",
        "outputId": "bff2d9d2-cabd-4303-dc77-879f1c25385d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n",
            "Combination revisited\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1C51pVb07aR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYKYG10E7aUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bf--wL4X7aXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SSA**"
      ],
      "metadata": {
        "id": "6Qzy3UVnkJ9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Density_Prediction_Dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 1. Data Preprocessing\n",
        "print(\"Starting Data Preprocessing...\")\n",
        "\n",
        "# Remove outliers using IQR method\n",
        "def remove_outliers_iqr(df, columns):\n",
        "    df_clean = df.copy()\n",
        "    for col in columns:\n",
        "        Q1 = df_clean[col].quantile(0.25)\n",
        "        Q3 = df_clean[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
        "    return df_clean\n",
        "\n",
        "# Select numerical columns for outlier removal\n",
        "numerical_cols = ['Temperature (°C)', 'Volume Concentration (ϕ)',\n",
        "                  'Density of Nano Particle 1 (ρnp)', 'Density of Nano Particle 2 (ρnp)',\n",
        "                  'Density of Base Fluid (ρbf)', 'Volume Mixture of Particle 1',\n",
        "                  'Volume Mixture of Particle 2', 'Density (ρ)']\n",
        "\n",
        "df_clean = remove_outliers_iqr(df, numerical_cols)\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"After outlier removal: {df_clean.shape}\")\n",
        "\n",
        "# Label encoding for categorical variables\n",
        "le_nanoparticle = LabelEncoder()\n",
        "le_basefluid = LabelEncoder()\n",
        "\n",
        "df_clean['Nano Particle_encoded'] = le_nanoparticle.fit_transform(df_clean['Nano Particle'])\n",
        "df_clean['Base Fluid_encoded'] = le_basefluid.fit_transform(df_clean['Base Fluid'])\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_clean[['Nano Particle_encoded', 'Base Fluid_encoded', 'Temperature (°C)',\n",
        "              'Volume Concentration (ϕ)', 'Density of Nano Particle 1 (ρnp)',\n",
        "              'Density of Nano Particle 2 (ρnp)', 'Density of Base Fluid (ρbf)',\n",
        "              'Volume Mixture of Particle 1', 'Volume Mixture of Particle 2']]\n",
        "y = df_clean['Density (ρ)']\n",
        "\n",
        "# 2. Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Train-Test Split (80:20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# Sparrow Search Algorithm Implementation\n",
        "class SSA:\n",
        "    def __init__(self, population_size, max_iterations, lower_bound, upper_bound, dimension, objective_function):\n",
        "        self.population_size = population_size\n",
        "        self.max_iterations = max_iterations\n",
        "        self.lower_bound = np.array(lower_bound)\n",
        "        self.upper_bound = np.array(upper_bound)\n",
        "        self.dimension = dimension\n",
        "        self.objective_function = objective_function\n",
        "\n",
        "    def optimize(self):\n",
        "        p_num = int(self.population_size * 0.2)  # Percentage of producers\n",
        "        x = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dimension))\n",
        "        fitness = np.apply_along_axis(self.objective_function, 1, x)\n",
        "        p_fit = fitness.copy()\n",
        "        p_x = x.copy()\n",
        "        best_idx = np.argmin(fitness)\n",
        "        f_min = fitness[best_idx]\n",
        "        best_x = x[best_idx, :]\n",
        "\n",
        "        convergence_curve = []\n",
        "\n",
        "        for t in range(self.max_iterations):\n",
        "            sorted_idx = np.argsort(p_fit)\n",
        "            worst_idx = np.argmax(p_fit)\n",
        "            worst_x = x[worst_idx, :]\n",
        "\n",
        "            # Producers\n",
        "            for i in range(p_num):\n",
        "                r1 = np.random.rand()\n",
        "                x[sorted_idx[i], :] = p_x[sorted_idx[i], :] * np.exp(-i / (r1 * self.max_iterations))\n",
        "                x[sorted_idx[i], :] = np.clip(x[sorted_idx[i], :], self.lower_bound, self.upper_bound)\n",
        "                fitness[sorted_idx[i]] = self.objective_function(x[sorted_idx[i], :])\n",
        "\n",
        "            # Scroungers and Sparrows\n",
        "            for i in range(p_num, self.population_size):\n",
        "                A = np.random.choice([-1, 1], size=self.dimension)\n",
        "                if i > self.population_size / 2:\n",
        "                    x[sorted_idx[i], :] = np.random.randn(self.dimension) * np.exp(\n",
        "                        (worst_x - p_x[sorted_idx[i], :]) / (i ** 2))\n",
        "                else:\n",
        "                    x[sorted_idx[i], :] = best_x + np.abs(p_x[sorted_idx[i], :] - best_x) * A\n",
        "\n",
        "                x[sorted_idx[i], :] = np.clip(x[sorted_idx[i], :], self.lower_bound, self.upper_bound)\n",
        "                fitness[sorted_idx[i]] = self.objective_function(x[sorted_idx[i], :])\n",
        "\n",
        "            for i in range(self.population_size):\n",
        "                if fitness[i] < p_fit[i]:\n",
        "                    p_fit[i] = fitness[i]\n",
        "                    p_x[i, :] = x[i, :]\n",
        "\n",
        "                    if p_fit[i] < f_min:\n",
        "                        f_min = p_fit[i]\n",
        "                        best_x = p_x[i, :]\n",
        "\n",
        "            convergence_curve.append(f_min)\n",
        "\n",
        "        return f_min, best_x, convergence_curve\n",
        "\n",
        "# Performance Metrics Calculation\n",
        "def calculate_metrics(y_true, y_pred, model, X_train, y_train):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate AIC and BIC\n",
        "    n = len(y_true)\n",
        "    k = X_train.shape[1] + 1  # number of parameters\n",
        "    aic = 2*k + n*np.log(mse)\n",
        "    bic = k*np.log(n) + n*np.log(mse)\n",
        "\n",
        "    # COD (Coefficient of Determination) is same as R2\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'R2': r2,\n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "        'COD': cod\n",
        "    }\n",
        "\n",
        "# Model implementations\n",
        "class SimpleRBFN:\n",
        "    def __init__(self, n_centers=10, gamma=1.0):\n",
        "        self.n_centers = n_centers\n",
        "        self.gamma = gamma\n",
        "        self.centers = None\n",
        "        self.weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        idx = np.random.choice(n_samples, min(self.n_centers, n_samples), replace=False)\n",
        "        self.centers = X[idx]\n",
        "\n",
        "        # Calculate RBF matrix\n",
        "        rbf_matrix = self._rbf_matrix(X)\n",
        "\n",
        "        # Calculate weights using pseudo-inverse\n",
        "        self.weights = np.linalg.pinv(rbf_matrix).dot(y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        rbf_matrix = self._rbf_matrix(X)\n",
        "        return rbf_matrix.dot(self.weights)\n",
        "\n",
        "    def _rbf_matrix(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        rbf_matrix = np.zeros((n_samples, self.n_centers))\n",
        "\n",
        "        for i, center in enumerate(self.centers):\n",
        "            rbf_matrix[:, i] = np.exp(-self.gamma * np.sum((X - center)**2, axis=1))\n",
        "\n",
        "        return rbf_matrix\n",
        "\n",
        "class SimpleELM:\n",
        "    def __init__(self, n_hidden=50):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.input_weights = None\n",
        "        self.biases = None\n",
        "        self.output_weights = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Random input weights and biases\n",
        "        self.input_weights = np.random.randn(n_features, self.n_hidden)\n",
        "        self.biases = np.random.randn(self.n_hidden)\n",
        "\n",
        "        # Calculate hidden layer output\n",
        "        H = np.tanh(X.dot(self.input_weights) + self.biases)\n",
        "\n",
        "        # Calculate output weights\n",
        "        self.output_weights = np.linalg.pinv(H).dot(y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        H = np.tanh(X.dot(self.input_weights) + self.biases)\n",
        "        return H.dot(self.output_weights)\n",
        "\n",
        "class SimpleGRNN:\n",
        "    def __init__(self, sigma=1.0):\n",
        "        self.sigma = sigma\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            distances = np.sum((self.X_train - x)**2, axis=1)\n",
        "            weights = np.exp(-distances / (2 * self.sigma**2))\n",
        "            prediction = np.sum(weights * self.y_train) / np.sum(weights)\n",
        "            predictions.append(prediction)\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Hyperparameter ranges\n",
        "hyperparameter_ranges = {\n",
        "    'SVM': {'C': (0.1, 100), 'gamma': (0.001, 1), 'epsilon': (0.01, 1)},\n",
        "    'Random Forest': {'n_estimators': (50, 200), 'max_depth': (5, 20), 'min_samples_split': (2, 10), 'min_samples_leaf': (1, 5)},\n",
        "    'Decision Tree': {'max_depth': (5, 20), 'min_samples_split': (2, 20), 'min_samples_leaf': (1, 10)},\n",
        "    'XGBoost': {'n_estimators': (50, 200), 'max_depth': (3, 10), 'learning_rate': (0.01, 0.3), 'subsample': (0.5, 1.0)},\n",
        "    'MLP': {'hidden_layer_sizes': (50, 200), 'alpha': (0.0001, 0.01), 'learning_rate_init': (0.001, 0.1)},\n",
        "    'Extra Trees': {'n_estimators': (50, 300), 'max_depth': (5, 25), 'min_samples_split': (2, 15), 'min_samples_leaf': (1, 8)},\n",
        "    'Elastic Net': {'alpha': (0.0001, 10), 'l1_ratio': (0.0, 1.0), 'max_iter': (500, 2000)},\n",
        "    'RBFN': {'n_centers': (5, 50), 'gamma': (0.1, 10.0)},\n",
        "    'ELM': {'n_hidden': (10, 500)},\n",
        "    'GRNN': {'sigma': (0.1, 5.0)}\n",
        "}\n",
        "\n",
        "# Results storage\n",
        "results_df = pd.DataFrame()\n",
        "hyperparams_df = pd.DataFrame()\n",
        "\n",
        "# Model optimization and evaluation\n",
        "def optimize_model(model_name, model_class, param_ranges, X_train, y_train, X_test, y_test):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Optimizing {model_name} with Sparrow Search Algorithm\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Define objective function\n",
        "    def objective_function(params):\n",
        "        try:\n",
        "            if model_name == 'SVM':\n",
        "                model = SVR(C=params[0], gamma=params[1], epsilon=params[2])\n",
        "            elif model_name == 'Decision Tree':\n",
        "                model = DecisionTreeRegressor(max_depth=int(params[0]), min_samples_split=int(params[1]),\n",
        "                                            min_samples_leaf=int(params[2]))\n",
        "            elif model_name == 'Random Forest':\n",
        "                model = RandomForestRegressor(n_estimators=int(params[0]), max_depth=int(params[1]),\n",
        "                                            min_samples_split=int(params[2]), min_samples_leaf=int(params[3]),\n",
        "                                            random_state=42)\n",
        "            elif model_name == 'XGBoost':\n",
        "                model = XGBRegressor(n_estimators=int(params[0]), max_depth=int(params[1]),\n",
        "                                   learning_rate=params[2], subsample=params[3], random_state=42)\n",
        "            elif model_name == 'MLP':\n",
        "                model = MLPRegressor(hidden_layer_sizes=(int(params[0]),), alpha=params[1],\n",
        "                                   learning_rate_init=params[2], max_iter=500, random_state=42)\n",
        "            elif model_name == 'Extra Trees':\n",
        "                model = ExtraTreesRegressor(n_estimators=int(params[0]), max_depth=int(params[1]),\n",
        "                                          min_samples_split=int(params[2]), min_samples_leaf=int(params[3]),\n",
        "                                          random_state=42)\n",
        "            elif model_name == 'Elastic Net':\n",
        "                model = ElasticNet(alpha=params[0], l1_ratio=params[1], max_iter=int(params[2]))\n",
        "            elif model_name == 'RBFN':\n",
        "                model = SimpleRBFN(n_centers=int(params[0]), gamma=params[1])\n",
        "            elif model_name == 'ELM':\n",
        "                model = SimpleELM(n_hidden=int(params[0]))\n",
        "            elif model_name == 'GRNN':\n",
        "                model = SimpleGRNN(sigma=params[0])\n",
        "\n",
        "            # Cross-validation\n",
        "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "            return -cv_scores.mean()  # Return positive MSE for minimization\n",
        "        except:\n",
        "            return 1e10  # Return large value if model fails\n",
        "\n",
        "    # Set up SSA parameters\n",
        "    param_names = list(param_ranges.keys())\n",
        "    lower_bounds = [param_ranges[param][0] for param in param_names]\n",
        "    upper_bounds = [param_ranges[param][1] for param in param_names]\n",
        "\n",
        "    # Run SSA optimization\n",
        "    ssa = SSA(population_size=20, max_iterations=30, lower_bound=lower_bounds,\n",
        "              upper_bound=upper_bounds, dimension=len(param_names), objective_function=objective_function)\n",
        "\n",
        "    best_fitness, best_params, convergence_curve = ssa.optimize()\n",
        "\n",
        "    # Create and train the best model\n",
        "    if model_name == 'SVM':\n",
        "        best_model = SVR(C=best_params[0], gamma=best_params[1], epsilon=best_params[2])\n",
        "    elif model_name == 'Decision Tree':\n",
        "        best_model = DecisionTreeRegressor(max_depth=int(best_params[0]), min_samples_split=int(best_params[1]),\n",
        "                                         min_samples_leaf=int(best_params[2]))\n",
        "    elif model_name == 'Random Forest':\n",
        "        best_model = RandomForestRegressor(n_estimators=int(best_params[0]), max_depth=int(best_params[1]),\n",
        "                                         min_samples_split=int(best_params[2]), min_samples_leaf=int(best_params[3]),\n",
        "                                         random_state=42)\n",
        "    elif model_name == 'XGBoost':\n",
        "        best_model = XGBRegressor(n_estimators=int(best_params[0]), max_depth=int(best_params[1]),\n",
        "                                learning_rate=best_params[2], subsample=best_params[3], random_state=42)\n",
        "    elif model_name == 'MLP':\n",
        "        best_model = MLPRegressor(hidden_layer_sizes=(int(best_params[0]),), alpha=best_params[1],\n",
        "                                learning_rate_init=best_params[2], max_iter=500, random_state=42)\n",
        "    elif model_name == 'Extra Trees':\n",
        "        best_model = ExtraTreesRegressor(n_estimators=int(best_params[0]), max_depth=int(best_params[1]),\n",
        "                                       min_samples_split=int(best_params[2]), min_samples_leaf=int(best_params[3]),\n",
        "                                       random_state=42)\n",
        "    elif model_name == 'Elastic Net':\n",
        "        best_model = ElasticNet(alpha=best_params[0], l1_ratio=best_params[1], max_iter=int(best_params[2]))\n",
        "    elif model_name == 'RBFN':\n",
        "        best_model = SimpleRBFN(n_centers=int(best_params[0]), gamma=best_params[1])\n",
        "    elif model_name == 'ELM':\n",
        "        best_model = SimpleELM(n_hidden=int(best_params[0]))\n",
        "    elif model_name == 'GRNN':\n",
        "        best_model = SimpleGRNN(sigma=best_params[0])\n",
        "\n",
        "    # Fit and predict\n",
        "    best_model.fit(X_train, y_train)\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(y_test, y_pred, best_model, X_train, y_train)\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    # Store results\n",
        "    result_row = {\n",
        "        'Model': model_name,\n",
        "        'MSE': metrics['MSE'],\n",
        "        'MAE': metrics['MAE'],\n",
        "        'RMSE': metrics['RMSE'],\n",
        "        'MAPE': metrics['MAPE'],\n",
        "        'R2': metrics['R2'],\n",
        "        'AIC': metrics['AIC'],\n",
        "        'BIC': metrics['BIC'],\n",
        "        'COD': metrics['COD'],\n",
        "        'Execution_Time': execution_time\n",
        "    }\n",
        "\n",
        "    # Store hyperparameters\n",
        "    hyperparam_row = {'Model': model_name}\n",
        "    for i, param_name in enumerate(param_names):\n",
        "        hyperparam_row[param_name] = best_params[i]\n",
        "\n",
        "    print(f\"Best parameters: {dict(zip(param_names, best_params))}\")\n",
        "    print(f\"Best cross-validation MSE: {best_fitness:.4f}\")\n",
        "    print(f\"Test R2 Score: {metrics['R2']:.4f}\")\n",
        "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    return result_row, hyperparam_row\n",
        "\n",
        "# Run optimization for all models\n",
        "models_to_optimize = [\n",
        "    ('SVM', SVR, hyperparameter_ranges['SVM']),\n",
        "    ('Decision Tree', DecisionTreeRegressor, hyperparameter_ranges['Decision Tree']),\n",
        "    ('Random Forest', RandomForestRegressor, hyperparameter_ranges['Random Forest']),\n",
        "    ('XGBoost', XGBRegressor, hyperparameter_ranges['XGBoost']),\n",
        "    ('MLP', MLPRegressor, hyperparameter_ranges['MLP']),\n",
        "    ('Extra Trees', ExtraTreesRegressor, hyperparameter_ranges['Extra Trees']),\n",
        "    ('Elastic Net', ElasticNet, hyperparameter_ranges['Elastic Net']),\n",
        "    ('RBFN', SimpleRBFN, hyperparameter_ranges['RBFN']),\n",
        "    ('ELM', SimpleELM, hyperparameter_ranges['ELM']),\n",
        "    ('GRNN', SimpleGRNN, hyperparameter_ranges['GRNN'])\n",
        "]\n",
        "\n",
        "results_list = []\n",
        "hyperparams_list = []\n",
        "\n",
        "for model_name, model_class, param_ranges in models_to_optimize:\n",
        "    try:\n",
        "        result_row, hyperparam_row = optimize_model(model_name, model_class, param_ranges,\n",
        "                                                   X_train, y_train, X_test, y_test)\n",
        "        results_list.append(result_row)\n",
        "        hyperparams_list.append(hyperparam_row)\n",
        "    except Exception as e:\n",
        "        print(f\"Error optimizing {model_name}: {str(e)}\")\n",
        "\n",
        "# Create final dataframes\n",
        "results_df = pd.DataFrame(results_list)\n",
        "hyperparams_df = pd.DataFrame(hyperparams_list)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nPerformance Metrics:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nOptimal Hyperparameters:\")\n",
        "print(hyperparams_df.to_string(index=False))\n",
        "\n",
        "# Sort by R2 score\n",
        "results_df_sorted = results_df.sort_values('R2', ascending=False)\n",
        "print(f\"\\nBest performing model: {results_df_sorted.iloc[0]['Model']}\")\n",
        "print(f\"R2 Score: {results_df_sorted.iloc[0]['R2']:.4f}\")\n",
        "print(f\"RMSE: {results_df_sorted.iloc[0]['RMSE']:.4f}\")"
      ],
      "metadata": {
        "id": "xoIKPBOk8pov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257faf9c-05b1-450c-b852-2db6035b0934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Data Preprocessing...\n",
            "Original dataset shape: (436, 10)\n",
            "After outlier removal: (206, 10)\n",
            "Training set shape: (164, 9)\n",
            "Test set shape: (42, 9)\n",
            "\n",
            "==================================================\n",
            "Optimizing SVM with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'C': np.float64(85.12287004753753), 'gamma': np.float64(0.37219344162140117), 'epsilon': np.float64(0.28702208220378633)}\n",
            "Best cross-validation MSE: 2.8368\n",
            "Test R2 Score: 0.9933\n",
            "Execution time: 20.54 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Decision Tree with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'max_depth': np.float64(8.576380578255437), 'min_samples_split': np.float64(2.674516423274105), 'min_samples_leaf': np.float64(1.514514844086937)}\n",
            "Best cross-validation MSE: 22.1849\n",
            "Test R2 Score: 0.8934\n",
            "Execution time: 6.28 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Random Forest with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'n_estimators': np.float64(67.6366276550799), 'max_depth': np.float64(17.04450011558895), 'min_samples_split': np.float64(2.0), 'min_samples_leaf': np.float64(1.0)}\n",
            "Best cross-validation MSE: 16.2014\n",
            "Test R2 Score: 0.9624\n",
            "Execution time: 255.55 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing XGBoost with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'n_estimators': np.float64(200.0), 'max_depth': np.float64(3.0), 'learning_rate': np.float64(0.3), 'subsample': np.float64(0.5)}\n",
            "Best cross-validation MSE: 3.9549\n",
            "Test R2 Score: 0.9920\n",
            "Execution time: 81.71 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing MLP with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'hidden_layer_sizes': np.float64(200.0), 'alpha': np.float64(0.006416271923176178), 'learning_rate_init': np.float64(0.1)}\n",
            "Best cross-validation MSE: 427.8314\n",
            "Test R2 Score: -0.4685\n",
            "Execution time: 690.51 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Extra Trees with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'n_estimators': np.float64(50.0), 'max_depth': np.float64(8.58935035031944), 'min_samples_split': np.float64(2.0595139769257798), 'min_samples_leaf': np.float64(1.0)}\n",
            "Best cross-validation MSE: 15.7404\n",
            "Test R2 Score: 0.9494\n",
            "Execution time: 177.09 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Elastic Net with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'alpha': np.float64(0.016347915952659716), 'l1_ratio': np.float64(1.0), 'max_iter': np.float64(500.0)}\n",
            "Best cross-validation MSE: 26.0405\n",
            "Test R2 Score: 0.9157\n",
            "Execution time: 7.61 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing RBFN with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'n_centers': np.float64(30.24016817541069), 'gamma': np.float64(9.558853401013382)}\n",
            "Best cross-validation MSE: 10000000000.0000\n",
            "Test R2 Score: -6283.4527\n",
            "Execution time: 0.80 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing ELM with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'n_hidden': np.float64(119.7565291793127)}\n",
            "Best cross-validation MSE: 10000000000.0000\n",
            "Test R2 Score: -0.4200\n",
            "Execution time: 0.87 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing GRNN with Sparrow Search Algorithm\n",
            "==================================================\n",
            "Best parameters: {'sigma': np.float64(3.168550036389804)}\n",
            "Best cross-validation MSE: 10000000000.0000\n",
            "Test R2 Score: 0.1870\n",
            "Execution time: 0.91 seconds\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS\n",
            "================================================================================\n",
            "\n",
            "Performance Metrics:\n",
            "        Model           MSE        MAE       RMSE      MAPE           R2        AIC        BIC          COD  Execution_Time\n",
            "          SVM      1.034872   0.706626   1.017287  0.069643     0.993272  21.439681  38.816378     0.993272       20.541871\n",
            "Decision Tree     16.393921   3.396837   4.048941  0.334859     0.893423 137.470246 154.846942     0.893423        6.284391\n",
            "Random Forest      5.791332   1.851850   2.406519  0.182627     0.962351  93.767217 111.143914     0.962351      255.552897\n",
            "      XGBoost      1.227156   0.875815   1.107771  0.086072     0.992022  28.597364  45.974060     0.992022       81.708931\n",
            "          MLP    225.893424   8.913301  15.029751  0.879109    -0.468531 247.642659 265.019355    -0.468531      690.506295\n",
            "  Extra Trees      7.789844   2.276277   2.791029  0.224185     0.949358 106.218473 123.595169     0.949358      177.090040\n",
            "  Elastic Net     12.971087   2.531101   3.601540  0.248354     0.915675 127.634357 145.011054     0.915675        7.606525\n",
            "         RBFN 966691.719089 980.341600 983.204821 96.204680 -6283.452739 598.828667 616.205363 -6283.452739        0.803536\n",
            "          ELM    218.424226   8.031728  14.779182  0.795685    -0.419974 246.230445 263.607141    -0.419974        0.872068\n",
            "         GRNN    125.057066   9.384700  11.182892  0.924445     0.187005 222.808347 240.185043     0.187005        0.907134\n",
            "\n",
            "Optimal Hyperparameters:\n",
            "        Model        C    gamma  epsilon  max_depth  min_samples_split  min_samples_leaf  n_estimators  learning_rate  subsample  hidden_layer_sizes    alpha  learning_rate_init  l1_ratio  max_iter  n_centers   n_hidden   sigma\n",
            "          SVM 85.12287 0.372193 0.287022        NaN                NaN               NaN           NaN            NaN        NaN                 NaN      NaN                 NaN       NaN       NaN        NaN        NaN     NaN\n",
            "Decision Tree      NaN      NaN      NaN   8.576381           2.674516          1.514515           NaN            NaN        NaN                 NaN      NaN                 NaN       NaN       NaN        NaN        NaN     NaN\n",
            "Random Forest      NaN      NaN      NaN  17.044500           2.000000          1.000000     67.636628            NaN        NaN                 NaN      NaN                 NaN       NaN       NaN        NaN        NaN     NaN\n",
            "      XGBoost      NaN      NaN      NaN   3.000000                NaN               NaN    200.000000            0.3        0.5                 NaN      NaN                 NaN       NaN       NaN        NaN        NaN     NaN\n",
            "          MLP      NaN      NaN      NaN        NaN                NaN               NaN           NaN            NaN        NaN               200.0 0.006416                 0.1       NaN       NaN        NaN        NaN     NaN\n",
            "  Extra Trees      NaN      NaN      NaN   8.589350           2.059514          1.000000     50.000000            NaN        NaN                 NaN      NaN                 NaN       NaN       NaN        NaN        NaN     NaN\n",
            "  Elastic Net      NaN      NaN      NaN        NaN                NaN               NaN           NaN            NaN        NaN                 NaN 0.016348                 NaN       1.0     500.0        NaN        NaN     NaN\n",
            "         RBFN      NaN 9.558853      NaN        NaN                NaN               NaN           NaN            NaN        NaN                 NaN      NaN                 NaN       NaN       NaN  30.240168        NaN     NaN\n",
            "          ELM      NaN      NaN      NaN        NaN                NaN               NaN           NaN            NaN        NaN                 NaN      NaN                 NaN       NaN       NaN        NaN 119.756529     NaN\n",
            "         GRNN      NaN      NaN      NaN        NaN                NaN               NaN           NaN            NaN        NaN                 NaN      NaN                 NaN       NaN       NaN        NaN        NaN 3.16855\n",
            "\n",
            "Best performing model: SVM\n",
            "R2 Score: 0.9933\n",
            "RMSE: 1.0173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTXwPQy68pre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firefly Algorithm"
      ],
      "metadata": {
        "id": "8Qez0g6ZlVH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Add, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Load and preprocess the nanofluid density dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Remove outliers using IQR method\n",
        "    def remove_outliers_iqr(df, columns):\n",
        "        df_clean = df.copy()\n",
        "        for col in columns:\n",
        "            Q1 = df_clean[col].quantile(0.25)\n",
        "            Q3 = df_clean[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
        "        return df_clean\n",
        "\n",
        "    # Numerical columns for outlier detection\n",
        "    numerical_cols = ['Temperature (°C)', 'Volume Concentration (ϕ)',\n",
        "                     'Density of Nano Particle 1 (ρnp)', 'Density of Nano Particle 2 (ρnp)',\n",
        "                     'Density of Base Fluid (ρbf)', 'Volume Mixture of Particle 1',\n",
        "                     'Volume Mixture of Particle 2', 'Density (ρ)']\n",
        "\n",
        "    df_clean = remove_outliers_iqr(df, numerical_cols)\n",
        "\n",
        "    # Label encode categorical variables\n",
        "    le_nano = LabelEncoder()\n",
        "    le_base = LabelEncoder()\n",
        "\n",
        "    df_clean['Nano Particle Encoded'] = le_nano.fit_transform(df_clean['Nano Particle'])\n",
        "    df_clean['Base Fluid Encoded'] = le_base.fit_transform(df_clean['Base Fluid'])\n",
        "\n",
        "    # Select features and target\n",
        "    feature_cols = ['Nano Particle Encoded', 'Base Fluid Encoded', 'Temperature (°C)',\n",
        "                   'Volume Concentration (ϕ)', 'Density of Nano Particle 1 (ρnp)',\n",
        "                   'Density of Nano Particle 2 (ρnp)', 'Density of Base Fluid (ρbf)',\n",
        "                   'Volume Mixture of Particle 1', 'Volume Mixture of Particle 2']\n",
        "\n",
        "    X = df_clean[feature_cols]\n",
        "    y = df_clean['Density (ρ)']\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "# Custom Regression Models\n",
        "class RBFNRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_centers=10, gamma=1.0):\n",
        "        self.n_centers = n_centers\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        # Use K-means to find RBF centers\n",
        "        kmeans = KMeans(n_clusters=self.n_centers, random_state=42)\n",
        "        self.centers = kmeans.fit(X).cluster_centers_\n",
        "\n",
        "        # Calculate RBF matrix\n",
        "        rbf_matrix = rbf_kernel(X, self.centers, gamma=self.gamma)\n",
        "        # Add bias term\n",
        "        rbf_matrix = np.column_stack([rbf_matrix, np.ones(X.shape[0])])\n",
        "\n",
        "        # Solve for weights using pseudo-inverse\n",
        "        self.weights = np.linalg.pinv(rbf_matrix) @ y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        rbf_matrix = rbf_kernel(X, self.centers, gamma=self.gamma)\n",
        "        rbf_matrix = np.column_stack([rbf_matrix, np.ones(X.shape[0])])\n",
        "        return rbf_matrix @ self.weights\n",
        "\n",
        "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_hidden=100):\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = X.shape[1]\n",
        "        # Random input weights and biases\n",
        "        np.random.seed(42)\n",
        "        self.input_weights = np.random.randn(n_features, self.n_hidden)\n",
        "        self.biases = np.random.randn(self.n_hidden)\n",
        "\n",
        "        # Calculate hidden layer output\n",
        "        H = np.tanh(X @ self.input_weights + self.biases)\n",
        "        # Calculate output weights\n",
        "        self.output_weights = np.linalg.pinv(H) @ y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        H = np.tanh(X @ self.input_weights + self.biases)\n",
        "        return H @ self.output_weights\n",
        "\n",
        "class GRNNRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, sigma=1.0):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Calculate distances\n",
        "            distances = np.sum((self.X_train - x) ** 2, axis=1)\n",
        "            # Calculate weights using Gaussian kernel\n",
        "            weights = np.exp(-distances / (2 * self.sigma ** 2))\n",
        "            weights = weights / np.sum(weights)\n",
        "            # Weighted average\n",
        "            prediction = np.sum(weights * self.y_train)\n",
        "            predictions.append(prediction)\n",
        "        return np.array(predictions)\n",
        "\n",
        "class CascadeCorrelationRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, max_hidden=10):\n",
        "        self.max_hidden = max_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simplified cascade correlation using MLPRegressor\n",
        "        self.model = MLPRegressor(hidden_layer_sizes=(self.max_hidden,),\n",
        "                                 max_iter=1000, random_state=42)\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "def create_deep_feedforward_model(input_dim, hidden_layers, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "\n",
        "    for i, units in enumerate(hidden_layers):\n",
        "        model.add(Dense(units, activation='relu'))\n",
        "        if i < len(hidden_layers) - 1:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "def create_resnet_model(input_dim, n_blocks, hidden_size, learning_rate):\n",
        "    inputs = Input(shape=(input_dim,))\n",
        "    x = Dense(hidden_size, activation='relu')(inputs)\n",
        "\n",
        "    for _ in range(n_blocks):\n",
        "        # Residual block\n",
        "        shortcut = x\n",
        "        x = Dense(hidden_size, activation='relu')(x)\n",
        "        x = Dense(hidden_size, activation='linear')(x)\n",
        "        x = Add()([x, shortcut])\n",
        "        x = tf.keras.activations.relu(x)\n",
        "\n",
        "    outputs = Dense(1, activation='linear')(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "class DeepFeedforwardRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, hidden_layers=[50, 25], learning_rate=0.001, epochs=100):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = create_deep_feedforward_model(X.shape[1], self.hidden_layers, self.learning_rate)\n",
        "        early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "        self.model.fit(X, y, epochs=self.epochs, validation_split=0.2,\n",
        "                      callbacks=[early_stopping], verbose=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X, verbose=0).flatten()\n",
        "\n",
        "class ResNetRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_blocks=2, hidden_size=50, learning_rate=0.001, epochs=100):\n",
        "        self.n_blocks = n_blocks\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = create_resnet_model(X.shape[1], self.n_blocks, self.hidden_size, self.learning_rate)\n",
        "        early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "        self.model.fit(X, y, epochs=self.epochs, validation_split=0.2,\n",
        "                      callbacks=[early_stopping], verbose=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X, verbose=0).flatten()\n",
        "\n",
        "\n",
        "# Firefly Algorithm for Hyperparameter Optimization\n",
        "class FireflyHyperparameterOptimization:\n",
        "    def __init__(self, X_train, y_train, X_test, y_test, model_type, param_bounds,\n",
        "                 n_fireflies=15, max_iterations=20, alpha=1.0, beta0=1.0, gamma=0.01):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.model_type = model_type\n",
        "        self.param_bounds = param_bounds\n",
        "        self.n_fireflies = n_fireflies\n",
        "        self.max_iterations = max_iterations\n",
        "        self.alpha = alpha\n",
        "        self.beta0 = beta0\n",
        "        self.gamma = gamma\n",
        "        self.theta = 0.97\n",
        "        self.population = None\n",
        "        self.best_position = None\n",
        "        self.best_fitness = float('inf')\n",
        "        self.best_params = None\n",
        "\n",
        "    def _initialize_population(self):\n",
        "        population = []\n",
        "        for _ in range(self.n_fireflies):\n",
        "            firefly = []\n",
        "            for param_name, param_info in self.param_bounds.items():\n",
        "                if isinstance(param_info, list): # Handle categorical parameters (like hidden_layers)\n",
        "                     firefly.append(np.random.choice(range(len(param_info))))\n",
        "                elif isinstance(param_info, tuple) and len(param_info) == 2:\n",
        "                    min_val, max_val = param_info\n",
        "                    if isinstance(min_val, int) and isinstance(max_val, int):\n",
        "                        firefly.append(np.random.randint(min_val, max_val + 1))\n",
        "                    else:\n",
        "                        firefly.append(np.random.uniform(min_val, max_val))\n",
        "                else:\n",
        "                     raise ValueError(f\"Invalid parameter bounds format for {param_name}: {param_info}\")\n",
        "            population.append(firefly)\n",
        "        return np.array(population)\n",
        "\n",
        "\n",
        "    def _position_to_params(self, position):\n",
        "        params = {}\n",
        "        for i, (param_name, param_info) in enumerate(self.param_bounds.items()):\n",
        "             if isinstance(param_info, list): # Handle categorical parameters\n",
        "                  params[param_name] = param_info[int(position[i]) % len(param_info)]\n",
        "             elif isinstance(param_info, tuple) and len(param_info) == 2:\n",
        "                  min_val, max_val = param_info\n",
        "                  if isinstance(min_val, int) and isinstance(max_val, int):\n",
        "                      params[param_name] = int(position[i])\n",
        "                  else:\n",
        "                      params[param_name] = position[i]\n",
        "        return params\n",
        "\n",
        "\n",
        "    def _evaluate_fitness(self, position):\n",
        "        try:\n",
        "            params = self._position_to_params(position)\n",
        "\n",
        "            model_map = {\n",
        "                'svm': SVR,\n",
        "                'rf': RandomForestRegressor,\n",
        "                'dt': DecisionTreeRegressor,\n",
        "                'xgb': xgb.XGBRegressor,\n",
        "                'mlp': MLPRegressor,\n",
        "                'extra_trees': ExtraTreesRegressor,\n",
        "                'elastic_net': ElasticNet,\n",
        "                'rbfn': RBFNRegressor,\n",
        "                'elm': ELMRegressor,\n",
        "                'grnn': GRNNRegressor,\n",
        "                'cascade': CascadeCorrelationRegressor,\n",
        "                'deep_ff': DeepFeedforwardRegressor,\n",
        "                'resnet': ResNetRegressor\n",
        "            }\n",
        "\n",
        "            if self.model_type in ['rf', 'dt', 'xgb', 'extra_trees']:\n",
        "                model = model_map[self.model_type](**params, random_state=42)\n",
        "            elif self.model_type == 'mlp':\n",
        "                model = model_map[self.model_type](**params, random_state=42, max_iter=1000)\n",
        "            elif self.model_type in ['deep_ff', 'resnet']:\n",
        "                 # Pass epochs and learning_rate directly for these models\n",
        "                 model = model_map[self.model_type](\n",
        "                     hidden_layers=params.get('hidden_layers', (50, 25)), # Default if not in params\n",
        "                     learning_rate=params.get('learning_rate', 0.001), # Default if not in params\n",
        "                     epochs=params.get('epochs', 100), # Default if not in params\n",
        "                     n_blocks=params.get('n_blocks', 2), # Default if not in params\n",
        "                     hidden_size=params.get('hidden_size', 50) # Default if not in params\n",
        "                 )\n",
        "            else:\n",
        "                model = model_map[self.model_type](**params)\n",
        "\n",
        "\n",
        "            # Use cross-validation for more robust evaluation\n",
        "            cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=3,\n",
        "                                      scoring='neg_mean_squared_error')\n",
        "            mse = -cv_scores.mean()\n",
        "            return mse\n",
        "\n",
        "        except Exception as e:\n",
        "            return float('inf')\n",
        "\n",
        "    def _calculate_distance(self, pos1, pos2):\n",
        "        return np.sqrt(np.sum((pos1 - pos2)**2))\n",
        "\n",
        "    def _update_position(self, pos_i, pos_j, fitness_i, fitness_j):\n",
        "        if fitness_j < fitness_i:\n",
        "            r = self._calculate_distance(pos_i, pos_j)\n",
        "            beta = self.beta0 * np.exp(-self.gamma * r**2)\n",
        "            new_pos = pos_i + beta * (pos_j - pos_i) + self.alpha * (np.random.rand(len(pos_i)) - 0.5)\n",
        "\n",
        "            # Ensure bounds are respected\n",
        "            for i, (param_name, param_info) in enumerate(self.param_bounds.items()):\n",
        "                if isinstance(param_info, list): # Handle categorical parameters\n",
        "                    # For categorical, ensure index is within bounds\n",
        "                    new_pos[i] = np.clip(np.round(new_pos[i]), 0, len(param_info) - 1)\n",
        "                elif isinstance(param_info, tuple) and len(param_info) == 2:\n",
        "                    min_val, max_val = param_info\n",
        "                    new_pos[i] = np.clip(new_pos[i], min_val, max_val)\n",
        "                    if isinstance(min_val, int) and isinstance(max_val, int):\n",
        "                         new_pos[i] = round(new_pos[i]) # Round integer parameters\n",
        "            return new_pos\n",
        "        return pos_i\n",
        "\n",
        "\n",
        "    def optimize(self):\n",
        "        self.population = self._initialize_population()\n",
        "        fitness_values = np.array([self._evaluate_fitness(pos) for pos in self.population])\n",
        "\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.best_position = self.population[best_idx].copy()\n",
        "        self.best_fitness = fitness_values[best_idx]\n",
        "        self.best_params = self._position_to_params(self.best_position)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.alpha *= self.theta\n",
        "\n",
        "            for i in range(self.n_fireflies):\n",
        "                for j in range(self.n_fireflies):\n",
        "                    if fitness_values[j] < fitness_values[i]:\n",
        "                        self.population[i] = self._update_position(\n",
        "                            self.population[i], self.population[j],\n",
        "                            fitness_values[i], fitness_values[j]\n",
        "                        )\n",
        "\n",
        "                fitness_values[i] = self._evaluate_fitness(self.population[i])\n",
        "\n",
        "                if fitness_values[i] < self.best_fitness:\n",
        "                    self.best_fitness = fitness_values[i]\n",
        "                    self.best_position = self.population[i].copy()\n",
        "                    self.best_params = self._position_to_params(self.best_position)\n",
        "\n",
        "            if (iteration + 1) % 5 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.max_iterations}, Best MSE: {self.best_fitness:.4f}\")\n",
        "\n",
        "        return self.best_params, self.best_fitness\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive regression metrics\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # MAPE (avoiding division by zero)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, 1, y_true))) * 100\n",
        "\n",
        "    # AIC and BIC approximations\n",
        "    n = len(y_true)\n",
        "    k = 2  # number of parameters (simplified)\n",
        "    aic = n * np.log(mse) + 2 * k\n",
        "    bic = n * np.log(mse) + k * np.log(n)\n",
        "\n",
        "    # Coefficient of determination\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "        'COD': cod,\n",
        "        'R2': r2\n",
        "    }\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    file_path = '/content/Density_Prediction_Dataset.csv'\n",
        "    X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path)\n",
        "\n",
        "    print(\"Data preprocessing completed!\")\n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "    # Hyperparameter ranges\n",
        "    hyperparameter_ranges = {\n",
        "        'SVM': {'C': (0.1, 100), 'gamma': (0.001, 1), 'epsilon': (0.01, 1)},\n",
        "        'Random Forest': {'n_estimators': (50, 200), 'max_depth': (5, 20), 'min_samples_split': (2, 10), 'min_samples_leaf': (1, 5)},\n",
        "        'Decision Tree': {'max_depth': (5, 20), 'min_samples_split': (2, 20), 'min_samples_leaf': (1, 10), 'max_features': (0.1, 1.0)},\n",
        "        'XGBoost': {'n_estimators': (50, 200), 'max_depth': (3, 10), 'learning_rate': (0.01, 0.3), 'subsample': (0.5, 1.0)},\n",
        "        'MLP': {'hidden_layer_sizes': (50, 200), 'alpha': (0.0001, 0.01), 'learning_rate_init': (0.001, 0.1)},\n",
        "        'Extra Trees': {'n_estimators': (50, 300), 'max_depth': (5, 25), 'min_samples_split': (2, 15), 'min_samples_leaf': (1, 8), 'max_features': (0.1, 1.0)},\n",
        "        'Elastic Net': {'alpha': (0.0001, 10), 'l1_ratio': (0.0, 1.0), 'max_iter': (500, 2000)},\n",
        "        'RBFN': {'n_centers': (5, 50), 'gamma': (0.1, 10.0)},\n",
        "        'ELM': {'n_hidden': (10, 500)},\n",
        "        'GRNN': {'sigma': (0.1, 5.0)},\n",
        "        'Cascade-Correlation': {'max_hidden': (2, 20)},\n",
        "        'Deep Feedforward': {'hidden_layers': [[20, 10], [50, 25], [100, 50, 25]], 'learning_rate': (0.001, 0.1), 'epochs': (50, 200)},\n",
        "        'ResNet': {'n_blocks': (1, 5), 'hidden_size': (10, 100), 'learning_rate': (0.001, 0.1), 'epochs': (50, 200)}\n",
        "    }\n",
        "\n",
        "    # Model mapping\n",
        "    model_mapping = {\n",
        "        'SVM': 'svm',\n",
        "        'Random Forest': 'rf',\n",
        "        'Decision Tree': 'dt',\n",
        "        'XGBoost': 'xgb',\n",
        "        'MLP': 'mlp',\n",
        "        'Extra Trees': 'extra_trees',\n",
        "        'Elastic Net': 'elastic_net',\n",
        "        'RBFN': 'rbfn',\n",
        "        'ELM': 'elm',\n",
        "        'GRNN': 'grnn',\n",
        "        'Cascade-Correlation': 'cascade',\n",
        "        'Deep Feedforward': 'deep_ff',\n",
        "        'ResNet': 'resnet'\n",
        "    }\n",
        "\n",
        "    # Results storage\n",
        "    performance_results = []\n",
        "    hyperparameter_results = []\n",
        "\n",
        "    # Run optimization for each model\n",
        "    for model_name, param_bounds in hyperparameter_ranges.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Optimizing {model_name} with Firefly Algorithm\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize optimizer\n",
        "        optimizer = FireflyHyperparameterOptimization(\n",
        "            X_train, y_train, X_test, y_test,\n",
        "            model_mapping[model_name], param_bounds,\n",
        "            n_fireflies=15, max_iterations=20\n",
        "        )\n",
        "\n",
        "        # Optimize hyperparameters\n",
        "        best_params, best_fitness = optimizer.optimize()\n",
        "\n",
        "        # Train final model with best parameters\n",
        "        model_classes = {\n",
        "            'svm': SVR,\n",
        "            'rf': RandomForestRegressor,\n",
        "            'dt': DecisionTreeRegressor,\n",
        "            'xgb': xgb.XGBRegressor,\n",
        "            'mlp': MLPRegressor,\n",
        "            'extra_trees': ExtraTreesRegressor,\n",
        "            'elastic_net': ElasticNet,\n",
        "            'rbfn': RBFNRegressor,\n",
        "            'elm': ELMRegressor,\n",
        "            'grnn': GRNNRegressor,\n",
        "            'cascade': CascadeCorrelationRegressor,\n",
        "            'deep_ff': DeepFeedforwardRegressor,\n",
        "            'resnet': ResNetRegressor\n",
        "        }\n",
        "\n",
        "        model_type = model_mapping[model_name]\n",
        "        if model_type in ['rf', 'dt', 'xgb', 'extra_trees']:\n",
        "            final_model = model_classes[model_type](**best_params, random_state=42)\n",
        "        elif model_type == 'mlp':\n",
        "            final_model = model_classes[model_type](**best_params, random_state=42, max_iter=1000)\n",
        "        elif model_type in ['deep_ff', 'resnet']:\n",
        "             # Pass epochs and learning_rate directly for these models\n",
        "             final_model = model_classes[model_type](\n",
        "                 hidden_layers=best_params.get('hidden_layers', (50, 25)), # Default if not in params\n",
        "                 learning_rate=best_params.get('learning_rate', 0.001), # Default if not in params\n",
        "                 epochs=best_params.get('epochs', 100), # Default if not in params\n",
        "                 n_blocks=best_params.get('n_blocks', 2), # Default if not in params\n",
        "                 hidden_size=best_params.get('hidden_size', 50) # Default if not in params\n",
        "             )\n",
        "        else:\n",
        "            final_model = model_classes[model_type](**best_params)\n",
        "\n",
        "\n",
        "        final_model.fit(X_train, y_train)\n",
        "        y_pred = final_model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "\n",
        "        # Store results\n",
        "        performance_results.append({\n",
        "            'Model': model_name,\n",
        "            'MSE': metrics['MSE'],\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'AIC': metrics['AIC'],\n",
        "            'BIC': metrics['BIC'],\n",
        "            'COD': metrics['COD'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Execution Time (s)': execution_time\n",
        "        })\n",
        "\n",
        "        # Store hyperparameters\n",
        "        hyperparameter_results.append({\n",
        "            'Model': model_name,\n",
        "            'Best Parameters': best_params,\n",
        "            'Best MSE': best_fitness\n",
        "        })\n",
        "\n",
        "        print(f\"Best parameters: {best_params}\")\n",
        "        print(f\"Best MSE: {best_fitness:.4f}\")\n",
        "        print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    # Create DataFrames\n",
        "    performance_df = pd.DataFrame(performance_results)\n",
        "    hyperparameter_df = pd.DataFrame(hyperparameter_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(performance_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPTIMAL HYPERPARAMETERS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    for idx, row in hyperparameter_df.iterrows():\n",
        "        print(f\"\\n{row['Model']}:\")\n",
        "        print(f\"  Best Parameters: {row['Best Parameters']}\")\n",
        "        print(f\"  Best MSE: {row['Best MSE']:.4f}\")\n",
        "\n",
        "    return performance_df, hyperparameter_df\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    performance_df, hyperparameter_df = main()"
      ],
      "metadata": {
        "id": "ZOyHWKL8EbAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e8b3ea7-0c5d-4fa4-ee4d-b0427774dc8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing completed!\n",
            "Training set shape: (164, 9)\n",
            "Test set shape: (42, 9)\n",
            "\n",
            "==================================================\n",
            "Optimizing SVM with Firefly Algorithm\n",
            "==================================================\n",
            "Iteration 5/20, Best MSE: 2.8030\n",
            "Iteration 10/20, Best MSE: 2.8029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-2344827206.py\", line 565, in <cell line: 0>\n",
            "    performance_df, hyperparameter_df = main()\n",
            "                                        ^^^^^^\n",
            "  File \"/tmp/ipython-input-2344827206.py\", line 473, in main\n",
            "    best_params, best_fitness = optimizer.optimize()\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2344827206.py\", line 367, in optimize\n",
            "    fitness_values[i] = self._evaluate_fitness(self.population[i])\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2344827206.py\", line 316, in _evaluate_fitness\n",
            "    cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=3,\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 684, in cross_val_score\n",
            "    cv_results = cross_validate(\n",
            "                 ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 411, in cross_validate\n",
            "    results = parallel(\n",
            "              ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 1986, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 858, in _fit_and_score\n",
            "    X_train, y_train = _safe_split(estimator, X, y, train)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/metaestimators.py\", line 159, in _safe_split\n",
            "    y_subset = _safe_indexing(y, indices)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_indexing.py\", line 266, in _safe_indexing\n",
            "    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_indexing.py\", line 47, in _pandas_indexing\n",
            "    return X.take(key, axis=axis)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\", line 4133, in take\n",
            "    new_data = self._mgr.take(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\", line 894, in take\n",
            "    return self.reindex_indexer(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\", line 680, in reindex_indexer\n",
            "    new_blocks = self._slice_take_blocks_ax0(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\", line 773, in _slice_take_blocks_ax0\n",
            "    blk.take_nd(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\", line 1307, in take_nd\n",
            "    new_values = algos.take_nd(\n",
            "                 ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\", line 117, in take_nd\n",
            "    return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\", line 133, in _take_nd_ndarray\n",
            "    dtype, fill_value, mask_info = _take_preprocess_indexer_and_fill_value(\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\", line 579, in _take_preprocess_indexer_and_fill_value\n",
            "    dtype, fill_value = maybe_promote(arr.dtype, fill_value)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/cast.py\", line 581, in maybe_promote\n",
            "    pass\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1718, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 182, in findsource\n",
            "    lines = linecache.getlines(file, globals_dict)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 142, in updatecache\n",
            "    lines = fp.readlines()\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"<frozen codecs>\", line 319, in decode\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2344827206.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0mperformance_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameter_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2344827206.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;31m# Optimize hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_fitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2344827206.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mfitness_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2344827206.py\u001b[0m in \u001b[0;36m_evaluate_fitness\u001b[0;34m(self, position)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# Use cross-validation for more robust evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=3,\n\u001b[0m\u001b[1;32m    317\u001b[0m                                       scoring='neg_mean_squared_error')\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[0;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0my_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# would require updating some tests such as test_train_test_split_mock_pandas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_polars_df_or_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_pandas_indexing\u001b[0;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# copy that will not raise SettingWithCopyWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             new_blocks = self._slice_take_blocks_ax0(\n\u001b[0m\u001b[1;32m    681\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    772\u001b[0m                     return [\n\u001b[0;32m--> 773\u001b[0;31m                         blk.take_nd(\n\u001b[0m\u001b[1;32m    774\u001b[0m                             \u001b[0mslobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;31m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         new_values = algos.take_nd(\n\u001b[0m\u001b[1;32m   1308\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     dtype, fill_value, mask_info = _take_preprocess_indexer_and_fill_value(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_preprocess_indexer_and_fill_value\u001b[0;34m(arr, indexer, fill_value, allow_fill, mask)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;31m# it's faster than computing a mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_promote\u001b[0;34m(dtype, fill_value)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GeEFB-jdlXn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1pVB7RokvnBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**skopt**"
      ],
      "metadata": {
        "id": "tsyv_zMkvuzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "from functools import partial\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real, Categorical"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdYVAKiPvzeK",
        "outputId": "8289627a-8928-4acc-e1bc-e8d76c3761c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.1)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.7.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from functools import partial\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real, Categorical\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available, skipping XGBoost regressor\")\n",
        "\n",
        "# Custom implementations for specialized regressors\n",
        "class RBFNRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_centers=10, gamma=1.0):\n",
        "        self.n_centers = n_centers\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        # Select centers randomly from training data\n",
        "        idx = np.random.choice(len(X), min(self.n_centers, len(X)), replace=False)\n",
        "        self.centers = X[idx]\n",
        "\n",
        "        # Calculate RBF weights\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centers, axis=2)\n",
        "        phi = np.exp(-self.gamma * distances**2)\n",
        "        self.weights = np.linalg.pinv(phi) @ y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centers, axis=2)\n",
        "        phi = np.exp(-self.gamma * distances**2)\n",
        "        return phi @ self.weights\n",
        "\n",
        "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_hidden=100):\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = X.shape[1]\n",
        "        self.input_weights = np.random.uniform(-1, 1, (n_features, self.n_hidden))\n",
        "        self.biases = np.random.uniform(-1, 1, self.n_hidden)\n",
        "\n",
        "        # Calculate hidden layer output\n",
        "        H = np.tanh(X @ self.input_weights + self.biases)\n",
        "        # Calculate output weights\n",
        "        self.output_weights = np.linalg.pinv(H) @ y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        H = np.tanh(X @ self.input_weights + self.biases)\n",
        "        return H @ self.output_weights\n",
        "\n",
        "class GRNNRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, sigma=1.0):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "            weights = np.exp(-distances**2 / (2 * self.sigma**2))\n",
        "            weights = weights / np.sum(weights)\n",
        "            predictions.append(np.sum(weights * self.y_train))\n",
        "        return np.array(predictions)\n",
        "\n",
        "class CascadeCorrelationRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, max_hidden=10):\n",
        "        self.max_hidden = max_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simplified cascade correlation - using MLPRegressor as base\n",
        "        self.model = MLPRegressor(hidden_layer_sizes=(self.max_hidden,),\n",
        "                                 max_iter=500, random_state=42)\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "class DeepFeedforwardRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, hidden_layers=(50, 25), learning_rate=0.001, epochs=100):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Using MLPRegressor with specified architecture\n",
        "        self.model = MLPRegressor(\n",
        "            hidden_layer_sizes=self.hidden_layers,\n",
        "            learning_rate_init=self.learning_rate,\n",
        "            max_iter=self.epochs,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "class ResNetRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_blocks=2, hidden_size=50, learning_rate=0.001, epochs=100):\n",
        "        self.n_blocks = n_blocks\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simplified ResNet using MLPRegressor with deeper architecture\n",
        "        hidden_layers = tuple([self.hidden_size] * (self.n_blocks * 2))\n",
        "        self.model = MLPRegressor(\n",
        "            hidden_layer_sizes=hidden_layers,\n",
        "            learning_rate_init=self.learning_rate,\n",
        "            max_iter=self.epochs,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive regression metrics\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # MAPE\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    # AIC and BIC (simplified calculation)\n",
        "    n = len(y_true)\n",
        "    k = 2  # number of parameters (simplified)\n",
        "    aic = n * np.log(mse) + 2 * k\n",
        "    bic = n * np.log(mse) + k * np.log(n)\n",
        "\n",
        "    # COD (Coefficient of Determination)\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "        'COD': cod,\n",
        "        'R2': r2\n",
        "    }\n",
        "\n",
        "def optimize_model(params, model_class, param_names, X_train, y_train):\n",
        "    \"\"\"Optimization function for Bayesian optimization\"\"\"\n",
        "    param_dict = dict(zip(param_names, params))\n",
        "\n",
        "    try:\n",
        "        model = model_class(**param_dict)\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "        return -np.mean(scores)\n",
        "    except Exception as e:\n",
        "        return 1e6  # Return large value for failed optimization\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"Load and preprocess the dataset\"\"\"\n",
        "    # Load data\n",
        "    df = pd.read_csv('/content/Density_Prediction_Dataset.csv')\n",
        "\n",
        "    # Preprocessing\n",
        "    # 1. Label encoding for categorical variables\n",
        "    le_nano = LabelEncoder()\n",
        "    le_base = LabelEncoder()\n",
        "\n",
        "    df['Nano Particle'] = le_nano.fit_transform(df['Nano Particle'])\n",
        "    df['Base Fluid'] = le_base.fit_transform(df['Base Fluid'])\n",
        "\n",
        "    # 2. Separate features and target\n",
        "    X = df.drop('Density (ρ)', axis=1)\n",
        "    y = df['Density (ρ)']\n",
        "\n",
        "    # 3. Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 4. Standard scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    X_train, X_test, y_train, y_test = load_and_preprocess_data()\n",
        "\n",
        "    # Define models and their hyperparameter spaces\n",
        "    models = {\n",
        "        'SVR': {\n",
        "            'class': SVR,\n",
        "            'space': [\n",
        "                Real(0.1, 100, name='C'),\n",
        "                Real(0.001, 1, name='gamma'),\n",
        "                Real(0.01, 1, name='epsilon')\n",
        "            ],\n",
        "            'param_names': ['C', 'gamma', 'epsilon']\n",
        "        },\n",
        "        'Decision Tree': {\n",
        "            'class': DecisionTreeRegressor,\n",
        "            'space': [\n",
        "                Integer(5, 20, name='max_depth'),\n",
        "                Integer(2, 20, name='min_samples_split'),\n",
        "                Integer(1, 10, name='min_samples_leaf'),\n",
        "                Real(0.1, 1.0, name='max_features')\n",
        "            ],\n",
        "            'param_names': ['max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'class': RandomForestRegressor,\n",
        "            'space': [\n",
        "                Integer(50, 200, name='n_estimators'),\n",
        "                Integer(5, 20, name='max_depth'),\n",
        "                Integer(2, 10, name='min_samples_split'),\n",
        "                Integer(1, 5, name='min_samples_leaf')\n",
        "            ],\n",
        "            'param_names': ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']\n",
        "        },\n",
        "        'Extra Trees': {\n",
        "            'class': ExtraTreesRegressor,\n",
        "            'space': [\n",
        "                Integer(50, 300, name='n_estimators'),\n",
        "                Integer(5, 25, name='max_depth'),\n",
        "                Integer(2, 15, name='min_samples_split'),\n",
        "                Integer(1, 8, name='min_samples_leaf'),\n",
        "                Real(0.1, 1.0, name='max_features')\n",
        "            ],\n",
        "            'param_names': ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']\n",
        "        },\n",
        "        'Elastic Net': {\n",
        "            'class': ElasticNet,\n",
        "            'space': [\n",
        "                Real(0.0001, 10, name='alpha'),\n",
        "                Real(0.0, 1.0, name='l1_ratio'),\n",
        "                Integer(500, 2000, name='max_iter')\n",
        "            ],\n",
        "            'param_names': ['alpha', 'l1_ratio', 'max_iter']\n",
        "        },\n",
        "        'MLP': {\n",
        "            'class': MLPRegressor,\n",
        "            'space': [\n",
        "                Integer(50, 200, name='hidden_layer_sizes'),\n",
        "                Real(0.0001, 0.01, name='alpha'),\n",
        "                Real(0.001, 0.1, name='learning_rate_init')\n",
        "            ],\n",
        "            'param_names': ['hidden_layer_sizes', 'alpha', 'learning_rate_init']\n",
        "        },\n",
        "        'RBFN': {\n",
        "            'class': RBFNRegressor,\n",
        "            'space': [\n",
        "                Integer(5, 50, name='n_centers'),\n",
        "                Real(0.1, 10.0, name='gamma')\n",
        "            ],\n",
        "            'param_names': ['n_centers', 'gamma']\n",
        "        },\n",
        "        'ELM': {\n",
        "            'class': ELMRegressor,\n",
        "            'space': [\n",
        "                Integer(10, 500, name='n_hidden')\n",
        "            ],\n",
        "            'param_names': ['n_hidden']\n",
        "        },\n",
        "        'GRNN': {\n",
        "            'class': GRNNRegressor,\n",
        "            'space': [\n",
        "                Real(0.1, 5.0, name='sigma')\n",
        "            ],\n",
        "            'param_names': ['sigma']\n",
        "        },\n",
        "        'Cascade-Correlation': {\n",
        "            'class': CascadeCorrelationRegressor,\n",
        "            'space': [\n",
        "                Integer(2, 20, name='max_hidden')\n",
        "            ],\n",
        "            'param_names': ['max_hidden']\n",
        "        },\n",
        "        'Deep Feedforward': {\n",
        "            'class': DeepFeedforwardRegressor,\n",
        "            'space': [\n",
        "                Categorical([(20, 10), (50, 25), (100, 50, 25)], name='hidden_layers'),\n",
        "                Real(0.001, 0.1, name='learning_rate'),\n",
        "                Integer(50, 200, name='epochs')\n",
        "            ],\n",
        "            'param_names': ['hidden_layers', 'learning_rate', 'epochs']\n",
        "        },\n",
        "        'ResNet': {\n",
        "            'class': ResNetRegressor,\n",
        "            'space': [\n",
        "                Integer(1, 5, name='n_blocks'),\n",
        "                Integer(10, 100, name='hidden_size'),\n",
        "                Real(0.001, 0.1, name='learning_rate'),\n",
        "                Integer(50, 200, name='epochs')\n",
        "            ],\n",
        "            'param_names': ['n_blocks', 'hidden_size', 'learning_rate', 'epochs']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add XGBoost if available\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        models['XGBoost'] = {\n",
        "            'class': xgb.XGBRegressor,\n",
        "            'space': [\n",
        "                Integer(50, 200, name='n_estimators'),\n",
        "                Integer(3, 10, name='max_depth'),\n",
        "                Real(0.01, 0.3, name='learning_rate'),\n",
        "                Real(0.5, 1.0, name='subsample')\n",
        "            ],\n",
        "            'param_names': ['n_estimators', 'max_depth', 'learning_rate', 'subsample']\n",
        "        }\n",
        "\n",
        "    # Initialize results storage\n",
        "    performance_results = []\n",
        "    hyperparameter_results = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for model_name, model_config in models.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Optimizing {model_name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create optimization function\n",
        "        optimization_function = partial(\n",
        "            optimize_model,\n",
        "            model_class=model_config['class'],\n",
        "            param_names=model_config['param_names'],\n",
        "            X_train=X_train,\n",
        "            y_train=y_train\n",
        "        )\n",
        "\n",
        "        # Perform Bayesian optimization\n",
        "        try:\n",
        "            result = gp_minimize(\n",
        "                optimization_function,\n",
        "                dimensions=model_config['space'],\n",
        "                n_calls=15,\n",
        "                n_random_starts=10,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            # Get best parameters\n",
        "            best_params = dict(zip(model_config['param_names'], result.x))\n",
        "\n",
        "            # Train final model with best parameters\n",
        "            final_model = model_config['class'](**best_params)\n",
        "            final_model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = final_model.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "            # Calculate execution time\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            # Store results\n",
        "            performance_row = {\n",
        "                'Model': model_name,\n",
        "                'MSE': metrics['MSE'],\n",
        "                'MAE': metrics['MAE'],\n",
        "                'RMSE': metrics['RMSE'],\n",
        "                'MAPE': metrics['MAPE'],\n",
        "                'AIC': metrics['AIC'],\n",
        "                'BIC': metrics['BIC'],\n",
        "                'COD': metrics['COD'],\n",
        "                'R2': metrics['R2'],\n",
        "                'Execution_Time_Seconds': execution_time\n",
        "            }\n",
        "            performance_results.append(performance_row)\n",
        "\n",
        "            # Store hyperparameters\n",
        "            hyperparameter_row = {'Model': model_name, **best_params}\n",
        "            hyperparameter_results.append(hyperparameter_row)\n",
        "\n",
        "            print(f\"Best Parameters: {best_params}\")\n",
        "            print(f\"R2 Score: {metrics['R2']:.4f}\")\n",
        "            print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
        "            print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error optimizing {model_name}: {str(e)}\")\n",
        "            # Add placeholder results for failed models\n",
        "            performance_row = {\n",
        "                'Model': model_name,\n",
        "                'MSE': np.nan,\n",
        "                'MAE': np.nan,\n",
        "                'RMSE': np.nan,\n",
        "                'MAPE': np.nan,\n",
        "                'AIC': np.nan,\n",
        "                'BIC': np.nan,\n",
        "                'COD': np.nan,\n",
        "                'R2': np.nan,\n",
        "                'Execution_Time_Seconds': np.nan\n",
        "            }\n",
        "            performance_results.append(performance_row)\n",
        "\n",
        "            hyperparameter_row = {'Model': model_name, 'Error': str(e)}\n",
        "            hyperparameter_results.append(hyperparameter_row)\n",
        "\n",
        "    # Create DataFrames\n",
        "    performance_df = pd.DataFrame(performance_results)\n",
        "    hyperparameters_df = pd.DataFrame(hyperparameter_results)\n",
        "\n",
        "    # Sort by R2 score (descending)\n",
        "    performance_df = performance_df.sort_values('R2', ascending=False)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(performance_df.to_string(index=False))\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"OPTIMAL HYPERPARAMETERS SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(hyperparameters_df.to_string(index=False))\n",
        "\n",
        "    return performance_df, hyperparameters_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    performance_df, hyperparameters_df = main()\n",
        "\n",
        "    # Save results to CSV files\n",
        "    performance_df.to_csv('performance_metrics.csv', index=False)\n",
        "    hyperparameters_df.to_csv('optimal_hyperparameters.csv', index=False)\n",
        "\n",
        "    print(\"\\nResults saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCsMPgdrvnES",
        "outputId": "08b5fb9a-e170-45b7-8228-6b94dfbf7272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Optimizing SVR\n",
            "==================================================\n",
            "Best Parameters: {'C': 59.725330778854065, 'gamma': 0.44638692010073766, 'epsilon': 0.10897516665982288}\n",
            "R2 Score: 0.9538\n",
            "RMSE: 5.4345\n",
            "Execution Time: 16.53 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Decision Tree\n",
            "==================================================\n",
            "Best Parameters: {'max_depth': np.int64(14), 'min_samples_split': np.int64(2), 'min_samples_leaf': np.int64(1), 'max_features': 0.5671154177147018}\n",
            "R2 Score: 0.9533\n",
            "RMSE: 5.4623\n",
            "Execution Time: 12.80 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Random Forest\n",
            "==================================================\n",
            "Best Parameters: {'n_estimators': np.int64(50), 'max_depth': np.int64(20), 'min_samples_split': np.int64(7), 'min_samples_leaf': np.int64(2)}\n",
            "R2 Score: 0.9637\n",
            "RMSE: 4.8128\n",
            "Execution Time: 16.10 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Extra Trees\n",
            "==================================================\n",
            "Best Parameters: {'n_estimators': np.int64(175), 'max_depth': np.int64(17), 'min_samples_split': np.int64(2), 'min_samples_leaf': np.int64(1), 'max_features': 0.5823894389036101}\n",
            "R2 Score: 0.9796\n",
            "RMSE: 3.6128\n",
            "Execution Time: 17.59 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Elastic Net\n",
            "==================================================\n",
            "Best Parameters: {'alpha': 0.0001, 'l1_ratio': 1.0, 'max_iter': np.int64(2000)}\n",
            "R2 Score: 0.9204\n",
            "RMSE: 7.1285\n",
            "Execution Time: 3.95 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing MLP\n",
            "==================================================\n",
            "Best Parameters: {'hidden_layer_sizes': np.int64(191), 'alpha': 0.00013856937278201907, 'learning_rate_init': 0.0991297666749273}\n",
            "R2 Score: 0.3505\n",
            "RMSE: 20.3654\n",
            "Execution Time: 21.35 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing RBFN\n",
            "==================================================\n",
            "Best Parameters: {'n_centers': np.int64(50), 'gamma': 0.1}\n",
            "R2 Score: 0.7257\n",
            "RMSE: 13.2344\n",
            "Execution Time: 2.61 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing ELM\n",
            "==================================================\n",
            "Best Parameters: {'n_hidden': np.int64(211)}\n",
            "R2 Score: 0.9819\n",
            "RMSE: 3.4039\n",
            "Execution Time: 6.12 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing GRNN\n",
            "==================================================\n",
            "Best Parameters: {'sigma': 0.48232075023381105}\n",
            "R2 Score: 0.9263\n",
            "RMSE: 6.8594\n",
            "Execution Time: 3.43 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Cascade-Correlation\n",
            "==================================================\n",
            "Best Parameters: {'max_hidden': np.int64(18)}\n",
            "R2 Score: -1344.1496\n",
            "RMSE: 926.8237\n",
            "Execution Time: 26.41 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Deep Feedforward\n",
            "==================================================\n",
            "Error optimizing Deep Feedforward: operands could not be broadcast together with shapes (3,) (2,) \n",
            "\n",
            "==================================================\n",
            "Optimizing ResNet\n",
            "==================================================\n",
            "Best Parameters: {'n_blocks': np.int64(1), 'hidden_size': np.int64(100), 'learning_rate': 0.1, 'epochs': np.int64(200)}\n",
            "R2 Score: 0.8525\n",
            "RMSE: 9.7040\n",
            "Execution Time: 52.20 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing XGBoost\n",
            "==================================================\n",
            "Best Parameters: {'n_estimators': np.int64(200), 'max_depth': np.int64(3), 'learning_rate': 0.2981946592549217, 'subsample': 0.8295659527741268}\n",
            "R2 Score: 0.9902\n",
            "RMSE: 2.5048\n",
            "Execution Time: 8.28 seconds\n",
            "\n",
            "======================================================================\n",
            "PERFORMANCE METRICS SUMMARY\n",
            "======================================================================\n",
            "              Model           MSE        MAE       RMSE      MAPE         AIC         BIC          COD           R2  Execution_Time_Seconds\n",
            "            XGBoost      6.274145   1.123831   2.504824  0.109164  165.606475  170.561149     0.990175     0.990175                8.278491\n",
            "                ELM     11.586249   1.328839   3.403858  0.129018  219.584068  224.538742     0.981857     0.981857                6.115402\n",
            "        Extra Trees     13.052109   1.941388   3.612770  0.188241  230.067576  235.022249     0.979561     0.979561               17.592394\n",
            "      Random Forest     23.163244   3.120706   4.812821  0.302225  280.545869  285.500543     0.963728     0.963728               16.103668\n",
            "                SVR     29.534079   2.470017   5.434527  0.236044  301.927945  306.882619     0.953751     0.953751               16.532035\n",
            "      Decision Tree     29.836295   3.609640   5.462261  0.350990  302.823853  307.778526     0.953278     0.953278               12.804042\n",
            "               GRNN     47.051399   4.209434   6.859402  0.406944  342.909173  347.863847     0.926320     0.926320                3.427243\n",
            "        Elastic Net     50.815042   4.924146   7.128467  0.478336  349.680933  354.635607     0.920426     0.920426                3.954930\n",
            "             ResNet     94.167068   6.553087   9.703972  0.639695  403.966206  408.920879     0.852540     0.852540               52.197075\n",
            "               RBFN    175.149034   9.529371  13.234388  0.937350  458.576077  463.530750     0.725726     0.725726                2.608005\n",
            "                MLP    414.748088  14.238274  20.365365  1.402003  534.435076  539.389750     0.350528     0.350528               21.349064\n",
            "Cascade-Correlation 859002.087378 925.001366 926.823655 90.725729 1206.390344 1211.345017 -1344.149569 -1344.149569               26.413537\n",
            "   Deep Feedforward           NaN        NaN        NaN       NaN         NaN         NaN          NaN          NaN                     NaN\n",
            "\n",
            "======================================================================\n",
            "OPTIMAL HYPERPARAMETERS SUMMARY\n",
            "======================================================================\n",
            "              Model         C    gamma  epsilon  max_depth  min_samples_split  min_samples_leaf  max_features  n_estimators    alpha  l1_ratio  max_iter  hidden_layer_sizes  learning_rate_init  n_centers  n_hidden    sigma  max_hidden                                                           Error  n_blocks  hidden_size  learning_rate  epochs  subsample\n",
            "                SVR 59.725331 0.446387 0.108975        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN      NaN         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "      Decision Tree       NaN      NaN      NaN       14.0                2.0               1.0      0.567115           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN      NaN         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "      Random Forest       NaN      NaN      NaN       20.0                7.0               2.0           NaN          50.0      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN      NaN         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "        Extra Trees       NaN      NaN      NaN       17.0                2.0               1.0      0.582389         175.0      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN      NaN         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "        Elastic Net       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN 0.000100       1.0    2000.0                 NaN                 NaN        NaN       NaN      NaN         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "                MLP       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN 0.000139       NaN       NaN               191.0             0.09913        NaN       NaN      NaN         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "               RBFN       NaN 0.100000      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN       50.0       NaN      NaN         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "                ELM       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN     211.0      NaN         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "               GRNN       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN 0.482321         NaN                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "Cascade-Correlation       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN      NaN        18.0                                                             NaN       NaN          NaN            NaN     NaN        NaN\n",
            "   Deep Feedforward       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN      NaN         NaN operands could not be broadcast together with shapes (3,) (2,)        NaN          NaN            NaN     NaN        NaN\n",
            "             ResNet       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN      NaN         NaN                                                             NaN       1.0        100.0       0.100000   200.0        NaN\n",
            "            XGBoost       NaN      NaN      NaN        3.0                NaN               NaN           NaN         200.0      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN      NaN         NaN                                                             NaN       NaN          NaN       0.298195     NaN   0.829566\n",
            "\n",
            "Results saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from functools import partial\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real, Categorical\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available, skipping XGBoost regressor\")\n",
        "\n",
        "# Custom implementations for specialized regressors\n",
        "class RBFNRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_centers=10, gamma=1.0):\n",
        "        self.n_centers = n_centers\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        # Select centers randomly from training data\n",
        "        idx = np.random.choice(len(X), min(self.n_centers, len(X)), replace=False)\n",
        "        self.centers = X[idx]\n",
        "\n",
        "        # Calculate RBF weights\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centers, axis=2)\n",
        "        phi = np.exp(-self.gamma * distances**2)\n",
        "        self.weights = np.linalg.pinv(phi) @ y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centers, axis=2)\n",
        "        phi = np.exp(-self.gamma * distances**2)\n",
        "        return phi @ self.weights\n",
        "\n",
        "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_hidden=100):\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = X.shape[1]\n",
        "        self.input_weights = np.random.uniform(-1, 1, (n_features, self.n_hidden))\n",
        "        self.biases = np.random.uniform(-1, 1, self.n_hidden)\n",
        "\n",
        "        # Calculate hidden layer output\n",
        "        H = np.tanh(X @ self.input_weights + self.biases)\n",
        "        # Calculate output weights\n",
        "        self.output_weights = np.linalg.pinv(H) @ y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        H = np.tanh(X @ self.input_weights + self.biases)\n",
        "        return H @ self.output_weights\n",
        "\n",
        "class GRNNRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, sigma=1.0):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "            weights = np.exp(-distances**2 / (2 * self.sigma**2))\n",
        "            weights = weights / np.sum(weights)\n",
        "            predictions.append(np.sum(weights * self.y_train))\n",
        "        return np.array(predictions)\n",
        "\n",
        "class CascadeCorrelationRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, max_hidden=10):\n",
        "        self.max_hidden = max_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simplified cascade correlation - using MLPRegressor as base\n",
        "        self.model = MLPRegressor(hidden_layer_sizes=(self.max_hidden,),\n",
        "                                 max_iter=500, random_state=42)\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "class DeepFeedforwardRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, hidden_layers=(50, 25), learning_rate=0.001, epochs=100):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Using MLPRegressor with specified architecture\n",
        "        self.model = MLPRegressor(\n",
        "            hidden_layer_sizes=self.hidden_layers,\n",
        "            learning_rate_init=self.learning_rate,\n",
        "            max_iter=self.epochs,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "class ResNetRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_blocks=2, hidden_size=50, learning_rate=0.001, epochs=100):\n",
        "        self.n_blocks = n_blocks\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simplified ResNet using MLPRegressor with deeper architecture\n",
        "        hidden_layers = tuple([self.hidden_size] * (self.n_blocks * 2))\n",
        "        self.model = MLPRegressor(\n",
        "            hidden_layer_sizes=hidden_layers,\n",
        "            learning_rate_init=self.learning_rate,\n",
        "            max_iter=self.epochs,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive regression metrics\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # MAPE\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    # AIC and BIC (simplified calculation)\n",
        "    n = len(y_true)\n",
        "    k = 2  # number of parameters (simplified)\n",
        "    aic = n * np.log(mse) + 2 * k\n",
        "    bic = n * np.log(mse) + k * np.log(n)\n",
        "\n",
        "    # COD (Coefficient of Determination)\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "        'COD': cod,\n",
        "        'R2': r2\n",
        "    }\n",
        "\n",
        "def optimize_model(params, model_class, param_names, X_train, y_train):\n",
        "    \"\"\"Optimization function for Bayesian optimization\"\"\"\n",
        "    param_dict = dict(zip(param_names, params))\n",
        "\n",
        "    try:\n",
        "        model = model_class(**param_dict)\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "        return -np.mean(scores)\n",
        "    except Exception as e:\n",
        "        return 1e6  # Return large value for failed optimization\n",
        "\n",
        "def detect_outliers_iqr(df, columns):\n",
        "    \"\"\"Detect outliers using IQR method\"\"\"\n",
        "    outlier_indices = set()\n",
        "\n",
        "    for column in columns:\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Define outlier bounds\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Find outliers\n",
        "        column_outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)].index\n",
        "        outlier_indices.update(column_outliers)\n",
        "\n",
        "        print(f\"Column '{column}': Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}\")\n",
        "        print(f\"  Outlier bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "        print(f\"  Outliers found: {len(column_outliers)}\")\n",
        "\n",
        "    return list(outlier_indices)\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"Load and preprocess the dataset with outlier detection and removal\"\"\"\n",
        "    # Load data\n",
        "    df = pd.read_csv('/content/Density_Prediction_Dataset.csv')\n",
        "\n",
        "    print(\"Dataset shape before preprocessing:\", df.shape)\n",
        "    print(\"\\nDataset info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    # Preprocessing\n",
        "    # 1. Outlier detection and removal using IQR method\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OUTLIER DETECTION AND REMOVAL (IQR METHOD)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Define numerical columns for outlier detection\n",
        "    numerical_columns = [\n",
        "        'Temperature (°C)',\n",
        "        'Volume Concentration (ϕ)',\n",
        "        'Density of Nano Particle 1 (ρnp)',\n",
        "        'Density of Nano Particle 2 (ρnp)',\n",
        "        'Density of Base Fluid (ρbf)',\n",
        "        'Volume Mixture of Particle 1',\n",
        "        'Volume Mixture of Particle 2',\n",
        "        'Density (ρ)'\n",
        "    ]\n",
        "\n",
        "    # Detect outliers\n",
        "    outlier_indices = detect_outliers_iqr(df, numerical_columns)\n",
        "\n",
        "    print(f\"\\nTotal unique outliers found: {len(outlier_indices)}\")\n",
        "    print(f\"Percentage of outliers: {len(outlier_indices)/len(df)*100:.2f}%\")\n",
        "\n",
        "    # Remove outliers\n",
        "    df_cleaned = df.drop(outlier_indices).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Dataset shape after outlier removal: {df_cleaned.shape}\")\n",
        "    print(f\"Rows removed: {len(df) - len(df_cleaned)}\")\n",
        "\n",
        "    # 2. Label encoding for categorical variables\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"LABEL ENCODING\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    le_nano = LabelEncoder()\n",
        "    le_base = LabelEncoder()\n",
        "\n",
        "    print(\"Unique values before encoding:\")\n",
        "    print(f\"Nano Particle: {df_cleaned['Nano Particle'].unique()}\")\n",
        "    print(f\"Base Fluid: {df_cleaned['Base Fluid'].unique()}\")\n",
        "\n",
        "    df_cleaned['Nano Particle'] = le_nano.fit_transform(df_cleaned['Nano Particle'])\n",
        "    df_cleaned['Base Fluid'] = le_base.fit_transform(df_cleaned['Base Fluid'])\n",
        "\n",
        "    print(\"Encoded values:\")\n",
        "    print(f\"Nano Particle: {df_cleaned['Nano Particle'].unique()}\")\n",
        "    print(f\"Base Fluid: {df_cleaned['Base Fluid'].unique()}\")\n",
        "\n",
        "    # 3. Separate features and target\n",
        "    X = df_cleaned.drop('Density (ρ)', axis=1)\n",
        "    y = df_cleaned['Density (ρ)']\n",
        "\n",
        "    print(f\"\\nFeatures shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "    # 4. Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"\\nTrain set shape: {X_train.shape}\")\n",
        "    print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "    # 5. Standard scaling\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"STANDARD SCALING\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(\"Features before scaling (first 5 rows of training set):\")\n",
        "    print(X_train.head())\n",
        "    print(\"\\nFeatures after scaling (first 5 rows of training set):\")\n",
        "    print(pd.DataFrame(X_train_scaled[:5], columns=X_train.columns))\n",
        "\n",
        "    # Display scaling statistics\n",
        "    print(f\"\\nScaling statistics:\")\n",
        "    print(f\"Mean of scaled features: {np.mean(X_train_scaled, axis=0)}\")\n",
        "    print(f\"Std of scaled features: {np.std(X_train_scaled, axis=0)}\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    X_train, X_test, y_train, y_test = load_and_preprocess_data()\n",
        "\n",
        "    # Define models and their hyperparameter spaces\n",
        "    models = {\n",
        "        'SVR': {\n",
        "            'class': SVR,\n",
        "            'space': [\n",
        "                Real(0.1, 100, name='C'),\n",
        "                Real(0.001, 1, name='gamma'),\n",
        "                Real(0.01, 1, name='epsilon')\n",
        "            ],\n",
        "            'param_names': ['C', 'gamma', 'epsilon']\n",
        "        },\n",
        "        'Decision Tree': {\n",
        "            'class': DecisionTreeRegressor,\n",
        "            'space': [\n",
        "                Integer(5, 20, name='max_depth'),\n",
        "                Integer(2, 20, name='min_samples_split'),\n",
        "                Integer(1, 10, name='min_samples_leaf'),\n",
        "                Real(0.1, 1.0, name='max_features')\n",
        "            ],\n",
        "            'param_names': ['max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'class': RandomForestRegressor,\n",
        "            'space': [\n",
        "                Integer(50, 200, name='n_estimators'),\n",
        "                Integer(5, 20, name='max_depth'),\n",
        "                Integer(2, 10, name='min_samples_split'),\n",
        "                Integer(1, 5, name='min_samples_leaf')\n",
        "            ],\n",
        "            'param_names': ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']\n",
        "        },\n",
        "        'Extra Trees': {\n",
        "            'class': ExtraTreesRegressor,\n",
        "            'space': [\n",
        "                Integer(50, 300, name='n_estimators'),\n",
        "                Integer(5, 25, name='max_depth'),\n",
        "                Integer(2, 15, name='min_samples_split'),\n",
        "                Integer(1, 8, name='min_samples_leaf'),\n",
        "                Real(0.1, 1.0, name='max_features')\n",
        "            ],\n",
        "            'param_names': ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']\n",
        "        },\n",
        "        'Elastic Net': {\n",
        "            'class': ElasticNet,\n",
        "            'space': [\n",
        "                Real(0.0001, 10, name='alpha'),\n",
        "                Real(0.0, 1.0, name='l1_ratio'),\n",
        "                Integer(500, 2000, name='max_iter')\n",
        "            ],\n",
        "            'param_names': ['alpha', 'l1_ratio', 'max_iter']\n",
        "        },\n",
        "        'MLP': {\n",
        "            'class': MLPRegressor,\n",
        "            'space': [\n",
        "                Integer(50, 200, name='hidden_layer_sizes'),\n",
        "                Real(0.0001, 0.01, name='alpha'),\n",
        "                Real(0.001, 0.1, name='learning_rate_init')\n",
        "            ],\n",
        "            'param_names': ['hidden_layer_sizes', 'alpha', 'learning_rate_init']\n",
        "        },\n",
        "        'RBFN': {\n",
        "            'class': RBFNRegressor,\n",
        "            'space': [\n",
        "                Integer(5, 50, name='n_centers'),\n",
        "                Real(0.1, 10.0, name='gamma')\n",
        "            ],\n",
        "            'param_names': ['n_centers', 'gamma']\n",
        "        },\n",
        "        'ELM': {\n",
        "            'class': ELMRegressor,\n",
        "            'space': [\n",
        "                Integer(10, 500, name='n_hidden')\n",
        "            ],\n",
        "            'param_names': ['n_hidden']\n",
        "        },\n",
        "        'GRNN': {\n",
        "            'class': GRNNRegressor,\n",
        "            'space': [\n",
        "                Real(0.1, 5.0, name='sigma')\n",
        "            ],\n",
        "            'param_names': ['sigma']\n",
        "        },\n",
        "        'Cascade-Correlation': {\n",
        "            'class': CascadeCorrelationRegressor,\n",
        "            'space': [\n",
        "                Integer(2, 20, name='max_hidden')\n",
        "            ],\n",
        "            'param_names': ['max_hidden']\n",
        "        },\n",
        "        'Deep Feedforward': {\n",
        "            'class': DeepFeedforwardRegressor,\n",
        "            'space': [\n",
        "                Categorical([(20, 10), (50, 25), (100, 50, 25)], name='hidden_layers'),\n",
        "                Real(0.001, 0.1, name='learning_rate'),\n",
        "                Integer(50, 200, name='epochs')\n",
        "            ],\n",
        "            'param_names': ['hidden_layers', 'learning_rate', 'epochs']\n",
        "        },\n",
        "        'ResNet': {\n",
        "            'class': ResNetRegressor,\n",
        "            'space': [\n",
        "                Integer(1, 5, name='n_blocks'),\n",
        "                Integer(10, 100, name='hidden_size'),\n",
        "                Real(0.001, 0.1, name='learning_rate'),\n",
        "                Integer(50, 200, name='epochs')\n",
        "            ],\n",
        "            'param_names': ['n_blocks', 'hidden_size', 'learning_rate', 'epochs']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add XGBoost if available\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        models['XGBoost'] = {\n",
        "            'class': xgb.XGBRegressor,\n",
        "            'space': [\n",
        "                Integer(50, 200, name='n_estimators'),\n",
        "                Integer(3, 10, name='max_depth'),\n",
        "                Real(0.01, 0.3, name='learning_rate'),\n",
        "                Real(0.5, 1.0, name='subsample')\n",
        "            ],\n",
        "            'param_names': ['n_estimators', 'max_depth', 'learning_rate', 'subsample']\n",
        "        }\n",
        "\n",
        "    # Initialize results storage\n",
        "    performance_results = []\n",
        "    hyperparameter_results = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for model_name, model_config in models.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Optimizing {model_name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create optimization function\n",
        "        optimization_function = partial(\n",
        "            optimize_model,\n",
        "            model_class=model_config['class'],\n",
        "            param_names=model_config['param_names'],\n",
        "            X_train=X_train,\n",
        "            y_train=y_train\n",
        "        )\n",
        "\n",
        "        # Perform Bayesian optimization\n",
        "        try:\n",
        "            result = gp_minimize(\n",
        "                optimization_function,\n",
        "                dimensions=model_config['space'],\n",
        "                n_calls=15,\n",
        "                n_random_starts=10,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            # Get best parameters\n",
        "            best_params = dict(zip(model_config['param_names'], result.x))\n",
        "\n",
        "            # Train final model with best parameters\n",
        "            final_model = model_config['class'](**best_params)\n",
        "            final_model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = final_model.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "            # Calculate execution time\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            # Store results\n",
        "            performance_row = {\n",
        "                'Model': model_name,\n",
        "                'MSE': metrics['MSE'],\n",
        "                'MAE': metrics['MAE'],\n",
        "                'RMSE': metrics['RMSE'],\n",
        "                'MAPE': metrics['MAPE'],\n",
        "                'AIC': metrics['AIC'],\n",
        "                'BIC': metrics['BIC'],\n",
        "                'COD': metrics['COD'],\n",
        "                'R2': metrics['R2'],\n",
        "                'Execution_Time_Seconds': execution_time\n",
        "            }\n",
        "            performance_results.append(performance_row)\n",
        "\n",
        "            # Store hyperparameters\n",
        "            hyperparameter_row = {'Model': model_name, **best_params}\n",
        "            hyperparameter_results.append(hyperparameter_row)\n",
        "\n",
        "            print(f\"Best Parameters: {best_params}\")\n",
        "            print(f\"R2 Score: {metrics['R2']:.4f}\")\n",
        "            print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
        "            print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error optimizing {model_name}: {str(e)}\")\n",
        "            # Add placeholder results for failed models\n",
        "            performance_row = {\n",
        "                'Model': model_name,\n",
        "                'MSE': np.nan,\n",
        "                'MAE': np.nan,\n",
        "                'RMSE': np.nan,\n",
        "                'MAPE': np.nan,\n",
        "                'AIC': np.nan,\n",
        "                'BIC': np.nan,\n",
        "                'COD': np.nan,\n",
        "                'R2': np.nan,\n",
        "                'Execution_Time_Seconds': np.nan\n",
        "            }\n",
        "            performance_results.append(performance_row)\n",
        "\n",
        "            hyperparameter_row = {'Model': model_name, 'Error': str(e)}\n",
        "            hyperparameter_results.append(hyperparameter_row)\n",
        "\n",
        "    # Create DataFrames\n",
        "    performance_df = pd.DataFrame(performance_results)\n",
        "    hyperparameters_df = pd.DataFrame(hyperparameter_results)\n",
        "\n",
        "    # Sort by R2 score (descending)\n",
        "    performance_df = performance_df.sort_values('R2', ascending=False)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(performance_df.to_string(index=False))\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"OPTIMAL HYPERPARAMETERS SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(hyperparameters_df.to_string(index=False))\n",
        "\n",
        "    return performance_df, hyperparameters_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    performance_df, hyperparameters_df = main()\n",
        "\n",
        "    # Save results to CSV files\n",
        "    performance_df.to_csv('performance_metrics.csv', index=False)\n",
        "    hyperparameters_df.to_csv('optimal_hyperparameters.csv', index=False)\n",
        "\n",
        "    print(\"\\nResults saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anK_cZ8XvnHD",
        "outputId": "6c55dbbf-46bb-4eed-a64f-af2936b2c407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape before preprocessing: (436, 10)\n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 436 entries, 0 to 435\n",
            "Data columns (total 10 columns):\n",
            " #   Column                            Non-Null Count  Dtype  \n",
            "---  ------                            --------------  -----  \n",
            " 0   Nano Particle                     436 non-null    object \n",
            " 1   Base Fluid                        436 non-null    object \n",
            " 2   Temperature (°C)                  436 non-null    int64  \n",
            " 3   Volume Concentration (ϕ)          436 non-null    float64\n",
            " 4   Density of Nano Particle 1 (ρnp)  436 non-null    int64  \n",
            " 5   Density of Nano Particle 2 (ρnp)  436 non-null    int64  \n",
            " 6   Density of Base Fluid (ρbf)       436 non-null    float64\n",
            " 7   Volume Mixture of Particle 1      436 non-null    int64  \n",
            " 8   Volume Mixture of Particle 2      436 non-null    int64  \n",
            " 9   Density (ρ)                       436 non-null    float64\n",
            "dtypes: float64(3), int64(5), object(2)\n",
            "memory usage: 34.2+ KB\n",
            "None\n",
            "\n",
            "============================================================\n",
            "OUTLIER DETECTION AND REMOVAL (IQR METHOD)\n",
            "============================================================\n",
            "Column 'Temperature (°C)': Q1=30.00, Q3=50.00, IQR=20.00\n",
            "  Outlier bounds: [0.00, 80.00]\n",
            "  Outliers found: 0\n",
            "Column 'Volume Concentration (ϕ)': Q1=0.10, Q3=1.00, IQR=0.90\n",
            "  Outlier bounds: [-1.25, 2.35]\n",
            "  Outliers found: 0\n",
            "Column 'Density of Nano Particle 1 (ρnp)': Q1=4320.00, Q3=5810.00, IQR=1490.00\n",
            "  Outlier bounds: [2085.00, 8045.00]\n",
            "  Outliers found: 30\n",
            "Column 'Density of Nano Particle 2 (ρnp)': Q1=2100.00, Q3=2220.00, IQR=120.00\n",
            "  Outlier bounds: [1920.00, 2400.00]\n",
            "  Outliers found: 100\n",
            "Column 'Density of Base Fluid (ρbf)': Q1=990.22, Q3=997.13, IQR=6.91\n",
            "  Outlier bounds: [979.86, 1007.49]\n",
            "  Outliers found: 56\n",
            "Column 'Volume Mixture of Particle 1': Q1=20.00, Q3=80.00, IQR=60.00\n",
            "  Outlier bounds: [-70.00, 170.00]\n",
            "  Outliers found: 0\n",
            "Column 'Volume Mixture of Particle 2': Q1=20.00, Q3=80.00, IQR=60.00\n",
            "  Outlier bounds: [-70.00, 170.00]\n",
            "  Outliers found: 0\n",
            "Column 'Density (ρ)': Q1=998.69, Q3=1028.17, IQR=29.49\n",
            "  Outlier bounds: [954.46, 1072.40]\n",
            "  Outliers found: 13\n",
            "\n",
            "Total unique outliers found: 160\n",
            "Percentage of outliers: 36.70%\n",
            "Dataset shape after outlier removal: (276, 10)\n",
            "Rows removed: 160\n",
            "\n",
            "============================================================\n",
            "LABEL ENCODING\n",
            "============================================================\n",
            "Unique values before encoding:\n",
            "Nano Particle: ['Al₂O₃/SiO₂ ' 'Fe₃O₄-MWCNT ' 'Al₂O₃-CNT ' 'Al₂O₃-MWCNT ' 'TiO₂-MWCNT '\n",
            " 'CeO₂-MWCNT ' 'ZnO-MWCNT ' 'MgO-MWCNT ' 'CuO-MWCNT ']\n",
            "Base Fluid: ['Water']\n",
            "Encoded values:\n",
            "Nano Particle: [2 5 0 1 7 3 8 6 4]\n",
            "Base Fluid: [0]\n",
            "\n",
            "Features shape: (276, 9)\n",
            "Target shape: (276,)\n",
            "\n",
            "Train set shape: (220, 9)\n",
            "Test set shape: (56, 9)\n",
            "\n",
            "============================================================\n",
            "STANDARD SCALING\n",
            "============================================================\n",
            "Features before scaling (first 5 rows of training set):\n",
            "     Nano Particle  Base Fluid  Temperature (°C)  Volume Concentration (ϕ)  \\\n",
            "258              8           0                35                      2.00   \n",
            "232              8           0                25                      1.00   \n",
            "33               5           0                20                      0.10   \n",
            "157              7           0                40                      1.75   \n",
            "148              7           0                25                      1.50   \n",
            "\n",
            "     Density of Nano Particle 1 (ρnp)  Density of Nano Particle 2 (ρnp)  \\\n",
            "258                              5600                              2100   \n",
            "232                              5600                              2100   \n",
            "33                               5810                              2100   \n",
            "157                              4320                              2100   \n",
            "148                              4320                              2100   \n",
            "\n",
            "     Density of Base Fluid (ρbf)  Volume Mixture of Particle 1  \\\n",
            "258                       994.08                            80   \n",
            "232                       997.13                            80   \n",
            "33                        998.50                            74   \n",
            "157                       992.25                            80   \n",
            "148                       997.13                            80   \n",
            "\n",
            "     Volume Mixture of Particle 2  \n",
            "258                            20  \n",
            "232                            20  \n",
            "33                             26  \n",
            "157                            20  \n",
            "148                            20  \n",
            "\n",
            "Features after scaling (first 5 rows of training set):\n",
            "   Nano Particle  Base Fluid  Temperature (°C)  Volume Concentration (ϕ)  \\\n",
            "0       1.354819         0.0         -0.402594                  1.733708   \n",
            "1       1.354819         0.0         -1.403393                  0.203945   \n",
            "2       0.212826         0.0         -1.903793                 -1.172841   \n",
            "3       0.974155         0.0          0.097805                  1.351267   \n",
            "4       0.974155         0.0         -1.403393                  0.968827   \n",
            "\n",
            "   Density of Nano Particle 1 (ρnp)  Density of Nano Particle 2 (ρnp)  \\\n",
            "0                          0.208791                         -0.240192   \n",
            "1                          0.208791                         -0.240192   \n",
            "2                          0.364474                         -0.240192   \n",
            "3                         -0.740135                         -0.240192   \n",
            "4                         -0.740135                         -0.240192   \n",
            "\n",
            "   Density of Base Fluid (ρbf)  Volume Mixture of Particle 1  \\\n",
            "0                     0.484618                      0.364927   \n",
            "1                     1.287074                      0.364927   \n",
            "2                     1.647521                     -0.055408   \n",
            "3                     0.003145                      0.364927   \n",
            "4                     1.287074                      0.364927   \n",
            "\n",
            "   Volume Mixture of Particle 2  \n",
            "0                     -0.364927  \n",
            "1                     -0.364927  \n",
            "2                      0.055408  \n",
            "3                     -0.364927  \n",
            "4                     -0.364927  \n",
            "\n",
            "Scaling statistics:\n",
            "Mean of scaled features: [ 1.77635684e-16  0.00000000e+00 -6.45947942e-17 -3.22973971e-17\n",
            "  2.96732336e-16  1.42915982e-15  2.09933081e-14 -3.67382892e-16\n",
            " -1.49375461e-16]\n",
            "Std of scaled features: [1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "==================================================\n",
            "Optimizing SVR\n",
            "==================================================\n",
            "Best Parameters: {'C': 79.67464438733728, 'gamma': 0.18425135507629767, 'epsilon': 0.7818940902700418}\n",
            "R2 Score: 0.9709\n",
            "RMSE: 2.2576\n",
            "Execution Time: 10.75 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Decision Tree\n",
            "==================================================\n",
            "Best Parameters: {'max_depth': np.int64(20), 'min_samples_split': np.int64(2), 'min_samples_leaf': np.int64(1), 'max_features': 0.6679608563374405}\n",
            "R2 Score: 0.8936\n",
            "RMSE: 4.3132\n",
            "Execution Time: 5.70 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Random Forest\n",
            "==================================================\n",
            "Best Parameters: {'n_estimators': np.int64(50), 'max_depth': np.int64(20), 'min_samples_split': np.int64(3), 'min_samples_leaf': np.int64(1)}\n",
            "R2 Score: 0.9697\n",
            "RMSE: 2.3026\n",
            "Execution Time: 15.32 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Extra Trees\n",
            "==================================================\n",
            "Best Parameters: {'n_estimators': np.int64(167), 'max_depth': np.int64(23), 'min_samples_split': np.int64(3), 'min_samples_leaf': np.int64(1), 'max_features': 0.5759294466406624}\n",
            "R2 Score: 0.9652\n",
            "RMSE: 2.4661\n",
            "Execution Time: 16.44 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Elastic Net\n",
            "==================================================\n",
            "Best Parameters: {'alpha': 0.0001, 'l1_ratio': 0.8920544216017461, 'max_iter': np.int64(500)}\n",
            "R2 Score: 0.8327\n",
            "RMSE: 5.4095\n",
            "Execution Time: 3.94 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing MLP\n",
            "==================================================\n",
            "Best Parameters: {'hidden_layer_sizes': np.int64(191), 'alpha': 0.00010770978182604186, 'learning_rate_init': 0.09922894436983057}\n",
            "R2 Score: -0.2831\n",
            "RMSE: 14.9790\n",
            "Execution Time: 14.42 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing RBFN\n",
            "==================================================\n",
            "Best Parameters: {'n_centers': np.int64(43), 'gamma': 0.1}\n",
            "R2 Score: -24.5596\n",
            "RMSE: 66.8551\n",
            "Execution Time: 3.04 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing ELM\n",
            "==================================================\n",
            "Best Parameters: {'n_hidden': np.int64(98)}\n",
            "R2 Score: 0.7862\n",
            "RMSE: 6.1145\n",
            "Execution Time: 3.21 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing GRNN\n",
            "==================================================\n",
            "Best Parameters: {'sigma': 0.370309938704223}\n",
            "R2 Score: 0.9345\n",
            "RMSE: 3.3839\n",
            "Execution Time: 4.28 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Cascade-Correlation\n",
            "==================================================\n",
            "Best Parameters: {'max_hidden': np.int64(20)}\n",
            "R2 Score: -5257.1572\n",
            "RMSE: 958.9019\n",
            "Execution Time: 16.98 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing Deep Feedforward\n",
            "==================================================\n",
            "Error optimizing Deep Feedforward: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
            "\n",
            "==================================================\n",
            "Optimizing ResNet\n",
            "==================================================\n",
            "Best Parameters: {'n_blocks': np.int64(1), 'hidden_size': np.int64(100), 'learning_rate': 0.1, 'epochs': np.int64(200)}\n",
            "R2 Score: 0.8055\n",
            "RMSE: 5.8326\n",
            "Execution Time: 17.67 seconds\n",
            "\n",
            "==================================================\n",
            "Optimizing XGBoost\n",
            "==================================================\n",
            "Best Parameters: {'n_estimators': np.int64(174), 'max_depth': np.int64(3), 'learning_rate': 0.23910743044414404, 'subsample': 0.800781412605054}\n",
            "R2 Score: 0.9930\n",
            "RMSE: 1.1103\n",
            "Execution Time: 7.39 seconds\n",
            "\n",
            "======================================================================\n",
            "PERFORMANCE METRICS SUMMARY\n",
            "======================================================================\n",
            "              Model           MSE        MAE       RMSE      MAPE        AIC        BIC          COD           R2  Execution_Time_Seconds\n",
            "            XGBoost      1.232786   0.861897   1.110309  0.085057  15.719500  19.770203     0.992950     0.992950                7.392333\n",
            "                SVR      5.096584   1.250476   2.257561  0.123656  95.199947  99.250650     0.970855     0.970855               10.754350\n",
            "      Random Forest      5.301814   1.708858   2.302567  0.168702  97.410745 101.461448     0.969681     0.969681               15.321326\n",
            "        Extra Trees      6.081838   1.713477   2.466138  0.169223 105.097187 109.147891     0.965221     0.965221               16.439954\n",
            "               GRNN     11.451007   2.510299   3.383934  0.248241 140.532349 144.583052     0.934517     0.934517                4.281690\n",
            "      Decision Tree     18.604051   3.677973   4.313241  0.362411 167.709244 171.759948     0.893612     0.893612                5.698517\n",
            "        Elastic Net     29.262862   4.099715   5.409516  0.405440 193.073876 197.124580     0.832659     0.832659                3.940330\n",
            "             ResNet     34.018838   4.151274   5.832567  0.410213 201.507209 205.557912     0.805462     0.805462               17.668212\n",
            "                ELM     37.387655   4.395548   6.114545  0.432767 206.795071 210.845775     0.786197     0.786197                3.211255\n",
            "                MLP    224.370368  11.551355  14.978998  1.139779 307.144695 311.195398    -0.283071    -0.283071               14.422961\n",
            "               RBFN   4469.602401  38.868997  66.855085  3.852174 474.683065 478.733769   -24.559605   -24.559605                3.041347\n",
            "Cascade-Correlation 919492.786770 958.652078 958.901865 94.441402 772.968339 777.019042 -5257.157224 -5257.157224               16.976662\n",
            "   Deep Feedforward           NaN        NaN        NaN       NaN        NaN        NaN          NaN          NaN                     NaN\n",
            "\n",
            "======================================================================\n",
            "OPTIMAL HYPERPARAMETERS SUMMARY\n",
            "======================================================================\n",
            "              Model         C    gamma  epsilon  max_depth  min_samples_split  min_samples_leaf  max_features  n_estimators    alpha  l1_ratio  max_iter  hidden_layer_sizes  learning_rate_init  n_centers  n_hidden   sigma  max_hidden                                                                                       Error  n_blocks  hidden_size  learning_rate  epochs  subsample\n",
            "                SVR 79.674644 0.184251 0.781894        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN     NaN         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "      Decision Tree       NaN      NaN      NaN       20.0                2.0               1.0      0.667961           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN     NaN         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "      Random Forest       NaN      NaN      NaN       20.0                3.0               1.0           NaN          50.0      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN     NaN         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "        Extra Trees       NaN      NaN      NaN       23.0                3.0               1.0      0.575929         167.0      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN     NaN         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "        Elastic Net       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN 0.000100  0.892054     500.0                 NaN                 NaN        NaN       NaN     NaN         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "                MLP       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN 0.000108       NaN       NaN               191.0            0.099229        NaN       NaN     NaN         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "               RBFN       NaN 0.100000      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN       43.0       NaN     NaN         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "                ELM       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN      98.0     NaN         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "               GRNN       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN 0.37031         NaN                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "Cascade-Correlation       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN     NaN        20.0                                                                                         NaN       NaN          NaN            NaN     NaN        NaN\n",
            "   Deep Feedforward       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN     NaN         NaN The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()       NaN          NaN            NaN     NaN        NaN\n",
            "             ResNet       NaN      NaN      NaN        NaN                NaN               NaN           NaN           NaN      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN     NaN         NaN                                                                                         NaN       1.0        100.0       0.100000   200.0        NaN\n",
            "            XGBoost       NaN      NaN      NaN        3.0                NaN               NaN           NaN         174.0      NaN       NaN       NaN                 NaN                 NaN        NaN       NaN     NaN         NaN                                                                                         NaN       NaN          NaN       0.239107     NaN   0.800781\n",
            "\n",
            "Results saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HyperOpt**"
      ],
      "metadata": {
        "id": "3MdvAFMLqSl7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTBeiQ8my2j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "# XGBoost\n",
        "import xgboost as xgb\n",
        "\n",
        "# Hyperopt for Bayesian Optimization\n",
        "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
        "\n",
        "# Statistical Libraries\n",
        "import scipy.stats as stats\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Custom Neural Network Implementations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class RBFNRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Radial Basis Function Network Regressor\"\"\"\n",
        "    def __init__(self, n_centers=10, gamma=1.0):\n",
        "        self.n_centers = n_centers\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def _rbf_kernel(self, X, centers):\n",
        "        distances = cdist(X, centers, 'euclidean')\n",
        "        return np.exp(-self.gamma * distances**2)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "        # Select centers using k-means-like approach\n",
        "        n_samples = X.shape[0]\n",
        "        if self.n_centers >= n_samples:\n",
        "            self.centers = X.copy()\n",
        "        else:\n",
        "            # Random selection of centers\n",
        "            center_indices = np.random.choice(n_samples, self.n_centers, replace=False)\n",
        "            self.centers = X[center_indices]\n",
        "\n",
        "        # Calculate RBF activations\n",
        "        phi = self._rbf_kernel(X, self.centers)\n",
        "\n",
        "        # Add bias term\n",
        "        phi_with_bias = np.column_stack([phi, np.ones(phi.shape[0])])\n",
        "\n",
        "        # Calculate weights using pseudo-inverse\n",
        "        self.weights = np.linalg.pinv(phi_with_bias) @ y\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        phi = self._rbf_kernel(X, self.centers)\n",
        "        phi_with_bias = np.column_stack([phi, np.ones(phi.shape[0])])\n",
        "        return phi_with_bias @ self.weights\n",
        "\n",
        "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Extreme Learning Machine Regressor\"\"\"\n",
        "    def __init__(self, n_hidden=100):\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Random input weights and biases\n",
        "        self.W_input = np.random.randn(n_features, self.n_hidden)\n",
        "        self.b_input = np.random.randn(self.n_hidden)\n",
        "\n",
        "        # Calculate hidden layer output\n",
        "        H = np.tanh(X @ self.W_input + self.b_input)\n",
        "\n",
        "        # Calculate output weights using pseudo-inverse\n",
        "        self.W_output = np.linalg.pinv(H) @ y\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        H = np.tanh(X @ self.W_input + self.b_input)\n",
        "        return H @ self.W_output\n",
        "\n",
        "class GRNNRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"General Regression Neural Network\"\"\"\n",
        "    def __init__(self, sigma=1.0):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Calculate distances to all training points\n",
        "            distances = np.sum((self.X_train - x)**2, axis=1)\n",
        "            # Calculate weights using Gaussian kernel\n",
        "            weights = np.exp(-distances / (2 * self.sigma**2))\n",
        "            # Weighted average\n",
        "            if np.sum(weights) > 0:\n",
        "                pred = np.sum(weights * self.y_train) / np.sum(weights)\n",
        "            else:\n",
        "                pred = np.mean(self.y_train)\n",
        "            predictions.append(pred)\n",
        "        return np.array(predictions)\n",
        "\n",
        "class CascadeCorrelationRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Simplified Cascade-Correlation Network\"\"\"\n",
        "    def __init__(self, max_hidden=10):\n",
        "        self.max_hidden = max_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        self.hidden_weights = []\n",
        "\n",
        "        # Start with linear model\n",
        "        self.output_weights = np.linalg.pinv(X) @ y\n",
        "\n",
        "        # Add hidden units incrementally\n",
        "        current_input = X\n",
        "        for _ in range(self.max_hidden):\n",
        "            # Random hidden unit\n",
        "            hidden_weight = np.random.randn(current_input.shape[1])\n",
        "            hidden_output = np.tanh(current_input @ hidden_weight)\n",
        "\n",
        "            # Add to input for next layer\n",
        "            current_input = np.column_stack([current_input, hidden_output])\n",
        "            self.hidden_weights.append(hidden_weight)\n",
        "\n",
        "            # Update output weights\n",
        "            self.output_weights = np.linalg.pinv(current_input) @ y\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        current_input = X\n",
        "        for weight in self.hidden_weights:\n",
        "            hidden_output = np.tanh(current_input @ weight)\n",
        "            current_input = np.column_stack([current_input, hidden_output])\n",
        "\n",
        "        return current_input @ self.output_weights\n",
        "\n",
        "class DeepFeedforwardRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Deep Feedforward Neural Network using PyTorch\"\"\"\n",
        "    def __init__(self, hidden_layers=[50, 25], learning_rate=0.001, epochs=100):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.input_size = X.shape[1]\n",
        "        self.output_size = 1\n",
        "\n",
        "        # Build network\n",
        "        layers = []\n",
        "        prev_size = self.input_size\n",
        "\n",
        "        for hidden_size in self.hidden_layers:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, self.output_size))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        # Convert to tensors\n",
        "        X_tensor = torch.FloatTensor(X)\n",
        "        y_tensor = torch.FloatTensor(y.reshape(-1, 1))\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.epochs):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(X_tensor)\n",
        "            loss = criterion(outputs, y_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_tensor = torch.FloatTensor(X)\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(X_tensor)\n",
        "        return predictions.numpy().flatten()\n",
        "\n",
        "class ResNetRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Residual Network Regressor\"\"\"\n",
        "    def __init__(self, n_blocks=2, hidden_size=50, learning_rate=0.001, epochs=100):\n",
        "        self.n_blocks = n_blocks\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.input_size = X.shape[1]\n",
        "\n",
        "        # Build ResNet\n",
        "        class ResBlock(nn.Module):\n",
        "            def __init__(self, size):\n",
        "                super().__init__()\n",
        "                self.linear1 = nn.Linear(size, size)\n",
        "                self.linear2 = nn.Linear(size, size)\n",
        "                self.relu = nn.ReLU()\n",
        "\n",
        "            def forward(self, x):\n",
        "                residual = x\n",
        "                out = self.relu(self.linear1(x))\n",
        "                out = self.linear2(out)\n",
        "                out += residual\n",
        "                return self.relu(out)\n",
        "\n",
        "        class ResNet(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size, n_blocks):\n",
        "                super().__init__()\n",
        "                self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "                self.res_blocks = nn.ModuleList([ResBlock(hidden_size) for _ in range(n_blocks)])\n",
        "                self.output_layer = nn.Linear(hidden_size, 1)\n",
        "                self.relu = nn.ReLU()\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = self.relu(self.input_layer(x))\n",
        "                for block in self.res_blocks:\n",
        "                    x = block(x)\n",
        "                return self.output_layer(x)\n",
        "\n",
        "        self.model = ResNet(self.input_size, self.hidden_size, self.n_blocks)\n",
        "\n",
        "        # Convert to tensors\n",
        "        X_tensor = torch.FloatTensor(X)\n",
        "        y_tensor = torch.FloatTensor(y.reshape(-1, 1))\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.epochs):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(X_tensor)\n",
        "            loss = criterion(outputs, y_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_tensor = torch.FloatTensor(X)\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(X_tensor)\n",
        "        return predictions.numpy().flatten()\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive regression metrics\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate AIC and BIC (simplified for regression)\n",
        "    n = len(y_true)\n",
        "    residuals = y_true - y_pred\n",
        "    rss = np.sum(residuals**2)\n",
        "\n",
        "    # Assuming k parameters (simplified)\n",
        "    k = 2  # intercept + slope for simple case\n",
        "    aic = n * np.log(rss/n) + 2 * k\n",
        "    bic = n * np.log(rss/n) + k * np.log(n)\n",
        "\n",
        "    # Coefficient of determination\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'R2': r2,\n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "        'COD': cod\n",
        "    }\n",
        "\n",
        "def remove_outliers_iqr(df, columns):\n",
        "    \"\"\"Remove outliers using IQR method\"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        Q1 = df_clean[col].quantile(0.25)\n",
        "        Q3 = df_clean[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Comprehensive data preprocessing\"\"\"\n",
        "    print(\"Starting data preprocessing...\")\n",
        "\n",
        "    # 1. Remove outliers from numerical columns\n",
        "    numerical_columns = ['Temperature (°C)', 'Volume Concentration (ϕ)',\n",
        "                        'Density of Nano Particle 1 (ρnp)', 'Density of Nano Particle 2 (ρnp)',\n",
        "                        'Density of Base Fluid (ρbf)', 'Volume Mixture of Particle 1',\n",
        "                        'Volume Mixture of Particle 2', 'Density (ρ)']\n",
        "\n",
        "    df_clean = remove_outliers_iqr(df, numerical_columns)\n",
        "    print(f\"Removed {len(df) - len(df_clean)} outliers\")\n",
        "\n",
        "    # 2. Label encoding for categorical variables\n",
        "    le_nano = LabelEncoder()\n",
        "    le_fluid = LabelEncoder()\n",
        "\n",
        "    df_clean['Nano_Particle_Encoded'] = le_nano.fit_transform(df_clean['Nano Particle'])\n",
        "    df_clean['Base_Fluid_Encoded'] = le_fluid.fit_transform(df_clean['Base Fluid'])\n",
        "\n",
        "    # 3. Select features and target\n",
        "    feature_columns = ['Nano_Particle_Encoded', 'Base_Fluid_Encoded', 'Temperature (°C)',\n",
        "                      'Volume Concentration (ϕ)', 'Density of Nano Particle 1 (ρnp)',\n",
        "                      'Density of Nano Particle 2 (ρnp)', 'Density of Base Fluid (ρbf)',\n",
        "                      'Volume Mixture of Particle 1', 'Volume Mixture of Particle 2']\n",
        "\n",
        "    X = df_clean[feature_columns]\n",
        "    y = df_clean['Density (ρ)']\n",
        "\n",
        "    # 4. Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 5. Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(\"Data preprocessing completed!\")\n",
        "    return X_train_scaled, X_test_scaled, y_train.values, y_test.values\n",
        "\n",
        "def optimize_hyperparameters(model_name, model_class, search_space, X_train, y_train, max_evals=50):\n",
        "    \"\"\"Optimize hyperparameters using Hyperopt\"\"\"\n",
        "    print(f\"\\nOptimizing {model_name}...\")\n",
        "\n",
        "    def objective(params):\n",
        "        try:\n",
        "            # Handle different parameter types\n",
        "            if model_name == 'Deep Feedforward Regressor':\n",
        "                if 'hidden_layers' in params:\n",
        "                    params['hidden_layers'] = tuple(params['hidden_layers'])\n",
        "            elif model_name == 'MLP Regressor':\n",
        "                 if 'hidden_layer_sizes' in params:\n",
        "                    params['hidden_layer_sizes'] = tuple(params['hidden_layer_sizes'])\n",
        "            elif model_name == 'ResNet Regressor':\n",
        "                 if 'hidden_size' in params:\n",
        "                    params['hidden_size'] = int(params['hidden_size'])\n",
        "                 if 'n_blocks' in params:\n",
        "                    params['n_blocks'] = int(params['n_blocks'])\n",
        "\n",
        "            # Create model with parameters\n",
        "            model = model_class(**params)\n",
        "\n",
        "            # 5-fold cross-validation\n",
        "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
        "\n",
        "            # Return negative MSE (Hyperopt minimizes)\n",
        "            return {'loss': -scores.mean(), 'status': STATUS_OK}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in optimization: {e}\")\n",
        "            return {'loss': float('inf'), 'status': STATUS_OK}\n",
        "\n",
        "    # Run optimization\n",
        "    trials = Trials()\n",
        "    start_time = time.time()\n",
        "\n",
        "    best = fmin(fn=objective,\n",
        "                space=search_space,\n",
        "                algo=tpe.suggest,\n",
        "                max_evals=max_evals,\n",
        "                trials=trials) # Removed random_state\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    # Get best parameters\n",
        "    best_params = space_eval(search_space, best)\n",
        "    # Convert hidden_layers back to tuple if needed\n",
        "    if model_name in ['Deep Feedforward Regressor', 'MLP Regressor']:\n",
        "         if 'hidden_layers' in best_params:\n",
        "              best_params['hidden_layers'] = tuple(best_params['hidden_layers'])\n",
        "         elif 'hidden_layer_sizes' in best_params:\n",
        "              best_params['hidden_layer_sizes'] = tuple(best_params['hidden_layer_sizes'])\n",
        "\n",
        "    print(f\"{model_name} optimization completed in {execution_time:.2f} seconds\")\n",
        "\n",
        "    return best_params, execution_time\n",
        "\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "    metrics['Execution_Time'] = execution_time\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv('Density_Prediction_Dataset.csv')\n",
        "\n",
        "    # Preprocess data\n",
        "    X_train, X_test, y_train, y_test = preprocess_data(df)\n",
        "\n",
        "    # Define hyperparameter search spaces\n",
        "    search_spaces = {\n",
        "        'Support Vector Regressor': {\n",
        "            'C': hp.uniform('C', 0.1, 100),\n",
        "            'gamma': hp.uniform('gamma', 0.001, 1),\n",
        "            'epsilon': hp.uniform('epsilon', 0.01, 1)\n",
        "        },\n",
        "        'Decision Tree Regressor': {\n",
        "            'max_depth': hp.choice('max_depth', range(5, 21)),\n",
        "            'min_samples_split': hp.choice('min_samples_split', range(2, 21)),\n",
        "            'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 11)),\n",
        "            'max_features': hp.uniform('max_features', 0.1, 1.0)\n",
        "        },\n",
        "        'Extra Trees Regressor': {\n",
        "            'n_estimators': hp.choice('n_estimators', range(50, 301)),\n",
        "            'max_depth': hp.choice('max_depth', range(5, 26)),\n",
        "            'min_samples_split': hp.choice('min_samples_split', range(2, 16)),\n",
        "            'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 9)),\n",
        "            'max_features': hp.uniform('max_features', 0.1, 1.0)\n",
        "        },\n",
        "        'Random Forest Regressor': {\n",
        "            'n_estimators': hp.choice('n_estimators', range(50, 201)),\n",
        "            'max_depth': hp.choice('max_depth', range(5, 21)),\n",
        "            'min_samples_split': hp.choice('min_samples_split', range(2, 11)),\n",
        "            'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 6))\n",
        "        },\n",
        "        'XGBoost Regressor': {\n",
        "            'n_estimators': hp.choice('n_estimators', range(50, 201)),\n",
        "            'max_depth': hp.choice('max_depth', range(3, 11)),\n",
        "            'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "            'subsample': hp.uniform('subsample', 0.5, 1.0)\n",
        "        },\n",
        "        'Elastic Net Regressor': {\n",
        "            'alpha': hp.uniform('alpha', 0.0001, 10),\n",
        "            'l1_ratio': hp.uniform('l1_ratio', 0.0, 1.0),\n",
        "            'max_iter': hp.choice('max_iter', range(500, 2001))\n",
        "        },\n",
        "        'MLP Regressor': {\n",
        "            'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(50,), (100,), (50, 25), (100, 50)]),\n",
        "            'alpha': hp.uniform('alpha', 0.0001, 0.01),\n",
        "            'learning_rate_init': hp.uniform('learning_rate_init', 0.001, 0.1)\n",
        "        },\n",
        "        'RBFN Regressor': {\n",
        "            'n_centers': hp.choice('n_centers', range(5, 51)),\n",
        "            'gamma': hp.uniform('gamma', 0.1, 10.0)\n",
        "        },\n",
        "        'ELM Regressor': {\n",
        "            'n_hidden': hp.choice('n_hidden', range(10, 501))\n",
        "        },\n",
        "        'GRNN Regressor': {\n",
        "            'sigma': hp.uniform('sigma', 0.1, 5.0)\n",
        "        },\n",
        "        'Cascade-Correlation Regressor': {\n",
        "            'max_hidden': hp.choice('max_hidden', range(2, 21))\n",
        "        },\n",
        "        'Deep Feedforward Regressor': {\n",
        "            'hidden_layers': hp.choice('hidden_layers', [[20, 10], [50, 25], [100, 50, 25]]),\n",
        "            'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
        "            'epochs': hp.choice('epochs', range(50, 201))\n",
        "        },\n",
        "        'ResNet Regressor': {\n",
        "            'n_blocks': hp.choice('n_blocks', range(1, 6)),\n",
        "            'hidden_size': hp.choice('hidden_size', range(10, 101)),\n",
        "            'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
        "            'epochs': hp.choice('epochs', range(50, 201))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Model classes\n",
        "    model_classes = {\n",
        "        'Support Vector Regressor': SVR,\n",
        "        'Decision Tree Regressor': DecisionTreeRegressor,\n",
        "        'Extra Trees Regressor': ExtraTreesRegressor,\n",
        "        'Random Forest Regressor': RandomForestRegressor,\n",
        "        'XGBoost Regressor': xgb.XGBRegressor,\n",
        "        'Elastic Net Regressor': ElasticNet,\n",
        "        'MLP Regressor': MLPRegressor,\n",
        "        'RBFN Regressor': RBFNRegressor,\n",
        "        'ELM Regressor': ELMRegressor,\n",
        "        'GRNN Regressor': GRNNRegressor,\n",
        "        'Cascade-Correlation Regressor': CascadeCorrelationRegressor,\n",
        "        'Deep Feedforward Regressor': DeepFeedforwardRegressor,\n",
        "        'ResNet Regressor': ResNetRegressor\n",
        "    }\n",
        "\n",
        "    # Results storage\n",
        "    performance_results = []\n",
        "    hyperparameter_results = []\n",
        "\n",
        "    # Iterate through models\n",
        "    for model_name in model_classes.keys():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Optimize hyperparameters\n",
        "        best_params, opt_time = optimize_hyperparameters(\n",
        "            model_name,\n",
        "            model_classes[model_name],\n",
        "            search_spaces[model_name],\n",
        "            X_train,\n",
        "            y_train\n",
        "        )\n",
        "\n",
        "        # Create model with best parameters\n",
        "        model = model_classes[model_name](**best_params)\n",
        "\n",
        "        # Evaluate model\n",
        "        metrics = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "        # Store results\n",
        "        performance_row = {'Model': model_name, **metrics}\n",
        "        performance_results.append(performance_row)\n",
        "\n",
        "        hyperparameter_row = {'Model': model_name, **best_params}\n",
        "        hyperparameter_results.append(hyperparameter_row)\n",
        "\n",
        "        print(f\"Best parameters: {best_params}\")\n",
        "        print(f\"Performance metrics: {metrics}\")\n",
        "\n",
        "    # Create DataFrames\n",
        "    performance_df = pd.DataFrame(performance_results)\n",
        "    hyperparameters_df = pd.DataFrame(hyperparameter_results)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(performance_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPTIMAL HYPERPARAMETERS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(hyperparameters_df.to_string(index=False))\n",
        "\n",
        "    # Save results\n",
        "    performance_df.to_csv('performance_metrics.csv', index=False)\n",
        "    hyperparameters_df.to_csv('optimal_hyperparameters.csv', index=False)\n",
        "\n",
        "    print(\"\\nResults saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\")\n",
        "\n",
        "    return performance_df, hyperparameters_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    performance_df, hyperparameters_df = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIIsMcYqqRje",
        "outputId": "725e723b-1b34-4844-9ce6-3f292c78a428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Starting data preprocessing...\n",
            "Removed 230 outliers\n",
            "Data preprocessing completed!\n",
            "\n",
            "============================================================\n",
            "Processing Support Vector Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing Support Vector Regressor...\n",
            "100%|██████████| 50/50 [00:02<00:00, 22.44trial/s, best loss: 1.742377869565503]\n",
            "Support Vector Regressor optimization completed in 2.23 seconds\n",
            "Best parameters: {'C': 99.73245140858623, 'epsilon': 0.13786343523017514, 'gamma': 0.33431722123913005}\n",
            "Performance metrics: {'MSE': 0.9551959777503595, 'MAE': 0.7091999671713736, 'RMSE': np.float64(0.9773412800809957), 'MAPE': np.float64(0.06985958851906526), 'R2': 0.993790280955204, 'AIC': np.float64(2.074772615477734), 'BIC': np.float64(5.550111852044471), 'COD': 0.993790280955204, 'Execution_Time': 0.02907562255859375}\n",
            "\n",
            "============================================================\n",
            "Processing Decision Tree Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing Decision Tree Regressor...\n",
            "100%|██████████| 50/50 [00:00<00:00, 67.34trial/s, best loss: 26.72148967811713] \n",
            "Decision Tree Regressor optimization completed in 0.75 seconds\n",
            "Best parameters: {'max_depth': 11, 'max_features': 0.9317718301446908, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
            "Performance metrics: {'MSE': 17.047833510656172, 'MAE': 3.316708138095226, 'RMSE': np.float64(4.128902216165475), 'MAPE': np.float64(0.32692072929751165), 'R2': 0.889172212939008, 'AIC': np.float64(123.11297141095106), 'BIC': np.float64(126.5883106475178), 'COD': 0.889172212939008, 'Execution_Time': 0.0033206939697265625}\n",
            "\n",
            "============================================================\n",
            "Processing Extra Trees Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing Extra Trees Regressor...\n",
            "100%|██████████| 50/50 [00:38<00:00,  1.29trial/s, best loss: 13.886705500323412]\n",
            "Extra Trees Regressor optimization completed in 38.83 seconds\n",
            "Best parameters: {'max_depth': 23, 'max_features': 0.46037909077852457, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 99}\n",
            "Performance metrics: {'MSE': 6.131298728061777, 'MAE': 1.9203088627946245, 'RMSE': np.float64(2.476145942399554), 'MAPE': np.float64(0.1894588434320635), 'R2': 0.9601404911998811, 'AIC': np.float64(80.16307685527205), 'BIC': np.float64(83.63841609183879), 'COD': 0.9601404911998811, 'Execution_Time': 0.08025240898132324}\n",
            "\n",
            "============================================================\n",
            "Processing Random Forest Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing Random Forest Regressor...\n",
            "100%|██████████| 50/50 [00:45<00:00,  1.11trial/s, best loss: 18.077213841253588]\n",
            "Random Forest Regressor optimization completed in 45.06 seconds\n",
            "Best parameters: {'max_depth': 14, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 177}\n",
            "Performance metrics: {'MSE': 5.566197007080553, 'MAE': 1.778725730938791, 'RMSE': np.float64(2.3592789167626096), 'MAPE': np.float64(0.17517055474396995), 'R2': 0.9638142115680182, 'AIC': np.float64(76.10190640280837), 'BIC': np.float64(79.5772456393751), 'COD': 0.9638142115680182, 'Execution_Time': 0.27892589569091797}\n",
            "\n",
            "============================================================\n",
            "Processing XGBoost Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing XGBoost Regressor...\n",
            "100%|██████████| 50/50 [00:14<00:00,  3.46trial/s, best loss: 6.2857875226491675]\n",
            "XGBoost Regressor optimization completed in 14.49 seconds\n",
            "Best parameters: {'learning_rate': 0.18510753929300075, 'max_depth': 3, 'n_estimators': 164, 'subsample': 0.5545065103731621}\n",
            "Performance metrics: {'MSE': 0.8023289427137961, 'MAE': 0.6961910346354168, 'RMSE': np.float64(0.8957281634032705), 'MAPE': np.float64(0.06827139241347427), 'R2': 0.9947840679485535, 'AIC': np.float64(-5.249937292217515), 'BIC': np.float64(-1.7745980556507783), 'COD': 0.9947840679485535, 'Execution_Time': 0.058496952056884766}\n",
            "\n",
            "============================================================\n",
            "Processing Elastic Net Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing Elastic Net Regressor...\n",
            "100%|██████████| 50/50 [00:05<00:00,  9.76trial/s, best loss: 26.114995387825353]\n",
            "Elastic Net Regressor optimization completed in 5.32 seconds\n",
            "Best parameters: {'alpha': 0.0024346921794942133, 'l1_ratio': 0.9733787056256963, 'max_iter': 1625}\n",
            "Performance metrics: {'MSE': 12.90788491712995, 'MAE': 2.519809479602592, 'RMSE': np.float64(3.5927545027638543), 'MAPE': np.float64(0.24721400678187322), 'R2': 0.9160859753757411, 'AIC': np.float64(111.42921105810555), 'BIC': np.float64(114.90455029467229), 'COD': 0.9160859753757411, 'Execution_Time': 0.002724885940551758}\n",
            "\n",
            "============================================================\n",
            "Processing MLP Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing MLP Regressor...\n",
            "100%|██████████| 50/50 [00:38<00:00,  1.31trial/s, best loss: 922.016192709131]\n",
            "MLP Regressor optimization completed in 38.24 seconds\n",
            "Best parameters: {'alpha': 0.0057848710900510825, 'hidden_layer_sizes': (100, 50), 'learning_rate_init': 0.0998686334394412}\n",
            "Performance metrics: {'MSE': 310.691650345066, 'MAE': 10.700055811545859, 'RMSE': np.float64(17.626447468082333), 'MAPE': np.float64(1.0575195791571332), 'R2': -1.0198031641116012, 'AIC': np.float64(245.0296395786859), 'BIC': np.float64(248.50497881525263), 'COD': -1.0198031641116012, 'Execution_Time': 0.2567474842071533}\n",
            "\n",
            "============================================================\n",
            "Processing RBFN Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing RBFN Regressor...\n",
            "100%|██████████| 50/50 [00:00<00:00, 64.82trial/s, best loss: 12.566901272586795]\n",
            "RBFN Regressor optimization completed in 0.78 seconds\n",
            "Best parameters: {'gamma': 0.12594191366203955, 'n_centers': 27}\n",
            "Performance metrics: {'MSE': 5.717787377271349, 'MAE': 1.7951155894585435, 'RMSE': np.float64(2.3911895318588505), 'MAPE': np.float64(0.1764389789663892), 'R2': 0.9628287241594569, 'AIC': np.float64(77.23044015285707), 'BIC': np.float64(80.70577938942381), 'COD': 0.9628287241594569, 'Execution_Time': 0.002359151840209961}\n",
            "\n",
            "============================================================\n",
            "Processing ELM Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing ELM Regressor...\n",
            "100%|██████████| 50/50 [00:03<00:00, 13.29trial/s, best loss: 102.67105237775202]\n",
            "ELM Regressor optimization completed in 3.82 seconds\n",
            "Best parameters: {'n_hidden': 212}\n",
            "Performance metrics: {'MSE': 76.77538544193091, 'MAE': 4.775215221670708, 'RMSE': np.float64(8.762156437882794), 'MAPE': np.float64(0.4715257206312682), 'R2': 0.5008840235350005, 'AIC': np.float64(186.3171316423943), 'BIC': np.float64(189.792470878961), 'COD': 0.5008840235350005, 'Execution_Time': 0.009649276733398438}\n",
            "\n",
            "============================================================\n",
            "Processing GRNN Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing GRNN Regressor...\n",
            "100%|██████████| 50/50 [00:00<00:00, 69.82trial/s, best loss: 14.720079188120815]\n",
            "GRNN Regressor optimization completed in 0.72 seconds\n",
            "Best parameters: {'sigma': 0.4213288664764813}\n",
            "Performance metrics: {'MSE': 6.140468799681124, 'MAE': 1.8914871567808242, 'RMSE': np.float64(2.477996932944253), 'MAPE': np.float64(0.186373392102651), 'R2': 0.9600808766603487, 'AIC': np.float64(80.2258458213281), 'BIC': np.float64(83.70118505789483), 'COD': 0.9600808766603487, 'Execution_Time': 0.003887653350830078}\n",
            "\n",
            "============================================================\n",
            "Processing Cascade-Correlation Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing Cascade-Correlation Regressor...\n",
            "100%|██████████| 50/50 [00:01<00:00, 35.98trial/s, best loss: 379544.75435578043]\n",
            "Cascade-Correlation Regressor optimization completed in 1.40 seconds\n",
            "Best parameters: {'max_hidden': 20}\n",
            "Performance metrics: {'MSE': 306159.07349515025, 'MAE': 406.69060228503787, 'RMSE': np.float64(553.3164316149939), 'MAPE': np.float64(40.14249633342416), 'R2': -1989.336929493223, 'AIC': np.float64(534.5381239431159), 'BIC': np.float64(538.0134631796826), 'COD': -1989.336929493223, 'Execution_Time': 0.010620832443237305}\n",
            "\n",
            "============================================================\n",
            "Processing Deep Feedforward Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing Deep Feedforward Regressor...\n",
            "100%|██████████| 50/50 [00:47<00:00,  1.06trial/s, best loss: 657.15024152478]\n",
            "Deep Feedforward Regressor optimization completed in 47.07 seconds\n",
            "Best parameters: {'epochs': 190, 'hidden_layers': (100, 50, 25), 'learning_rate': 0.09988769626428506}\n",
            "Performance metrics: {'MSE': 629.8005026196051, 'MAE': 13.480467715978424, 'RMSE': np.float64(25.095826398419423), 'MAPE': np.float64(1.3339945818114092), 'R2': -3.0943264697887507, 'AIC': np.float64(274.70693048261194), 'BIC': np.float64(278.1822697191787), 'COD': -3.0943264697887507, 'Execution_Time': 0.27043604850769043}\n",
            "\n",
            "============================================================\n",
            "Processing ResNet Regressor\n",
            "============================================================\n",
            "\n",
            "Optimizing ResNet Regressor...\n",
            "100%|██████████| 50/50 [01:16<00:00,  1.53s/trial, best loss: 195.548951318041]\n",
            "ResNet Regressor optimization completed in 76.41 seconds\n",
            "Best parameters: {'epochs': 164, 'hidden_size': 88, 'learning_rate': 0.09907773115166663, 'n_blocks': 1}\n",
            "Performance metrics: {'MSE': 26.098156958662397, 'MAE': 3.717375168080353, 'RMSE': np.float64(5.1086355280703275), 'MAPE': np.float64(0.3666024399635166), 'R2': 0.8303361550140074, 'AIC': np.float64(140.99831728343668), 'BIC': np.float64(144.4736565200034), 'COD': 0.8303361550140074, 'Execution_Time': 0.3853952884674072}\n",
            "\n",
            "================================================================================\n",
            "PERFORMANCE METRICS SUMMARY\n",
            "================================================================================\n",
            "                        Model           MSE        MAE       RMSE      MAPE           R2        AIC        BIC          COD  Execution_Time\n",
            "     Support Vector Regressor      0.955196   0.709200   0.977341  0.069860     0.993790   2.074773   5.550112     0.993790        0.029076\n",
            "      Decision Tree Regressor     17.047834   3.316708   4.128902  0.326921     0.889172 123.112971 126.588311     0.889172        0.003321\n",
            "        Extra Trees Regressor      6.131299   1.920309   2.476146  0.189459     0.960140  80.163077  83.638416     0.960140        0.080252\n",
            "      Random Forest Regressor      5.566197   1.778726   2.359279  0.175171     0.963814  76.101906  79.577246     0.963814        0.278926\n",
            "            XGBoost Regressor      0.802329   0.696191   0.895728  0.068271     0.994784  -5.249937  -1.774598     0.994784        0.058497\n",
            "        Elastic Net Regressor     12.907885   2.519809   3.592755  0.247214     0.916086 111.429211 114.904550     0.916086        0.002725\n",
            "                MLP Regressor    310.691650  10.700056  17.626447  1.057520    -1.019803 245.029640 248.504979    -1.019803        0.256747\n",
            "               RBFN Regressor      5.717787   1.795116   2.391190  0.176439     0.962829  77.230440  80.705779     0.962829        0.002359\n",
            "                ELM Regressor     76.775385   4.775215   8.762156  0.471526     0.500884 186.317132 189.792471     0.500884        0.009649\n",
            "               GRNN Regressor      6.140469   1.891487   2.477997  0.186373     0.960081  80.225846  83.701185     0.960081        0.003888\n",
            "Cascade-Correlation Regressor 306159.073495 406.690602 553.316432 40.142496 -1989.336929 534.538124 538.013463 -1989.336929        0.010621\n",
            "   Deep Feedforward Regressor    629.800503  13.480468  25.095826  1.333995    -3.094326 274.706930 278.182270    -3.094326        0.270436\n",
            "             ResNet Regressor     26.098157   3.717375   5.108636  0.366602     0.830336 140.998317 144.473657     0.830336        0.385395\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL HYPERPARAMETERS SUMMARY\n",
            "================================================================================\n",
            "                        Model         C  epsilon    gamma  max_depth  max_features  min_samples_leaf  min_samples_split  n_estimators  learning_rate  subsample    alpha  l1_ratio  max_iter hidden_layer_sizes  learning_rate_init  n_centers  n_hidden    sigma  max_hidden  epochs hidden_layers  hidden_size  n_blocks\n",
            "     Support Vector Regressor 99.732451 0.137863 0.334317        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "      Decision Tree Regressor       NaN      NaN      NaN       11.0      0.931772               1.0                3.0           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "        Extra Trees Regressor       NaN      NaN      NaN       23.0      0.460379               1.0                3.0          99.0            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "      Random Forest Regressor       NaN      NaN      NaN       14.0           NaN               1.0                2.0         177.0            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "            XGBoost Regressor       NaN      NaN      NaN        3.0           NaN               NaN                NaN         164.0       0.185108   0.554507      NaN       NaN       NaN                NaN                 NaN        NaN       NaN      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "        Elastic Net Regressor       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN 0.002435  0.973379    1625.0                NaN                 NaN        NaN       NaN      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "                MLP Regressor       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN 0.005785       NaN       NaN          (100, 50)            0.099869        NaN       NaN      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "               RBFN Regressor       NaN      NaN 0.125942        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN       27.0       NaN      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "                ELM Regressor       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN     212.0      NaN         NaN     NaN           NaN          NaN       NaN\n",
            "               GRNN Regressor       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN 0.421329         NaN     NaN           NaN          NaN       NaN\n",
            "Cascade-Correlation Regressor       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN      NaN        20.0     NaN           NaN          NaN       NaN\n",
            "   Deep Feedforward Regressor       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN       0.099888        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN      NaN         NaN   190.0 (100, 50, 25)          NaN       NaN\n",
            "             ResNet Regressor       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN       0.099078        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN      NaN         NaN   164.0           NaN         88.0       1.0\n",
            "\n",
            "Results saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjeTRbb78pzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomizedSearchCV**"
      ],
      "metadata": {
        "id": "-4zVGFNNJ6RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Custom implementations for advanced models\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "class RBFNRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Radial Basis Function Network Regressor\"\"\"\n",
        "    def __init__(self, n_centers=10, gamma=1.0):\n",
        "        self.n_centers = n_centers\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.kmeans = KMeans(n_clusters=self.n_centers, random_state=42)\n",
        "        self.centers = self.kmeans.fit(X).cluster_centers_\n",
        "\n",
        "        # Calculate RBF activations\n",
        "        G = np.exp(-self.gamma * cdist(X, self.centers)**2)\n",
        "\n",
        "        # Add bias term\n",
        "        G_bias = np.column_stack([G, np.ones(G.shape[0])])\n",
        "\n",
        "        # Solve linear system\n",
        "        self.weights = np.linalg.pinv(G_bias) @ y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = check_array(X)\n",
        "        G = np.exp(-self.gamma * cdist(X, self.centers)**2)\n",
        "        G_bias = np.column_stack([G, np.ones(G.shape[0])])\n",
        "        return G_bias @ self.weights\n",
        "\n",
        "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Extreme Learning Machine Regressor\"\"\"\n",
        "    def __init__(self, n_hidden=100):\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Random weights and biases\n",
        "        self.W = np.random.randn(n_features, self.n_hidden)\n",
        "        self.b = np.random.randn(self.n_hidden)\n",
        "\n",
        "        # Hidden layer output\n",
        "        H = np.tanh(X @ self.W + self.b)\n",
        "\n",
        "        # Output weights\n",
        "        self.beta = np.linalg.pinv(H) @ y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = check_array(X)\n",
        "        H = np.tanh(X @ self.W + self.b)\n",
        "        return H @ self.beta\n",
        "\n",
        "class GRNNRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"General Regression Neural Network\"\"\"\n",
        "    def __init__(self, sigma=1.0):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = check_array(X)\n",
        "        predictions = []\n",
        "\n",
        "        for x in X:\n",
        "            # Calculate distances\n",
        "            distances = np.sum((self.X_train - x)**2, axis=1)\n",
        "\n",
        "            # Gaussian weights\n",
        "            weights = np.exp(-distances / (2 * self.sigma**2))\n",
        "\n",
        "            # Weighted average\n",
        "            if np.sum(weights) > 0:\n",
        "                pred = np.sum(weights * self.y_train) / np.sum(weights)\n",
        "            else:\n",
        "                pred = np.mean(self.y_train)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "class CascadeCorrelationRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Simplified Cascade-Correlation Network\"\"\"\n",
        "    def __init__(self, max_hidden=10):\n",
        "        self.max_hidden = max_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "        # Simple implementation using polynomial features\n",
        "        from sklearn.preprocessing import PolynomialFeatures\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "\n",
        "        self.poly = PolynomialFeatures(degree=min(2, self.max_hidden//5 + 1))\n",
        "        X_poly = self.poly.fit_transform(X)\n",
        "\n",
        "        self.regressor = LinearRegression()\n",
        "        self.regressor.fit(X_poly, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = check_array(X)\n",
        "        X_poly = self.poly.transform(X)\n",
        "        return self.regressor.predict(X_poly)\n",
        "\n",
        "class DeepFeedforwardRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Deep Feedforward Neural Network\"\"\"\n",
        "    def __init__(self, hidden_layers=(50, 25), learning_rate=0.01, epochs=100):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y)\n",
        "\n",
        "        # Use MLPRegressor as base\n",
        "        self.mlp = MLPRegressor(\n",
        "            hidden_layer_sizes=self.hidden_layers,\n",
        "            learning_rate_init=self.learning_rate,\n",
        "            max_iter=self.epochs,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.mlp.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = check_array(X)\n",
        "        return self.mlp.predict(X)\n",
        "\n",
        "class ResNetRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Residual Network Regressor (simplified)\"\"\"\n",
        "    def __init__(self, n_blocks=2, hidden_size=50, learning_rate=0.01, epochs=100):\n",
        "        self.n_blocks = n_blocks\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y)\n",
        "\n",
        "        # Simplified ResNet using multiple MLPs\n",
        "        hidden_layers = tuple([self.hidden_size] * (self.n_blocks + 1))\n",
        "\n",
        "        self.mlp = MLPRegressor(\n",
        "            hidden_layer_sizes=hidden_layers,\n",
        "            learning_rate_init=self.learning_rate,\n",
        "            max_iter=self.epochs,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.mlp.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = check_array(X)\n",
        "        return self.mlp.predict(X)\n",
        "\n",
        "# Performance metrics calculation\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive regression metrics\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # MAPE (Mean Absolute Percentage Error)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    # AIC and BIC approximations (for regression)\n",
        "    n = len(y_true)\n",
        "    k = 2  # Approximate number of parameters\n",
        "\n",
        "    # Log-likelihood approximation for normal distribution\n",
        "    log_likelihood = -n/2 * np.log(2*np.pi) - n/2 * np.log(mse) - n/2\n",
        "\n",
        "    aic = 2*k - 2*log_likelihood\n",
        "    bic = k*np.log(n) - 2*log_likelihood\n",
        "\n",
        "    # Coefficient of Determination (same as R²)\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "        'COD': cod,\n",
        "        'R2': r2\n",
        "    }\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"Load and preprocess the density prediction dataset\"\"\"\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv('/content/Density_Prediction_Dataset.csv')\n",
        "\n",
        "    # Handle categorical variables\n",
        "    le_nano = LabelEncoder()\n",
        "    le_fluid = LabelEncoder()\n",
        "\n",
        "    df['Nano_Particle_Encoded'] = le_nano.fit_transform(df['Nano Particle'])\n",
        "    df['Base_Fluid_Encoded'] = le_fluid.fit_transform(df['Base Fluid'])\n",
        "\n",
        "    # Select features\n",
        "    features = ['Nano_Particle_Encoded', 'Base_Fluid_Encoded', 'Temperature (°C)',\n",
        "                'Volume Concentration (ϕ)', 'Density of Nano Particle 1 (ρnp)',\n",
        "                'Density of Nano Particle 2 (ρnp)', 'Density of Base Fluid (ρbf)',\n",
        "                'Volume Mixture of Particle 1', 'Volume Mixture of Particle 2']\n",
        "\n",
        "    X = df[features]\n",
        "    y = df['Density (ρ)']\n",
        "\n",
        "    # Outlier detection and removal using IQR method\n",
        "    print(\"Applying outlier detection and removal...\")\n",
        "    Q1 = X.quantile(0.25)\n",
        "    Q3 = X.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define outlier bounds\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Remove outliers\n",
        "    outlier_mask = ((X < lower_bound) | (X > upper_bound)).any(axis=1)\n",
        "    X_clean = X[~outlier_mask]\n",
        "    y_clean = y[~outlier_mask]\n",
        "\n",
        "    print(f\"Removed {outlier_mask.sum()} outliers. Remaining samples: {len(X_clean)}\")\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_clean)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y_clean, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(\"Starting Density Prediction ML Pipeline...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    X_train, X_test, y_train, y_test = load_and_preprocess_data()\n",
        "\n",
        "    # Define models and their parameter distributions\n",
        "    models = {\n",
        "        'SVR': {\n",
        "            'estimator': SVR(),\n",
        "            'params': {\n",
        "                'C': uniform(0.1, 100),\n",
        "                'gamma': uniform(0.001, 1),\n",
        "                'epsilon': uniform(0.01, 1)\n",
        "            }\n",
        "        },\n",
        "        'Decision Tree': {\n",
        "            'estimator': DecisionTreeRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'max_depth': randint(5, 20),\n",
        "                'min_samples_split': randint(2, 20),\n",
        "                'min_samples_leaf': randint(1, 10),\n",
        "                'max_features': uniform(0.1, 0.9)\n",
        "            }\n",
        "        },\n",
        "        'Extra Trees': {\n",
        "            'estimator': ExtraTreesRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': randint(50, 300),\n",
        "                'max_depth': randint(5, 25),\n",
        "                'min_samples_split': randint(2, 15),\n",
        "                'min_samples_leaf': randint(1, 8),\n",
        "                'max_features': uniform(0.1, 0.9)\n",
        "            }\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'estimator': RandomForestRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': randint(50, 200),\n",
        "                'max_depth': randint(5, 20),\n",
        "                'min_samples_split': randint(2, 10),\n",
        "                'min_samples_leaf': randint(1, 5)\n",
        "            }\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'estimator': xgb.XGBRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': randint(50, 200),\n",
        "                'max_depth': randint(3, 10),\n",
        "                'learning_rate': uniform(0.01, 0.29),\n",
        "                'subsample': uniform(0.5, 0.5)\n",
        "            }\n",
        "        },\n",
        "        'Elastic Net': {\n",
        "            'estimator': ElasticNet(random_state=42),\n",
        "            'params': {\n",
        "                'alpha': uniform(0.0001, 9.9999),\n",
        "                'l1_ratio': uniform(0.0, 1.0),\n",
        "                'max_iter': randint(500, 1500)\n",
        "            }\n",
        "        },\n",
        "        'MLP': {\n",
        "            'estimator': MLPRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],\n",
        "                'alpha': uniform(0.0001, 0.0099),\n",
        "                'learning_rate_init': uniform(0.001, 0.099)\n",
        "            }\n",
        "        },\n",
        "        'RBFN': {\n",
        "            'estimator': RBFNRegressor(),\n",
        "            'params': {\n",
        "                'n_centers': randint(5, 50),\n",
        "                'gamma': uniform(0.1, 9.9)\n",
        "            }\n",
        "        },\n",
        "        'ELM': {\n",
        "            'estimator': ELMRegressor(),\n",
        "            'params': {\n",
        "                'n_hidden': randint(10, 490)\n",
        "            }\n",
        "        },\n",
        "        'GRNN': {\n",
        "            'estimator': GRNNRegressor(),\n",
        "            'params': {\n",
        "                'sigma': uniform(0.1, 4.9)\n",
        "            }\n",
        "        },\n",
        "        'Cascade-Correlation': {\n",
        "            'estimator': CascadeCorrelationRegressor(),\n",
        "            'params': {\n",
        "                'max_hidden': randint(2, 18)\n",
        "            }\n",
        "        },\n",
        "        'Deep Feedforward': {\n",
        "            'estimator': DeepFeedforwardRegressor(),\n",
        "            'params': {\n",
        "                'hidden_layers': [(20, 10), (50, 25), (100, 50, 25)],\n",
        "                'learning_rate': uniform(0.001, 0.099),\n",
        "                'epochs': randint(50, 150)\n",
        "            }\n",
        "        },\n",
        "        'ResNet': {\n",
        "            'estimator': ResNetRegressor(),\n",
        "            'params': {\n",
        "                'n_blocks': randint(1, 5),\n",
        "                'hidden_size': randint(10, 90),\n",
        "                'learning_rate': uniform(0.001, 0.099),\n",
        "                'epochs': randint(50, 150)\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Storage for results\n",
        "    performance_results = []\n",
        "    hyperparameter_results = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for model_name, model_config in models.items():\n",
        "        print(f\"\\n{'='*20} {model_name} {'='*20}\")\n",
        "        print(f\"Starting hyperparameter optimization for {model_name}...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Randomized search\n",
        "            random_search = RandomizedSearchCV(\n",
        "                estimator=model_config['estimator'],\n",
        "                param_distributions=model_config['params'],\n",
        "                n_iter=10,\n",
        "                cv=5,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                random_state=42,\n",
        "                n_jobs=-1,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Fit the model\n",
        "            random_search.fit(X_train, y_train)\n",
        "\n",
        "            # Get best model\n",
        "            best_model = random_search.best_estimator_\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = best_model.predict(X_test)\n",
        "\n",
        "            # Calculate execution time\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = calculate_metrics(y_test, y_pred)\n",
        "            metrics['Execution_Time'] = execution_time\n",
        "            metrics['Model'] = model_name\n",
        "\n",
        "            # Store results\n",
        "            performance_results.append(metrics)\n",
        "\n",
        "            # Store hyperparameters\n",
        "            hyperparams = {'Model': model_name}\n",
        "            hyperparams.update(random_search.best_params_)\n",
        "            hyperparameter_results.append(hyperparams)\n",
        "\n",
        "            print(f\"✓ {model_name} completed in {execution_time:.2f} seconds\")\n",
        "            print(f\"Best R² Score: {metrics['R2']:.4f}\")\n",
        "            print(f\"Best RMSE: {metrics['RMSE']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error training {model_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Create DataFrames\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CREATING FINAL RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Performance metrics DataFrame\n",
        "    performance_df = pd.DataFrame(performance_results)\n",
        "    performance_df = performance_df.round(4)\n",
        "\n",
        "    # Hyperparameters DataFrame\n",
        "    hyperparams_df = pd.DataFrame(hyperparameter_results)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n📊 PERFORMANCE METRICS:\")\n",
        "    print(performance_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n🎯 OPTIMAL HYPERPARAMETERS:\")\n",
        "    print(hyperparams_df.to_string(index=False))\n",
        "\n",
        "    # Save results\n",
        "    performance_df.to_csv('performance_metrics.csv', index=False)\n",
        "    hyperparams_df.to_csv('optimal_hyperparameters.csv', index=False)\n",
        "\n",
        "    print(\"\\n✅ Results saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\")\n",
        "\n",
        "    return performance_df, hyperparams_df\n",
        "\n",
        "# Execute the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    performance_df, hyperparams_df = main()"
      ],
      "metadata": {
        "id": "LKOfg3Ge8p2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3f8fe5-651a-4755-ee92-8f48b04717b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Density Prediction ML Pipeline...\n",
            "==================================================\n",
            "Loading and preprocessing data...\n",
            "Applying outlier detection and removal...\n",
            "Removed 160 outliers. Remaining samples: 276\n",
            "\n",
            "==================== SVR ====================\n",
            "Starting hyperparameter optimization for SVR...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ SVR completed in 2.38 seconds\n",
            "Best R² Score: 0.9675\n",
            "Best RMSE: 2.3839\n",
            "\n",
            "==================== Decision Tree ====================\n",
            "Starting hyperparameter optimization for Decision Tree...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ Decision Tree completed in 0.16 seconds\n",
            "Best R² Score: 0.7825\n",
            "Best RMSE: 6.1678\n",
            "\n",
            "==================== Extra Trees ====================\n",
            "Starting hyperparameter optimization for Extra Trees...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ Extra Trees completed in 7.61 seconds\n",
            "Best R² Score: 0.9350\n",
            "Best RMSE: 3.3709\n",
            "\n",
            "==================== Random Forest ====================\n",
            "Starting hyperparameter optimization for Random Forest...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ Random Forest completed in 5.55 seconds\n",
            "Best R² Score: 0.9524\n",
            "Best RMSE: 2.8840\n",
            "\n",
            "==================== XGBoost ====================\n",
            "Starting hyperparameter optimization for XGBoost...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ XGBoost completed in 1.76 seconds\n",
            "Best R² Score: 0.9832\n",
            "Best RMSE: 1.7123\n",
            "\n",
            "==================== Elastic Net ====================\n",
            "Starting hyperparameter optimization for Elastic Net...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ Elastic Net completed in 0.13 seconds\n",
            "Best R² Score: 0.8291\n",
            "Best RMSE: 5.4662\n",
            "\n",
            "==================== MLP ====================\n",
            "Starting hyperparameter optimization for MLP...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ MLP completed in 6.04 seconds\n",
            "Best R² Score: -0.4567\n",
            "Best RMSE: 15.9606\n",
            "\n",
            "==================== RBFN ====================\n",
            "Starting hyperparameter optimization for RBFN...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ RBFN completed in 0.56 seconds\n",
            "Best R² Score: 0.9193\n",
            "Best RMSE: 3.7560\n",
            "\n",
            "==================== ELM ====================\n",
            "Starting hyperparameter optimization for ELM...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ ELM completed in 0.39 seconds\n",
            "Best R² Score: -0.1725\n",
            "Best RMSE: 14.3193\n",
            "\n",
            "==================== GRNN ====================\n",
            "Starting hyperparameter optimization for GRNN...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ GRNN completed in 0.21 seconds\n",
            "Best R² Score: 0.9361\n",
            "Best RMSE: 3.3431\n",
            "\n",
            "==================== Cascade-Correlation ====================\n",
            "Starting hyperparameter optimization for Cascade-Correlation...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ Cascade-Correlation completed in 0.19 seconds\n",
            "Best R² Score: 0.9847\n",
            "Best RMSE: 1.6347\n",
            "\n",
            "==================== Deep Feedforward ====================\n",
            "Starting hyperparameter optimization for Deep Feedforward...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ Deep Feedforward completed in 3.07 seconds\n",
            "Best R² Score: -3.3869\n",
            "Best RMSE: 27.6973\n",
            "\n",
            "==================== ResNet ====================\n",
            "Starting hyperparameter optimization for ResNet...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "✓ ResNet completed in 3.17 seconds\n",
            "Best R² Score: -6.6281\n",
            "Best RMSE: 36.5229\n",
            "\n",
            "==================================================\n",
            "CREATING FINAL RESULTS\n",
            "==================================================\n",
            "\n",
            "📊 PERFORMANCE METRICS:\n",
            "      MSE     MAE    RMSE   MAPE      AIC      BIC     COD      R2  Execution_Time               Model\n",
            "   5.6830  1.1200  2.3839 0.1107 260.2202 264.2709  0.9675  0.9675          2.3813                 SVR\n",
            "  38.0413  4.9484  6.1678 0.4868 366.6868 370.7375  0.7825  0.7825          0.1565       Decision Tree\n",
            "  11.3628  2.3769  3.3709 0.2350 299.0205 303.0712  0.9350  0.9350          7.6131         Extra Trees\n",
            "   8.3172  2.2038  2.8840 0.2175 281.5472 285.5979  0.9524  0.9524          5.5529       Random Forest\n",
            "   2.9321  1.2350  1.7123 0.1217 223.1606 227.2113  0.9832  0.9832          1.7645             XGBoost\n",
            "  29.8792  4.1502  5.4662 0.4107 353.1622 357.2129  0.8291  0.8291          0.1297         Elastic Net\n",
            " 254.7410 13.2658 15.9606 1.3100 473.1750 477.2257 -0.4567 -0.4567          6.0439                 MLP\n",
            "  14.1078  3.0282  3.7560 0.2983 311.1378 315.1885  0.9193  0.9193          0.5644                RBFN\n",
            " 205.0411  8.5821 14.3193 0.8421 461.0209 465.0716 -0.1725 -0.1725          0.3933                 ELM\n",
            "  11.1765  2.4897  3.3431 0.2463 298.0948 302.1455  0.9361  0.9361          0.2149                GRNN\n",
            "   2.6721  1.2314  1.6347 0.1210 217.9622 222.0129  0.9847  0.9847          0.1858 Cascade-Correlation\n",
            " 767.1383 21.8460 27.6973 2.1556 534.9105 538.9612 -3.3869 -3.3869          3.0657    Deep Feedforward\n",
            "1333.9249 31.7060 36.5229 3.1218 565.8904 569.9411 -6.6281 -6.6281          3.1721              ResNet\n",
            "\n",
            "🎯 OPTIMAL HYPERPARAMETERS:\n",
            "              Model         C  epsilon    gamma  max_depth  max_features  min_samples_leaf  min_samples_split  n_estimators  learning_rate  subsample    alpha  l1_ratio  max_iter hidden_layer_sizes  learning_rate_init  n_centers  n_hidden   sigma  max_hidden  epochs hidden_layers  hidden_size  n_blocks\n",
            "                SVR 83.344264 0.222339 0.182825        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "      Decision Tree       NaN      NaN      NaN       12.0      0.737265               6.0                3.0           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "        Extra Trees       NaN      NaN      NaN       24.0      0.633173               3.0                6.0         100.0            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "      Random Forest       NaN      NaN      NaN       11.0           NaN               2.0                4.0         124.0            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "            XGBoost       NaN      NaN      NaN        9.0           NaN               NaN                NaN         171.0       0.183611   0.577997      NaN       NaN       NaN                NaN                 NaN        NaN       NaN     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "        Elastic Net       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN 0.205943   0.96991     991.0                NaN                 NaN        NaN       NaN     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "                MLP       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN 0.009740       NaN       NaN           (50, 25)            0.046151        NaN       NaN     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "               RBFN       NaN      NaN 0.107710        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN       25.0       NaN     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "                ELM       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN     445.0     NaN         NaN     NaN           NaN          NaN       NaN\n",
            "               GRNN       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN 0.38461         NaN     NaN           NaN          NaN       NaN\n",
            "Cascade-Correlation       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN     NaN         8.0     NaN           NaN          NaN       NaN\n",
            "   Deep Feedforward       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN       0.060510        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN     NaN         NaN   137.0      (20, 10)          NaN       NaN\n",
            "             ResNet       NaN      NaN      NaN        NaN           NaN               NaN                NaN           NaN       0.062220        NaN      NaN       NaN       NaN                NaN                 NaN        NaN       NaN     NaN         NaN   111.0           NaN         56.0       4.0\n",
            "\n",
            "✅ Results saved to 'performance_metrics.csv' and 'optimal_hyperparameters.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wBl2DbZ8p4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5pHrNmyk8jSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid_search**"
      ],
      "metadata": {
        "id": "FehbK--BHixl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import xgboost as xgb\n",
        "\n",
        "# Custom implementations for specialized models\n",
        "class RBFNRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_centers=20, gamma=1.0):\n",
        "        self.n_centers = n_centers\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simple RBFN implementation\n",
        "        from sklearn.cluster import KMeans\n",
        "        self.kmeans = KMeans(n_clusters=self.n_centers, random_state=42)\n",
        "        self.centers = self.kmeans.fit(X).cluster_centers_\n",
        "\n",
        "        # Calculate RBF activations\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centers, axis=2)\n",
        "        rbf_activations = np.exp(-self.gamma * distances**2)\n",
        "\n",
        "        # Linear regression on RBF activations\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "        self.linear_model = LinearRegression()\n",
        "        self.linear_model.fit(rbf_activations, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centers, axis=2)\n",
        "        rbf_activations = np.exp(-self.gamma * distances**2)\n",
        "        return self.linear_model.predict(rbf_activations)\n",
        "\n",
        "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_hidden=100):\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = X.shape[1]\n",
        "        # Random input weights and biases\n",
        "        self.W_input = np.random.randn(n_features, self.n_hidden)\n",
        "        self.b_input = np.random.randn(self.n_hidden)\n",
        "\n",
        "        # Hidden layer output\n",
        "        H = np.tanh(np.dot(X, self.W_input) + self.b_input)\n",
        "\n",
        "        # Output weights using Moore-Penrose pseudoinverse\n",
        "        self.W_output = np.dot(np.linalg.pinv(H), y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        H = np.tanh(np.dot(X, self.W_input) + self.b_input)\n",
        "        return np.dot(H, self.W_output)\n",
        "\n",
        "class GRNNRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, sigma=1.0):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Calculate distances to all training points\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "            # Gaussian kernel\n",
        "            weights = np.exp(-distances**2 / (2 * self.sigma**2))\n",
        "            # Weighted average\n",
        "            if np.sum(weights) > 0:\n",
        "                pred = np.sum(weights * self.y_train) / np.sum(weights)\n",
        "            else:\n",
        "                pred = np.mean(self.y_train)\n",
        "            predictions.append(pred)\n",
        "        return np.array(predictions)\n",
        "\n",
        "class DeepFeedforwardRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, hidden_layers=(100, 50), learning_rate=0.001, epochs=100):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simple neural network using sklearn's MLPRegressor\n",
        "        self.model = MLPRegressor(\n",
        "            hidden_layer_sizes=self.hidden_layers,\n",
        "            learning_rate_init=self.learning_rate,\n",
        "            max_iter=self.epochs,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "class ResNetRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_blocks=2, hidden_size=50, learning_rate=0.001, epochs=100):\n",
        "        self.n_blocks = n_blocks\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simplified ResNet using multiple MLPs\n",
        "        hidden_layers = tuple([self.hidden_size] * (self.n_blocks + 1))\n",
        "        self.model = MLPRegressor(\n",
        "            hidden_layer_sizes=hidden_layers,\n",
        "            learning_rate_init=self.learning_rate,\n",
        "            max_iter=self.epochs,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Load and preprocess the dataset\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Remove outliers using IQR method\n",
        "    def remove_outliers_iqr(df, column):\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "    # Remove outliers from numerical columns\n",
        "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numerical_columns:\n",
        "        if col != 'Density (ρ)':  # Don't remove outliers from target\n",
        "            df = remove_outliers_iqr(df, col)\n",
        "\n",
        "    # Encode categorical variables\n",
        "    label_encoders = {}\n",
        "    categorical_columns = ['Nano Particle', 'Base Fluid']\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop('Density (ρ)', axis=1)\n",
        "    y = df['Density (ρ)']\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y, label_encoders, scaler\n",
        "\n",
        "# Calculate additional metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive regression metrics\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate AIC and BIC (approximations for regression)\n",
        "    n = len(y_true)\n",
        "    k = 2  # Assuming 2 parameters for simplicity\n",
        "    aic = n * np.log(mse) + 2 * k\n",
        "    bic = n * np.log(mse) + k * np.log(n)\n",
        "\n",
        "    # Coefficient of determination (same as R²)\n",
        "    cod = r2\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'AIC': aic,\n",
        "        'BIC': bic,\n",
        "        'COD': cod,\n",
        "        'R2': r2\n",
        "    }\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load data\n",
        "    file_path = 'Density_Prediction_Dataset.csv'  # Update with your file path\n",
        "    X, y, encoders, scaler = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define models and their hyperparameter grids\n",
        "    models = {\n",
        "        'SVR': {\n",
        "            'model': SVR(),\n",
        "            'params': {\n",
        "                'C': [0.1, 1, 10, 100],\n",
        "                'gamma': [0.001, 0.01, 0.1, 1],\n",
        "                'epsilon': [0.01, 0.1, 0.5, 1]\n",
        "            }\n",
        "        },\n",
        "        'Decision Tree': {\n",
        "            'model': DecisionTreeRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'max_depth': [5, 10, 15, 20],\n",
        "                'min_samples_split': [2, 5, 10, 15],\n",
        "                'min_samples_leaf': [1, 2, 5, 10],\n",
        "                'max_features': [0.1, 0.3, 0.5, 0.7, 1.0]\n",
        "            }\n",
        "        },\n",
        "        'Extra Trees': {\n",
        "            'model': ExtraTreesRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': [50, 100, 200, 300],\n",
        "                'max_depth': [5, 10, 15, 20, 25],\n",
        "                'min_samples_split': [2, 5, 10, 15],\n",
        "                'min_samples_leaf': [1, 2, 5, 8],\n",
        "                'max_features': [0.1, 0.3, 0.5, 0.7, 1.0]\n",
        "            }\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'model': RandomForestRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': [50, 100, 150, 200],\n",
        "                'max_depth': [5, 10, 15, 20],\n",
        "                'min_samples_split': [2, 5, 8, 10],\n",
        "                'min_samples_leaf': [1, 2, 4, 5]\n",
        "            }\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'model': xgb.XGBRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': [50, 100, 150, 200],\n",
        "                'max_depth': [3, 6, 9, 12],\n",
        "                'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "                'subsample': [0.5, 0.7, 0.8, 1.0]\n",
        "            }\n",
        "        },\n",
        "        'Elastic Net': {\n",
        "            'model': ElasticNet(random_state=42),\n",
        "            'params': {\n",
        "                'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
        "                'l1_ratio': [0.0, 0.25, 0.5, 0.75, 1.0],\n",
        "                'max_iter': [500, 1000, 1500, 2000]\n",
        "            }\n",
        "        },\n",
        "        'MLP': {\n",
        "            'model': MLPRegressor(random_state=42),\n",
        "            'params': {\n",
        "                'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (100, 50), (150, 75)],\n",
        "                'alpha': [0.0001, 0.001, 0.01],\n",
        "                'learning_rate_init': [0.001, 0.01, 0.1]\n",
        "            }\n",
        "        },\n",
        "        'RBFN': {\n",
        "            'model': RBFNRegressor(),\n",
        "            'params': {\n",
        "                'n_centers': [5, 10, 20, 30, 50],\n",
        "                'gamma': [0.1, 1.0, 5.0, 10.0]\n",
        "            }\n",
        "        },\n",
        "        'ELM': {\n",
        "            'model': ELMRegressor(),\n",
        "            'params': {\n",
        "                'n_hidden': [10, 50, 100, 200, 500]\n",
        "            }\n",
        "        },\n",
        "        'GRNN': {\n",
        "            'model': GRNNRegressor(),\n",
        "            'params': {\n",
        "                'sigma': [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "            }\n",
        "        },\n",
        "        'Deep Feedforward': {\n",
        "            'model': DeepFeedforwardRegressor(),\n",
        "            'params': {\n",
        "                'hidden_layers': [(20, 10), (50, 25), (100, 50), (100, 50, 25)],\n",
        "                'learning_rate': [0.001, 0.01, 0.1],\n",
        "                'epochs': [50, 100, 200]\n",
        "            }\n",
        "        },\n",
        "        'ResNet': {\n",
        "            'model': ResNetRegressor(),\n",
        "            'params': {\n",
        "                'n_blocks': [1, 2, 3, 5],\n",
        "                'hidden_size': [10, 30, 50, 100],\n",
        "                'learning_rate': [0.001, 0.01, 0.1],\n",
        "                'epochs': [50, 100, 200]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Results storage\n",
        "    results = []\n",
        "    best_params_list = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for model_name, model_config in models.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training {model_name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # GridSearchCV\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model_config['model'],\n",
        "            param_grid=model_config['params'],\n",
        "            cv=5,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            grid_search.fit(X_train, y_train)\n",
        "\n",
        "            # Best model predictions\n",
        "            best_model = grid_search.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "            # Execution time\n",
        "            execution_time = time.time() - start_time\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'Model': model_name,\n",
        "                'MSE': metrics['MSE'],\n",
        "                'MAE': metrics['MAE'],\n",
        "                'RMSE': metrics['RMSE'],\n",
        "                'MAPE': metrics['MAPE'],\n",
        "                'AIC': metrics['AIC'],\n",
        "                'BIC': metrics['BIC'],\n",
        "                'COD': metrics['COD'],\n",
        "                'R2': metrics['R2'],\n",
        "                'Execution_Time_Seconds': execution_time\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Store best parameters\n",
        "            best_params = {\n",
        "                'Model': model_name,\n",
        "                'Best_Score': grid_search.best_score_,\n",
        "                **grid_search.best_params_\n",
        "            }\n",
        "            best_params_list.append(best_params)\n",
        "\n",
        "            print(f\"Best Score: {grid_search.best_score_:.4f}\")\n",
        "            print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "            print(f\"R² Score: {metrics['R2']:.4f}\")\n",
        "            print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {model_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Create DataFrames\n",
        "    performance_df = pd.DataFrame(results)\n",
        "    hyperparameters_df = pd.DataFrame(best_params_list)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(performance_df.to_string(index=False))\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"OPTIMAL HYPERPARAMETERS SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(hyperparameters_df.to_string(index=False))\n",
        "\n",
        "    # Save results to CSV\n",
        "    performance_df.to_csv('model_performance_metrics.csv', index=False)\n",
        "    hyperparameters_df.to_csv('optimal_hyperparameters.csv', index=False)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"Results saved to 'model_performance_metrics.csv' and 'optimal_hyperparameters.csv'\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    return performance_df, hyperparameters_df\n",
        "\n",
        "# Execute the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    performance_df, hyperparameters_df = main()"
      ],
      "metadata": {
        "id": "mKlL_av38jUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357a1a40-b197-490a-ce4c-f72dfb8bce78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Training SVR\n",
            "==================================================\n",
            "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
            "Best Score: -9.3255\n",
            "Best Parameters: {'C': 100, 'epsilon': 0.01, 'gamma': 0.1}\n",
            "R² Score: 0.9882\n",
            "Execution Time: 1.11 seconds\n",
            "\n",
            "==================================================\n",
            "Training Decision Tree\n",
            "==================================================\n",
            "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
            "Best Score: -22.8931\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "R² Score: 0.9118\n",
            "Execution Time: 2.73 seconds\n",
            "\n",
            "==================================================\n",
            "Training Extra Trees\n",
            "==================================================\n",
            "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
            "Best Score: -10.3644\n",
            "Best Parameters: {'max_depth': 10, 'max_features': 0.3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "R² Score: 0.9756\n",
            "Execution Time: 1026.96 seconds\n",
            "\n",
            "==================================================\n",
            "Training Random Forest\n",
            "==================================================\n",
            "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
            "Best Score: -16.7943\n",
            "Best Parameters: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "R² Score: 0.9644\n",
            "Execution Time: 187.49 seconds\n",
            "\n",
            "==================================================\n",
            "Training XGBoost\n",
            "==================================================\n",
            "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
            "Best Score: -3.9549\n",
            "Best Parameters: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}\n",
            "R² Score: 0.9920\n",
            "Execution Time: 43.56 seconds\n",
            "\n",
            "==================================================\n",
            "Training Elastic Net\n",
            "==================================================\n",
            "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
            "Best Score: -26.0408\n",
            "Best Parameters: {'alpha': 0.01, 'l1_ratio': 1.0, 'max_iter': 500}\n",
            "R² Score: 0.9156\n",
            "Execution Time: 2.17 seconds\n",
            "\n",
            "==================================================\n",
            "Training MLP\n",
            "==================================================\n",
            "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
            "Best Score: -685.5011\n",
            "Best Parameters: {'alpha': 0.001, 'hidden_layer_sizes': (150, 75), 'learning_rate_init': 0.1}\n",
            "R² Score: -0.7483\n",
            "Execution Time: 32.71 seconds\n",
            "\n",
            "==================================================\n",
            "Training RBFN\n",
            "==================================================\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Score: -3.4742\n",
            "Best Parameters: {'gamma': 0.1, 'n_centers': 50}\n",
            "R² Score: 0.9888\n",
            "Execution Time: 0.82 seconds\n",
            "\n",
            "==================================================\n",
            "Training ELM\n",
            "==================================================\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Score: -146.7918\n",
            "Best Parameters: {'n_hidden': 200}\n",
            "R² Score: 0.9074\n",
            "Execution Time: 0.16 seconds\n",
            "\n",
            "==================================================\n",
            "Training GRNN\n",
            "==================================================\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Score: -16.6020\n",
            "Best Parameters: {'sigma': 0.5}\n",
            "R² Score: 0.9511\n",
            "Execution Time: 0.20 seconds\n",
            "\n",
            "==================================================\n",
            "Training Deep Feedforward\n",
            "==================================================\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Best Score: -979.0521\n",
            "Best Parameters: {'epochs': 200, 'hidden_layers': (50, 25), 'learning_rate': 0.1}\n",
            "R² Score: -2.5943\n",
            "Execution Time: 13.43 seconds\n",
            "\n",
            "==================================================\n",
            "Training ResNet\n",
            "==================================================\n",
            "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
            "Best Score: -409.0203\n",
            "Best Parameters: {'epochs': 200, 'hidden_size': 100, 'learning_rate': 0.01, 'n_blocks': 5}\n",
            "R² Score: -0.4677\n",
            "Execution Time: 66.82 seconds\n",
            "\n",
            "================================================================================\n",
            "PERFORMANCE METRICS SUMMARY\n",
            "================================================================================\n",
            "           Model        MSE       MAE      RMSE     MAPE        AIC        BIC       COD        R2  Execution_Time_Seconds\n",
            "             SVR   1.818124  0.739820  1.348378 0.000727  29.107826  32.583165  0.988180  0.988180                1.114423\n",
            "   Decision Tree  13.567704  3.118881  3.683436 0.003073 113.523075 116.998414  0.911796  0.911796                2.730490\n",
            "     Extra Trees   3.749746  1.407415  1.936426 0.001391  59.510900  62.986239  0.975623  0.975623             1026.955745\n",
            "   Random Forest   5.476755  1.811916  2.340247 0.001786  75.421538  78.896877  0.964396  0.964396              187.488847\n",
            "         XGBoost   1.227156  0.875815  1.107771 0.000861  12.597364  16.072703  0.992022  0.992022               43.559918\n",
            "     Elastic Net  12.990139  2.534432  3.604184 0.002487 111.696002 115.171342  0.915551  0.915551                2.167920\n",
            "             MLP 268.921904 10.204509 16.398839 0.010081 238.965683 242.441022 -0.748258 -0.748258               32.712902\n",
            "            RBFN   1.716813  1.024026  1.310272 0.001009  26.699724  30.175063  0.988839  0.988839                0.824996\n",
            "             ELM  14.249469  2.257102  3.774847 0.002228 115.582224 119.057563  0.907364  0.907364                0.162916\n",
            "            GRNN   7.517391  2.109076  2.741786 0.002078  88.723206  92.198545  0.951130  0.951130                0.202981\n",
            "Deep Feedforward 552.882691 13.816861 23.513458 0.013654 269.236126 272.711465 -2.594285 -2.594285               13.426625\n",
            "          ResNet 225.759537 10.630393 15.025297 0.010457 231.617758 235.093098 -0.467660 -0.467660               66.820886\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL HYPERPARAMETERS SUMMARY\n",
            "================================================================================\n",
            "           Model  Best_Score     C  epsilon  gamma  max_depth  max_features  min_samples_leaf  min_samples_split  n_estimators  learning_rate  subsample  alpha  l1_ratio  max_iter hidden_layer_sizes  learning_rate_init  n_centers  n_hidden  sigma  epochs hidden_layers  hidden_size  n_blocks\n",
            "             SVR   -9.325462 100.0     0.01    0.1        NaN           NaN               NaN                NaN           NaN            NaN        NaN    NaN       NaN       NaN                NaN                 NaN        NaN       NaN    NaN     NaN           NaN          NaN       NaN\n",
            "   Decision Tree  -22.893099   NaN      NaN    NaN       15.0           1.0               1.0                2.0           NaN            NaN        NaN    NaN       NaN       NaN                NaN                 NaN        NaN       NaN    NaN     NaN           NaN          NaN       NaN\n",
            "     Extra Trees  -10.364406   NaN      NaN    NaN       10.0           0.3               1.0                2.0         300.0            NaN        NaN    NaN       NaN       NaN                NaN                 NaN        NaN       NaN    NaN     NaN           NaN          NaN       NaN\n",
            "   Random Forest  -16.794311   NaN      NaN    NaN       15.0           NaN               1.0                2.0         100.0            NaN        NaN    NaN       NaN       NaN                NaN                 NaN        NaN       NaN    NaN     NaN           NaN          NaN       NaN\n",
            "         XGBoost   -3.954877   NaN      NaN    NaN        3.0           NaN               NaN                NaN         200.0           0.30        0.5    NaN       NaN       NaN                NaN                 NaN        NaN       NaN    NaN     NaN           NaN          NaN       NaN\n",
            "     Elastic Net  -26.040806   NaN      NaN    NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN  0.010       1.0     500.0                NaN                 NaN        NaN       NaN    NaN     NaN           NaN          NaN       NaN\n",
            "             MLP -685.501062   NaN      NaN    NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN  0.001       NaN       NaN          (150, 75)                 0.1        NaN       NaN    NaN     NaN           NaN          NaN       NaN\n",
            "            RBFN   -3.474219   NaN      NaN    0.1        NaN           NaN               NaN                NaN           NaN            NaN        NaN    NaN       NaN       NaN                NaN                 NaN       50.0       NaN    NaN     NaN           NaN          NaN       NaN\n",
            "             ELM -146.791801   NaN      NaN    NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN    NaN       NaN       NaN                NaN                 NaN        NaN     200.0    NaN     NaN           NaN          NaN       NaN\n",
            "            GRNN  -16.602014   NaN      NaN    NaN        NaN           NaN               NaN                NaN           NaN            NaN        NaN    NaN       NaN       NaN                NaN                 NaN        NaN       NaN    0.5     NaN           NaN          NaN       NaN\n",
            "Deep Feedforward -979.052096   NaN      NaN    NaN        NaN           NaN               NaN                NaN           NaN           0.10        NaN    NaN       NaN       NaN                NaN                 NaN        NaN       NaN    NaN   200.0      (50, 25)          NaN       NaN\n",
            "          ResNet -409.020349   NaN      NaN    NaN        NaN           NaN               NaN                NaN           NaN           0.01        NaN    NaN       NaN       NaN                NaN                 NaN        NaN       NaN    NaN   200.0           NaN        100.0       5.0\n",
            "\n",
            "================================================================================\n",
            "Results saved to 'model_performance_metrics.csv' and 'optimal_hyperparameters.csv'\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qi2oDCeq8jXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQ1GD7FY8jZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}